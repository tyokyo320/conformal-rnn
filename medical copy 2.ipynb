{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bfaf9e81-c095-4d66-812e-3459dbc93c65",
   "metadata": {},
   "source": [
    "# Experiments on real-world data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e8602d8b-cb7c-488b-9787-1e624204f8f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from utils.train_medical import run_medical_experiments\n",
    "from utils.results import (\n",
    "    get_joint_medical_coverages, \n",
    "    get_medical_interval_widths, \n",
    "    load_medical_results, \n",
    "    get_uncorrected_medical_results\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f13c3ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFORMAL_BASELINES = [\"CFRNN\", \"AdaptiveCFRNN\"]\n",
    "CONFORMAL_BASELINES = [\"CFRNN\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9455748f-ff7b-4fa6-8c02-45dfd5509810",
   "metadata": {},
   "source": [
    "To obtain the results as presented in the paper, run the following three sections."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20370c7c-eaca-49aa-8e45-7f91c01c4faa",
   "metadata": {},
   "source": [
    "## Electricity Consumption dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "84cb28a9-05b8-4be6-9656-44cc61953beb",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training CFRNN\n",
      "Epoch: 0\tTrain loss: 1321766.5416666667\n",
      "Epoch: 50\tTrain loss: 1252043.4791666667\n",
      "Epoch: 100\tTrain loss: 1189141.0833333333\n",
      "Epoch: 150\tTrain loss: 1129833.3020833333\n",
      "Epoch: 200\tTrain loss: 1073516.171875\n",
      "Epoch: 250\tTrain loss: 1019876.0520833334\n",
      "Epoch: 300\tTrain loss: 968642.9479166666\n",
      "Epoch: 350\tTrain loss: 919820.6614583334\n",
      "Epoch: 400\tTrain loss: 872994.9375\n",
      "Epoch: 450\tTrain loss: 828535.5625\n",
      "Epoch: 500\tTrain loss: 786026.96875\n",
      "Epoch: 550\tTrain loss: 745707.9791666666\n",
      "Epoch: 600\tTrain loss: 707308.8333333334\n",
      "Epoch: 650\tTrain loss: 670644.453125\n",
      "Epoch: 700\tTrain loss: 635898.6458333334\n",
      "Epoch: 750\tTrain loss: 602793.671875\n",
      "Epoch: 800\tTrain loss: 571524.75\n",
      "Epoch: 850\tTrain loss: 541814.2552083334\n",
      "Epoch: 900\tTrain loss: 513574.203125\n",
      "Epoch: 950\tTrain loss: 487008.9427083333\n",
      "Epoch: 1000\tTrain loss: 461844.6380208333\n",
      "Epoch: 1050\tTrain loss: 438249.2265625\n",
      "Epoch: 1100\tTrain loss: 415907.5364583333\n",
      "Epoch: 1150\tTrain loss: 395081.4973958333\n",
      "Epoch: 1200\tTrain loss: 375496.875\n",
      "Epoch: 1250\tTrain loss: 357214.8203125\n",
      "Epoch: 1300\tTrain loss: 340149.2526041667\n",
      "Epoch: 1350\tTrain loss: 324339.3645833333\n",
      "Epoch: 1400\tTrain loss: 309651.9375\n",
      "Epoch: 1450\tTrain loss: 296020.5950520833\n",
      "Epoch: 1500\tTrain loss: 283476.8932291667\n",
      "Epoch: 1550\tTrain loss: 271999.8645833333\n",
      "Epoch: 1600\tTrain loss: 261424.61328125\n",
      "Epoch: 1650\tTrain loss: 251826.71223958334\n",
      "Epoch: 1700\tTrain loss: 243119.99609375\n",
      "Epoch: 1750\tTrain loss: 235200.96484375\n",
      "Epoch: 1800\tTrain loss: 228069.75065104166\n",
      "Epoch: 1850\tTrain loss: 221650.03190104166\n",
      "Epoch: 1900\tTrain loss: 215995.1923828125\n",
      "Epoch: 1950\tTrain loss: 210981.83235677084\n",
      "Epoch: 2000\tTrain loss: 206548.04296875\n",
      "Epoch: 2050\tTrain loss: 202599.40478515625\n",
      "Epoch: 2100\tTrain loss: 199190.72998046875\n",
      "Epoch: 2150\tTrain loss: 196236.96223958334\n",
      "Epoch: 2200\tTrain loss: 193675.1015625\n",
      "Epoch: 2250\tTrain loss: 191486.953125\n",
      "Epoch: 2300\tTrain loss: 189617.94401041666\n",
      "Epoch: 2350\tTrain loss: 188060.671875\n",
      "Epoch: 2400\tTrain loss: 163126.02994791666\n",
      "Epoch: 2450\tTrain loss: 130771.18912760417\n",
      "Epoch: 2500\tTrain loss: 122733.49678548177\n",
      "Epoch: 2550\tTrain loss: 115057.20892588298\n",
      "Epoch: 2600\tTrain loss: 108341.01330566406\n",
      "Epoch: 2650\tTrain loss: 101208.91967773438\n",
      "Epoch: 2700\tTrain loss: 94679.27971394856\n",
      "Epoch: 2750\tTrain loss: 88487.40192921956\n",
      "Epoch: 2800\tTrain loss: 82534.5904083252\n",
      "Epoch: 2850\tTrain loss: 77025.59839375813\n",
      "Epoch: 2900\tTrain loss: 71623.0666809082\n",
      "Epoch: 2950\tTrain loss: 66605.73034413655\n",
      "Epoch: 3000\tTrain loss: 61825.81783040365\n",
      "Epoch: 3050\tTrain loss: 57214.96110280355\n",
      "Epoch: 3100\tTrain loss: 52882.78849029541\n",
      "Epoch: 3150\tTrain loss: 48829.98705546061\n",
      "Epoch: 3200\tTrain loss: 45030.01276270548\n",
      "Epoch: 3250\tTrain loss: 41475.15118408203\n",
      "Epoch: 3300\tTrain loss: 38068.19110997518\n",
      "Epoch: 3350\tTrain loss: 35004.70650736491\n",
      "Epoch: 3400\tTrain loss: 32060.533933003742\n",
      "Epoch: 3450\tTrain loss: 29395.702523549397\n",
      "Epoch: 3500\tTrain loss: 27096.96551132202\n",
      "Epoch: 3550\tTrain loss: 24588.990319569904\n",
      "Epoch: 3600\tTrain loss: 22637.79815165202\n",
      "Epoch: 3650\tTrain loss: 20593.424455006916\n",
      "Epoch: 3700\tTrain loss: 18856.440119425457\n",
      "Epoch: 3750\tTrain loss: 17273.548116048176\n",
      "Epoch: 3800\tTrain loss: 15874.029879252115\n",
      "Epoch: 3850\tTrain loss: 14713.86361694336\n",
      "Epoch: 3900\tTrain loss: 13485.160502115885\n",
      "Epoch: 3950\tTrain loss: 12858.268758138021\n",
      "Epoch: 4000\tTrain loss: 11632.73282623291\n",
      "Epoch: 4050\tTrain loss: 10960.720467885336\n",
      "Epoch: 4100\tTrain loss: 10246.345489501953\n",
      "Epoch: 4150\tTrain loss: 9676.907676696777\n",
      "Epoch: 4200\tTrain loss: 9203.497613271078\n",
      "Epoch: 4250\tTrain loss: 7758.2370440165205\n",
      "Epoch: 4300\tTrain loss: 6971.370347340901\n",
      "Epoch: 4350\tTrain loss: 6144.359967549642\n",
      "Epoch: 4400\tTrain loss: 5428.472307840983\n",
      "Epoch: 4450\tTrain loss: 4763.231101989746\n",
      "Epoch: 4500\tTrain loss: 4144.706644694011\n",
      "Epoch: 4550\tTrain loss: 3625.862668991089\n",
      "Epoch: 4600\tTrain loss: 3109.7563705444336\n",
      "Epoch: 4650\tTrain loss: 2683.0911966959634\n",
      "Epoch: 4700\tTrain loss: 2254.7872816721597\n",
      "Epoch: 4750\tTrain loss: 1930.2466583251953\n",
      "Epoch: 4800\tTrain loss: 1598.379955291748\n",
      "Epoch: 4850\tTrain loss: 1314.0019232432048\n",
      "Epoch: 4900\tTrain loss: 1188.6475321451824\n",
      "Epoch: 4950\tTrain loss: 877.5970738728842\n",
      "Epoch: 5000\tTrain loss: 711.7950356801351\n",
      "Epoch: 5050\tTrain loss: 562.8866068522135\n",
      "Epoch: 5100\tTrain loss: 447.64072545369464\n",
      "Epoch: 5150\tTrain loss: 358.0184218088786\n",
      "Epoch: 5200\tTrain loss: 719.9789072672526\n",
      "Epoch: 5250\tTrain loss: 223.20011138916016\n",
      "Epoch: 5300\tTrain loss: 171.7331714630127\n",
      "Epoch: 5350\tTrain loss: 137.02929051717123\n",
      "Epoch: 5400\tTrain loss: 112.25388431549072\n",
      "Epoch: 5450\tTrain loss: 101.53400039672852\n",
      "Epoch: 5500\tTrain loss: 81.81518173217773\n",
      "Epoch: 5550\tTrain loss: 340.622314453125\n",
      "Epoch: 5600\tTrain loss: 71.96510823567708\n",
      "Epoch: 5650\tTrain loss: 68.70612303415935\n",
      "Epoch: 5700\tTrain loss: 59.00647862752279\n",
      "Epoch: 5750\tTrain loss: 57.33224741617838\n",
      "Epoch: 5800\tTrain loss: 58.12713750203451\n",
      "Epoch: 5850\tTrain loss: 54.49497604370117\n",
      "Epoch: 5900\tTrain loss: 261.23562876383465\n",
      "Epoch: 5950\tTrain loss: 51.9383872350057\n",
      "Epoch: 6000\tTrain loss: 48.483059883117676\n",
      "Epoch: 6050\tTrain loss: 46.96792221069336\n",
      "Epoch: 6100\tTrain loss: 45.43679682413737\n",
      "Epoch: 6150\tTrain loss: 43.861802101135254\n",
      "Epoch: 6200\tTrain loss: 45.12251281738281\n",
      "Epoch: 6250\tTrain loss: 40.76676972707113\n",
      "Epoch: 6300\tTrain loss: 39.0088996887207\n",
      "Epoch: 6350\tTrain loss: 43.58837445576986\n",
      "Epoch: 6400\tTrain loss: 35.51939900716146\n",
      "Epoch: 6450\tTrain loss: 270.2483367919922\n",
      "Epoch: 6500\tTrain loss: 37.23177846272787\n",
      "Epoch: 6550\tTrain loss: 30.18438148498535\n",
      "Epoch: 6600\tTrain loss: 27.797844250996906\n",
      "Epoch: 6650\tTrain loss: 25.410313924153645\n",
      "Epoch: 6700\tTrain loss: 34.0686461130778\n",
      "Epoch: 6750\tTrain loss: 30.883179664611816\n",
      "Epoch: 6800\tTrain loss: 22.022488594055176\n",
      "Epoch: 6850\tTrain loss: 19.853376706441242\n",
      "Epoch: 6900\tTrain loss: 18.778450965881348\n",
      "Epoch: 6950\tTrain loss: 17.92073090871175\n",
      "Epoch: 7000\tTrain loss: 119.51889832814534\n",
      "Epoch: 7050\tTrain loss: 17.785284519195557\n",
      "Epoch: 7100\tTrain loss: 16.24018681049347\n",
      "Epoch: 7150\tTrain loss: 15.53950039545695\n",
      "Epoch: 7200\tTrain loss: 15.92655642827352\n",
      "Epoch: 7250\tTrain loss: 14.790888031323751\n",
      "Epoch: 7300\tTrain loss: 20.482258478800457\n",
      "Epoch: 7350\tTrain loss: 14.765920281410217\n",
      "Epoch: 7400\tTrain loss: 13.868999242782593\n",
      "Epoch: 7450\tTrain loss: 13.753235658009848\n",
      "Epoch: 7500\tTrain loss: 14.898611644903818\n",
      "Epoch: 7550\tTrain loss: 13.213653484980265\n",
      "Epoch: 7600\tTrain loss: 12.938222964604696\n",
      "Epoch: 7650\tTrain loss: 12.699103116989136\n",
      "Epoch: 7700\tTrain loss: 12.459096471468607\n",
      "Epoch: 7750\tTrain loss: 12.202576259771982\n",
      "Epoch: 7800\tTrain loss: 11.983588774998983\n",
      "Epoch: 7850\tTrain loss: 12.056655824184418\n",
      "Epoch: 7900\tTrain loss: 35.00636521975199\n",
      "Epoch: 7950\tTrain loss: 11.989569948986173\n",
      "Epoch: 8000\tTrain loss: 11.22859765123576\n",
      "Epoch: 8050\tTrain loss: 11.011548022429148\n",
      "Epoch: 8100\tTrain loss: 10.792923053105673\n",
      "Epoch: 8150\tTrain loss: 10.586709757645925\n",
      "Epoch: 8200\tTrain loss: 10.664385795593262\n",
      "Epoch: 8250\tTrain loss: 10.122353457690528\n",
      "Epoch: 8300\tTrain loss: 9.914214545705667\n",
      "Epoch: 8350\tTrain loss: 12.664502521355947\n",
      "Epoch: 8400\tTrain loss: 15.413089911142984\n",
      "Epoch: 8450\tTrain loss: 9.797652761141459\n",
      "Epoch: 8500\tTrain loss: 9.097892651955286\n",
      "Epoch: 8550\tTrain loss: 8.87501779695352\n",
      "Epoch: 8600\tTrain loss: 9.41092192629973\n",
      "Epoch: 8650\tTrain loss: 8.587670370936394\n",
      "Epoch: 8700\tTrain loss: 8.409169976909956\n",
      "Epoch: 8750\tTrain loss: 8.030674263834953\n",
      "Epoch: 8800\tTrain loss: 8.110194964334369\n",
      "Epoch: 8850\tTrain loss: 7.892871464292209\n",
      "Epoch: 8900\tTrain loss: 18.298235098520916\n",
      "Epoch: 8950\tTrain loss: 7.321019907792409\n",
      "Epoch: 9000\tTrain loss: 7.1560329248507815\n",
      "Epoch: 9050\tTrain loss: 7.024030347665151\n",
      "Epoch: 9100\tTrain loss: 6.864015241463979\n",
      "Epoch: 9150\tTrain loss: 6.702954448759556\n",
      "Epoch: 9200\tTrain loss: 6.553871158297018\n",
      "Epoch: 9250\tTrain loss: 6.381476998329163\n",
      "Epoch: 9300\tTrain loss: 6.217338986927643\n",
      "Epoch: 9350\tTrain loss: 6.022477969333219\n",
      "Epoch: 9400\tTrain loss: 5.861196594729942\n",
      "Epoch: 9450\tTrain loss: 25.91714906692505\n",
      "Epoch: 9500\tTrain loss: 14.077301025390625\n",
      "Epoch: 9550\tTrain loss: 6.437514305114746\n",
      "Epoch: 9600\tTrain loss: 5.205037710567315\n",
      "Epoch: 9650\tTrain loss: 5.068481971820195\n",
      "Epoch: 9700\tTrain loss: 4.931752313141867\n",
      "Epoch: 9750\tTrain loss: 4.794301355878512\n",
      "Epoch: 9800\tTrain loss: 4.646472714841366\n",
      "Epoch: 9850\tTrain loss: 4.551792344699304\n",
      "Epoch: 9900\tTrain loss: 4.598267989854018\n",
      "Epoch: 9950\tTrain loss: 8.520325561364492\n",
      "Training CFRNN\n",
      "Epoch: 0\tTrain loss: 1207803.3333333333\n",
      "Epoch: 50\tTrain loss: 1140862.6458333333\n",
      "Epoch: 100\tTrain loss: 1080335.7291666667\n",
      "Epoch: 150\tTrain loss: 1023422.9583333334\n",
      "Epoch: 200\tTrain loss: 969580.1979166666\n",
      "Epoch: 250\tTrain loss: 918207.0625\n",
      "Epoch: 300\tTrain loss: 869391.84375\n",
      "Epoch: 350\tTrain loss: 822956.6770833334\n",
      "Epoch: 400\tTrain loss: 778658.109375\n",
      "Epoch: 450\tTrain loss: 736786.4166666666\n",
      "Epoch: 500\tTrain loss: 696806.0\n",
      "Epoch: 550\tTrain loss: 658975.9583333334\n",
      "Epoch: 600\tTrain loss: 622895.34375\n",
      "Epoch: 650\tTrain loss: 588695.7252604166\n",
      "Epoch: 700\tTrain loss: 556208.5677083334\n",
      "Epoch: 750\tTrain loss: 525572.53125\n",
      "Epoch: 800\tTrain loss: 496535.5416666667\n",
      "Epoch: 850\tTrain loss: 469181.8541666667\n",
      "Epoch: 900\tTrain loss: 443328.5026041667\n",
      "Epoch: 950\tTrain loss: 419080.7317708333\n",
      "Epoch: 1000\tTrain loss: 396201.9765625\n",
      "Epoch: 1050\tTrain loss: 374778.8125\n",
      "Epoch: 1100\tTrain loss: 354666.4518229167\n",
      "Epoch: 1150\tTrain loss: 335897.7252604167\n",
      "Epoch: 1200\tTrain loss: 318467.04573567706\n",
      "Epoch: 1250\tTrain loss: 302207.5781656901\n",
      "Epoch: 1300\tTrain loss: 287110.8333333333\n",
      "Epoch: 1350\tTrain loss: 273209.8704427083\n",
      "Epoch: 1400\tTrain loss: 260360.048828125\n",
      "Epoch: 1450\tTrain loss: 248562.50520833334\n",
      "Epoch: 1500\tTrain loss: 237760.79296875\n",
      "Epoch: 1550\tTrain loss: 227994.31510416666\n",
      "Epoch: 1600\tTrain loss: 219060.47265625\n",
      "Epoch: 1650\tTrain loss: 210995.95377604166\n",
      "Epoch: 1700\tTrain loss: 203721.115234375\n",
      "Epoch: 1750\tTrain loss: 197244.99544270834\n",
      "Epoch: 1800\tTrain loss: 191421.015625\n",
      "Epoch: 1850\tTrain loss: 186224.52278645834\n",
      "Epoch: 1900\tTrain loss: 181702.86328125\n",
      "Epoch: 1950\tTrain loss: 177694.822265625\n",
      "Epoch: 2000\tTrain loss: 174176.765625\n",
      "Epoch: 2050\tTrain loss: 171206.3515625\n",
      "Epoch: 2100\tTrain loss: 168596.19401041666\n",
      "Epoch: 2150\tTrain loss: 166402.47005208334\n",
      "Epoch: 2200\tTrain loss: 164458.16145833334\n",
      "Epoch: 2250\tTrain loss: 162832.46525065103\n",
      "Epoch: 2300\tTrain loss: 154207.42057291666\n",
      "Epoch: 2350\tTrain loss: 130512.642578125\n",
      "Epoch: 2400\tTrain loss: 113335.37600708008\n",
      "Epoch: 2450\tTrain loss: 106948.28631591797\n",
      "Epoch: 2500\tTrain loss: 100834.85166931152\n",
      "Epoch: 2550\tTrain loss: 95355.42523701985\n",
      "Epoch: 2600\tTrain loss: 90344.40563456218\n",
      "Epoch: 2650\tTrain loss: 85591.10451253255\n",
      "Epoch: 2700\tTrain loss: 81379.79791005452\n",
      "Epoch: 2750\tTrain loss: 76689.36993789673\n",
      "Epoch: 2800\tTrain loss: 72527.1869951884\n",
      "Epoch: 2850\tTrain loss: 68632.84370295207\n",
      "Epoch: 2900\tTrain loss: 64873.54224014282\n",
      "Epoch: 2950\tTrain loss: 61474.51657613119\n",
      "Epoch: 3000\tTrain loss: 58600.711840311684\n",
      "Epoch: 3050\tTrain loss: 55086.125608444214\n",
      "Epoch: 3100\tTrain loss: 52264.45983123779\n",
      "Epoch: 3150\tTrain loss: 49563.35698572794\n",
      "Epoch: 3200\tTrain loss: 47125.475494384766\n",
      "Epoch: 3250\tTrain loss: 44803.95253690084\n",
      "Epoch: 3300\tTrain loss: 42317.66172917684\n",
      "Epoch: 3350\tTrain loss: 39941.06941731771\n",
      "Epoch: 3400\tTrain loss: 37628.341327667236\n",
      "Epoch: 3450\tTrain loss: 35499.91984049479\n",
      "Epoch: 3500\tTrain loss: 33273.71079889933\n",
      "Epoch: 3550\tTrain loss: 31235.607040405273\n",
      "Epoch: 3600\tTrain loss: 29166.8405901591\n",
      "Epoch: 3650\tTrain loss: 27317.629934946697\n",
      "Epoch: 3700\tTrain loss: 25394.950174013775\n",
      "Epoch: 3750\tTrain loss: 23642.964022318523\n",
      "Epoch: 3800\tTrain loss: 21923.7016620636\n",
      "Epoch: 3850\tTrain loss: 20290.82391166687\n",
      "Epoch: 3900\tTrain loss: 18723.07852935791\n",
      "Epoch: 3950\tTrain loss: 17247.124821662903\n",
      "Epoch: 4000\tTrain loss: 15935.174644470215\n",
      "Epoch: 4050\tTrain loss: 14515.291138966879\n",
      "Epoch: 4100\tTrain loss: 13287.524780273438\n",
      "Epoch: 4150\tTrain loss: 12074.306587219238\n",
      "Epoch: 4200\tTrain loss: 10929.866938908895\n",
      "Epoch: 4250\tTrain loss: 9876.245152791342\n",
      "Epoch: 4300\tTrain loss: 8915.480796813965\n",
      "Epoch: 4350\tTrain loss: 7971.471537590027\n",
      "Epoch: 4400\tTrain loss: 7117.876461029053\n",
      "Epoch: 4450\tTrain loss: 6320.271998087565\n",
      "Epoch: 4500\tTrain loss: 5589.575886726379\n",
      "Epoch: 4550\tTrain loss: 4937.725529670715\n",
      "Epoch: 4600\tTrain loss: 4303.097043673198\n",
      "Epoch: 4650\tTrain loss: 3750.9587217966714\n",
      "Epoch: 4700\tTrain loss: 3241.7575359344482\n",
      "Epoch: 4750\tTrain loss: 2774.12628809611\n",
      "Epoch: 4800\tTrain loss: 2374.230005900065\n",
      "Epoch: 4850\tTrain loss: 2053.7226378122964\n",
      "Epoch: 4900\tTrain loss: 1685.1479148864746\n",
      "Epoch: 4950\tTrain loss: 1406.0317341486614\n",
      "Epoch: 5000\tTrain loss: 1167.3493957519531\n",
      "Epoch: 5050\tTrain loss: 1024.1949920654297\n",
      "Epoch: 5100\tTrain loss: 784.2610088984171\n",
      "Epoch: 5150\tTrain loss: 638.7393341064453\n",
      "Epoch: 5200\tTrain loss: 517.8775657018026\n",
      "Epoch: 5250\tTrain loss: 418.4305404027303\n",
      "Epoch: 5300\tTrain loss: 339.4903262456258\n",
      "Epoch: 5350\tTrain loss: 352.467778523763\n",
      "Epoch: 5400\tTrain loss: 341.3953348795573\n",
      "Epoch: 5450\tTrain loss: 190.67641989390054\n",
      "Epoch: 5500\tTrain loss: 161.93962574005127\n",
      "Epoch: 5550\tTrain loss: 140.85584131876627\n",
      "Epoch: 5600\tTrain loss: 124.97225443522136\n",
      "Epoch: 5650\tTrain loss: 113.32464599609375\n",
      "Epoch: 5700\tTrain loss: 150.68159866333008\n",
      "Epoch: 5750\tTrain loss: 98.693785349528\n",
      "Epoch: 5800\tTrain loss: 113.94382222493489\n",
      "Epoch: 5850\tTrain loss: 122.4273452758789\n",
      "Epoch: 5900\tTrain loss: 110.33960978190105\n",
      "Epoch: 5950\tTrain loss: 79.80831527709961\n",
      "Epoch: 6000\tTrain loss: 76.05758380889893\n",
      "Epoch: 6050\tTrain loss: 72.52674357096355\n",
      "Epoch: 6100\tTrain loss: 69.01562754313152\n",
      "Epoch: 6150\tTrain loss: 65.7157376607259\n",
      "Epoch: 6200\tTrain loss: 62.420786221822105\n",
      "Epoch: 6250\tTrain loss: 59.835854848225914\n",
      "Epoch: 6300\tTrain loss: 56.56429354349772\n",
      "Epoch: 6350\tTrain loss: 152.6043904622396\n",
      "Epoch: 6400\tTrain loss: 52.55159950256348\n",
      "Epoch: 6450\tTrain loss: 47.78166643778483\n",
      "Epoch: 6500\tTrain loss: 45.204178174336754\n",
      "Epoch: 6550\tTrain loss: 48.619370778401695\n",
      "Epoch: 6600\tTrain loss: 42.78285471598307\n",
      "Epoch: 6650\tTrain loss: 39.14220873514811\n",
      "Epoch: 6700\tTrain loss: 39.91191864013672\n",
      "Epoch: 6750\tTrain loss: 34.303955713907875\n",
      "Epoch: 6800\tTrain loss: 32.484908739725746\n",
      "Epoch: 6850\tTrain loss: 30.804217497507732\n",
      "Epoch: 6900\tTrain loss: 29.133357365926106\n",
      "Epoch: 6950\tTrain loss: 27.63694699605306\n",
      "Epoch: 7000\tTrain loss: 26.210038503011067\n",
      "Epoch: 7050\tTrain loss: 41.91037050882975\n",
      "Epoch: 7100\tTrain loss: 23.753835837046307\n",
      "Epoch: 7150\tTrain loss: 22.57257588704427\n",
      "Epoch: 7200\tTrain loss: 21.846519867579143\n",
      "Epoch: 7250\tTrain loss: 21.056038697560627\n",
      "Epoch: 7300\tTrain loss: 19.72392813364665\n",
      "Epoch: 7350\tTrain loss: 41.466202100118004\n",
      "Epoch: 7400\tTrain loss: 19.051650365193684\n",
      "Epoch: 7450\tTrain loss: 17.435283660888672\n",
      "Epoch: 7500\tTrain loss: 16.76823874314626\n",
      "Epoch: 7550\tTrain loss: 16.16142161687215\n",
      "Epoch: 7600\tTrain loss: 15.692876180013021\n",
      "Epoch: 7650\tTrain loss: 15.081300099690756\n",
      "Epoch: 7700\tTrain loss: 14.592851479848227\n",
      "Epoch: 7750\tTrain loss: 14.955196062723795\n",
      "Epoch: 7800\tTrain loss: 25.279635032018025\n",
      "Epoch: 7850\tTrain loss: 13.944382031758627\n",
      "Epoch: 7900\tTrain loss: 69.33274904886882\n",
      "Epoch: 7950\tTrain loss: 12.647636930147806\n",
      "Epoch: 8000\tTrain loss: 12.352205276489258\n",
      "Epoch: 8050\tTrain loss: 12.058147390683493\n",
      "Epoch: 8100\tTrain loss: 11.786203602949778\n",
      "Epoch: 8150\tTrain loss: 11.520661552747091\n",
      "Epoch: 8200\tTrain loss: 11.276843080917994\n",
      "Epoch: 8250\tTrain loss: 11.106749276320139\n",
      "Epoch: 8300\tTrain loss: 10.793477217356363\n",
      "Epoch: 8350\tTrain loss: 17.84832541147868\n",
      "Epoch: 8400\tTrain loss: 10.66749574492375\n",
      "Epoch: 8450\tTrain loss: 28.185152769088745\n",
      "Epoch: 8500\tTrain loss: 10.297082602977753\n",
      "Epoch: 8550\tTrain loss: 10.676165810475746\n",
      "Epoch: 8600\tTrain loss: 9.415650675694147\n",
      "Epoch: 8650\tTrain loss: 9.205115099747976\n",
      "Epoch: 8700\tTrain loss: 8.91444573799769\n",
      "Epoch: 8750\tTrain loss: 8.764211647833386\n",
      "Epoch: 8800\tTrain loss: 8.4432024260362\n",
      "Epoch: 8850\tTrain loss: 8.195312837759653\n",
      "Epoch: 8900\tTrain loss: 8.096011767784754\n",
      "Epoch: 8950\tTrain loss: 10.881462454795837\n",
      "Epoch: 9000\tTrain loss: 7.499560574690501\n",
      "Epoch: 9050\tTrain loss: 8.079923138022423\n",
      "Epoch: 9100\tTrain loss: 9.047744254271189\n",
      "Epoch: 9150\tTrain loss: 6.676432728767395\n",
      "Epoch: 9200\tTrain loss: 6.417457327246666\n",
      "Epoch: 9250\tTrain loss: 6.496437609195709\n",
      "Epoch: 9300\tTrain loss: 7.675753037134807\n",
      "Epoch: 9350\tTrain loss: 7.706397692362468\n",
      "Epoch: 9400\tTrain loss: 23.79927158355713\n",
      "Epoch: 9450\tTrain loss: 10.11434038480123\n",
      "Epoch: 9500\tTrain loss: 4.826513042052587\n",
      "Epoch: 9550\tTrain loss: 4.557633578777313\n",
      "Epoch: 9600\tTrain loss: 4.309904292225838\n",
      "Epoch: 9650\tTrain loss: 4.057263761758804\n",
      "Epoch: 9700\tTrain loss: 3.784278313318888\n",
      "Epoch: 9750\tTrain loss: 7.940913374225299\n",
      "Epoch: 9800\tTrain loss: 3.333684802055359\n",
      "Epoch: 9850\tTrain loss: 3.5463368743658066\n",
      "Epoch: 9900\tTrain loss: 2.5758361419041953\n",
      "Epoch: 9950\tTrain loss: 2.3466297344615064\n",
      "Training CFRNN\n",
      "Epoch: 0\tTrain loss: 1760207.3958333333\n",
      "Epoch: 50\tTrain loss: 1683815.0416666667\n",
      "Epoch: 100\tTrain loss: 1614734.3125\n",
      "Epoch: 150\tTrain loss: 1549185.6666666667\n",
      "Epoch: 200\tTrain loss: 1486504.6770833333\n",
      "Epoch: 250\tTrain loss: 1426707.5\n",
      "Epoch: 300\tTrain loss: 1369236.65625\n",
      "Epoch: 350\tTrain loss: 1314188.6536458333\n",
      "Epoch: 400\tTrain loss: 1261309.7916666667\n",
      "Epoch: 450\tTrain loss: 1210718.21875\n",
      "Epoch: 500\tTrain loss: 1161858.25\n",
      "Epoch: 550\tTrain loss: 1114881.125\n",
      "Epoch: 600\tTrain loss: 1070200.625\n",
      "Epoch: 650\tTrain loss: 1027241.0625\n",
      "Epoch: 700\tTrain loss: 986008.46875\n",
      "Epoch: 750\tTrain loss: 946667.0885416666\n",
      "Epoch: 800\tTrain loss: 909196.84375\n",
      "Epoch: 850\tTrain loss: 873197.8802083334\n",
      "Epoch: 900\tTrain loss: 838808.2083333334\n",
      "Epoch: 950\tTrain loss: 805979.4055989584\n",
      "Epoch: 1000\tTrain loss: 774909.2682291666\n",
      "Epoch: 1050\tTrain loss: 745261.2604166666\n",
      "Epoch: 1100\tTrain loss: 717174.1666666666\n",
      "Epoch: 1150\tTrain loss: 690441.8723958334\n",
      "Epoch: 1200\tTrain loss: 664966.7916666666\n",
      "Epoch: 1250\tTrain loss: 641025.4010416666\n",
      "Epoch: 1300\tTrain loss: 618407.3567708334\n",
      "Epoch: 1350\tTrain loss: 597066.7708333334\n",
      "Epoch: 1400\tTrain loss: 576878.2604166666\n",
      "Epoch: 1450\tTrain loss: 558009.7317708334\n",
      "Epoch: 1500\tTrain loss: 540327.5052083334\n",
      "Epoch: 1550\tTrain loss: 523756.0833333333\n",
      "Epoch: 1600\tTrain loss: 508215.7760416667\n",
      "Epoch: 1650\tTrain loss: 493816.1666666667\n",
      "Epoch: 1700\tTrain loss: 480266.3776041667\n",
      "Epoch: 1750\tTrain loss: 467816.625\n",
      "Epoch: 1800\tTrain loss: 456288.5494791667\n",
      "Epoch: 1850\tTrain loss: 445545.0338541667\n",
      "Epoch: 1900\tTrain loss: 435713.234375\n",
      "Epoch: 1950\tTrain loss: 426705.5859375\n",
      "Epoch: 2000\tTrain loss: 418463.2421875\n",
      "Epoch: 2050\tTrain loss: 410977.765625\n",
      "Epoch: 2100\tTrain loss: 404094.7825520833\n",
      "Epoch: 2150\tTrain loss: 397853.7552083333\n",
      "Epoch: 2200\tTrain loss: 369721.25390625\n",
      "Epoch: 2250\tTrain loss: 290968.06997680664\n",
      "Epoch: 2300\tTrain loss: 275396.48013242084\n",
      "Epoch: 2350\tTrain loss: 261006.91145833334\n",
      "Epoch: 2400\tTrain loss: 247431.6473388672\n",
      "Epoch: 2450\tTrain loss: 235007.767578125\n",
      "Epoch: 2500\tTrain loss: 223309.919921875\n",
      "Epoch: 2550\tTrain loss: 211741.91861979166\n",
      "Epoch: 2600\tTrain loss: 199575.76578521729\n",
      "Epoch: 2650\tTrain loss: 188427.25553385416\n",
      "Epoch: 2700\tTrain loss: 177898.56371307373\n",
      "Epoch: 2750\tTrain loss: 167902.4611860911\n",
      "Epoch: 2800\tTrain loss: 158480.05488077799\n",
      "Epoch: 2850\tTrain loss: 149539.5597330729\n",
      "Epoch: 2900\tTrain loss: 141134.6009902954\n",
      "Epoch: 2950\tTrain loss: 133211.34348042807\n",
      "Epoch: 3000\tTrain loss: 125786.6842142741\n",
      "Epoch: 3050\tTrain loss: 118765.4247233073\n",
      "Epoch: 3100\tTrain loss: 112199.54209391277\n",
      "Epoch: 3150\tTrain loss: 105901.06993357341\n",
      "Epoch: 3200\tTrain loss: 99280.2167409261\n",
      "Epoch: 3250\tTrain loss: 93027.75934791565\n",
      "Epoch: 3300\tTrain loss: 87056.62703196208\n",
      "Epoch: 3350\tTrain loss: 81408.54340171814\n",
      "Epoch: 3400\tTrain loss: 75881.32146962483\n",
      "Epoch: 3450\tTrain loss: 70679.13871892293\n",
      "Epoch: 3500\tTrain loss: 65715.71373303731\n",
      "Epoch: 3550\tTrain loss: 61074.715240478516\n",
      "Epoch: 3600\tTrain loss: 56498.793144861855\n",
      "Epoch: 3650\tTrain loss: 52256.7412109375\n",
      "Epoch: 3700\tTrain loss: 48209.29347674052\n",
      "Epoch: 3750\tTrain loss: 44435.97411092123\n",
      "Epoch: 3800\tTrain loss: 40944.58488591512\n",
      "Epoch: 3850\tTrain loss: 37581.10368537903\n",
      "Epoch: 3900\tTrain loss: 34455.06922785441\n",
      "Epoch: 3950\tTrain loss: 31552.985991160076\n",
      "Epoch: 4000\tTrain loss: 28848.763465563457\n",
      "Epoch: 4050\tTrain loss: 26332.20375951131\n",
      "Epoch: 4100\tTrain loss: 24051.386506398518\n",
      "Epoch: 4150\tTrain loss: 21932.893683115642\n",
      "Epoch: 4200\tTrain loss: 19995.820682525635\n",
      "Epoch: 4250\tTrain loss: 18245.604360262554\n",
      "Epoch: 4300\tTrain loss: 16666.68149439494\n",
      "Epoch: 4350\tTrain loss: 15241.216855684916\n",
      "Epoch: 4400\tTrain loss: 13970.353331247965\n",
      "Epoch: 4450\tTrain loss: 12836.578578313192\n",
      "Epoch: 4500\tTrain loss: 11825.3701171875\n",
      "Epoch: 4550\tTrain loss: 10955.854178110758\n",
      "Epoch: 4600\tTrain loss: 10207.960747400919\n",
      "Epoch: 4650\tTrain loss: 9546.638498624166\n",
      "Epoch: 4700\tTrain loss: 8980.170181274414\n",
      "Epoch: 4750\tTrain loss: 7945.790262858073\n",
      "Epoch: 4800\tTrain loss: 7046.389545440674\n",
      "Epoch: 4850\tTrain loss: 6244.213803132375\n",
      "Epoch: 4900\tTrain loss: 5557.880237579346\n",
      "Epoch: 4950\tTrain loss: 4881.7479731241865\n",
      "Epoch: 5000\tTrain loss: 4304.873241424561\n",
      "Epoch: 5050\tTrain loss: 3725.6284364064536\n",
      "Epoch: 5100\tTrain loss: 3218.78316561381\n",
      "Epoch: 5150\tTrain loss: 2792.414026260376\n",
      "Epoch: 5200\tTrain loss: 2371.7678451538086\n",
      "Epoch: 5250\tTrain loss: 2021.738876024882\n",
      "Epoch: 5300\tTrain loss: 1693.4167722066243\n",
      "Epoch: 5350\tTrain loss: 1701.1524976094563\n",
      "Epoch: 5400\tTrain loss: 1178.6226539611816\n",
      "Epoch: 5450\tTrain loss: 963.4724604288737\n",
      "Epoch: 5500\tTrain loss: 783.445894241333\n",
      "Epoch: 5550\tTrain loss: 631.1170585155487\n",
      "Epoch: 5600\tTrain loss: 506.16353193918866\n",
      "Epoch: 5650\tTrain loss: 402.41275215148926\n",
      "Epoch: 5700\tTrain loss: 319.81559952100116\n",
      "Epoch: 5750\tTrain loss: 254.21219889322916\n",
      "Epoch: 5800\tTrain loss: 202.4852066040039\n",
      "Epoch: 5850\tTrain loss: 163.44854990641275\n",
      "Epoch: 5900\tTrain loss: 134.32026227315268\n",
      "Epoch: 5950\tTrain loss: 123.16263516743977\n",
      "Epoch: 6000\tTrain loss: 100.43052212397258\n",
      "Epoch: 6050\tTrain loss: 87.94286235173543\n",
      "Epoch: 6100\tTrain loss: 78.93310165405273\n",
      "Epoch: 6150\tTrain loss: 72.54902633031209\n",
      "Epoch: 6200\tTrain loss: 67.84760840733846\n",
      "Epoch: 6250\tTrain loss: 64.24209213256836\n",
      "Epoch: 6300\tTrain loss: 61.382290045420326\n",
      "Epoch: 6350\tTrain loss: 69.31478436787923\n",
      "Epoch: 6400\tTrain loss: 56.78566106160482\n",
      "Epoch: 6450\tTrain loss: 54.446743647257485\n",
      "Epoch: 6500\tTrain loss: 52.29923311869303\n",
      "Epoch: 6550\tTrain loss: 50.1424872080485\n",
      "Epoch: 6600\tTrain loss: 48.0236021677653\n",
      "Epoch: 6650\tTrain loss: 45.897740046183266\n",
      "Epoch: 6700\tTrain loss: 43.77429326375326\n",
      "Epoch: 6750\tTrain loss: 41.60429811477661\n",
      "Epoch: 6800\tTrain loss: 39.443377335866295\n",
      "Epoch: 6850\tTrain loss: 37.273965199788414\n",
      "Epoch: 6900\tTrain loss: 35.105368296305336\n",
      "Epoch: 6950\tTrain loss: 36.824618657430015\n",
      "Epoch: 7000\tTrain loss: 57.54458491007487\n",
      "Epoch: 7050\tTrain loss: 29.238879521687824\n",
      "Epoch: 7100\tTrain loss: 27.19526735941569\n",
      "Epoch: 7150\tTrain loss: 25.265706062316895\n",
      "Epoch: 7200\tTrain loss: 23.354631741841633\n",
      "Epoch: 7250\tTrain loss: 21.527620315551758\n",
      "Epoch: 7300\tTrain loss: 19.737826347351074\n",
      "Epoch: 7350\tTrain loss: 18.03415584564209\n",
      "Epoch: 7400\tTrain loss: 16.572320302327473\n",
      "Epoch: 7450\tTrain loss: 15.087327559789022\n",
      "Epoch: 7500\tTrain loss: 14.082148551940918\n",
      "Epoch: 7550\tTrain loss: 12.158294041951498\n",
      "Epoch: 7600\tTrain loss: 12.214400291442871\n",
      "Epoch: 7650\tTrain loss: 12.818142890930176\n",
      "Epoch: 7700\tTrain loss: 9.411580324172974\n",
      "Epoch: 7750\tTrain loss: 8.403100530306498\n",
      "Epoch: 7800\tTrain loss: 7.493555227915446\n",
      "Epoch: 7850\tTrain loss: 6.6648538907368975\n",
      "Epoch: 7900\tTrain loss: 5.918043474356334\n",
      "Epoch: 7950\tTrain loss: 5.251334190368652\n",
      "Epoch: 8000\tTrain loss: 4.6434076229731245\n",
      "Epoch: 8050\tTrain loss: 5.329662720362346\n",
      "Epoch: 8100\tTrain loss: 3.794362942377726\n",
      "Epoch: 8150\tTrain loss: 3.3974159161249795\n",
      "Epoch: 8200\tTrain loss: 3.3526872396469116\n",
      "Epoch: 8250\tTrain loss: 2.723822514216105\n",
      "Epoch: 8300\tTrain loss: 2.4276627699534097\n",
      "Epoch: 8350\tTrain loss: 2.156115929285685\n",
      "Epoch: 8400\tTrain loss: 1.8998632729053497\n",
      "Epoch: 8450\tTrain loss: 1.662316823999087\n",
      "Epoch: 8500\tTrain loss: 1.434746379032731\n",
      "Epoch: 8550\tTrain loss: 1.2224549651145935\n",
      "Epoch: 8600\tTrain loss: 1.0219289362430573\n",
      "Epoch: 8650\tTrain loss: 0.8395023345947266\n",
      "Epoch: 8700\tTrain loss: 0.673939103881518\n",
      "Epoch: 8750\tTrain loss: 0.529456153512001\n",
      "Epoch: 8800\tTrain loss: 1.7956661780675252\n",
      "Epoch: 8850\tTrain loss: 0.8659212724305689\n",
      "Epoch: 8900\tTrain loss: 0.28722672288616496\n",
      "Epoch: 8950\tTrain loss: 0.2536707619825999\n",
      "Epoch: 9000\tTrain loss: 5.951141119003296\n",
      "Epoch: 9050\tTrain loss: 0.9722546140352885\n",
      "Epoch: 9100\tTrain loss: 0.09285925577084224\n",
      "Epoch: 9150\tTrain loss: 0.07066281822820504\n",
      "Epoch: 9200\tTrain loss: 0.054716032619277634\n",
      "Epoch: 9250\tTrain loss: 0.04221844032872468\n",
      "Epoch: 9300\tTrain loss: 0.03222744073718786\n",
      "Epoch: 9350\tTrain loss: 0.024225985223893076\n",
      "Epoch: 9400\tTrain loss: 0.017572787047053378\n",
      "Epoch: 9450\tTrain loss: 0.012553565048923096\n",
      "Epoch: 9500\tTrain loss: 0.008746234389642874\n",
      "Epoch: 9550\tTrain loss: 0.005959496988604466\n",
      "Epoch: 9600\tTrain loss: 0.003968579578213394\n",
      "Epoch: 9650\tTrain loss: 0.0025968519233477614\n",
      "Epoch: 9700\tTrain loss: 0.0017528441579391558\n",
      "Epoch: 9750\tTrain loss: 0.44488314042488736\n",
      "Epoch: 9800\tTrain loss: 6.238999704519908\n",
      "Epoch: 9850\tTrain loss: 7.146116693814595\n",
      "Epoch: 9900\tTrain loss: 0.025434912686857086\n",
      "Epoch: 9950\tTrain loss: 0.0014029767141134168\n",
      "Training CFRNN\n",
      "Epoch: 0\tTrain loss: 1656984.3854166667\n",
      "Epoch: 50\tTrain loss: 1581262.375\n",
      "Epoch: 100\tTrain loss: 1513042.3645833333\n",
      "Epoch: 150\tTrain loss: 1448572.3125\n",
      "Epoch: 200\tTrain loss: 1387139.0833333333\n",
      "Epoch: 250\tTrain loss: 1328056.5520833333\n",
      "Epoch: 300\tTrain loss: 1271481.28125\n",
      "Epoch: 350\tTrain loss: 1217417.4479166667\n",
      "Epoch: 400\tTrain loss: 1165418.15625\n",
      "Epoch: 450\tTrain loss: 1115609.9166666667\n",
      "Epoch: 500\tTrain loss: 1067948.5729166667\n",
      "Epoch: 550\tTrain loss: 1022514.734375\n",
      "Epoch: 600\tTrain loss: 978841.6302083334\n",
      "Epoch: 650\tTrain loss: 937076.734375\n",
      "Epoch: 700\tTrain loss: 897128.9166666666\n",
      "Epoch: 750\tTrain loss: 858875.4778645834\n",
      "Epoch: 800\tTrain loss: 822485.6614583334\n",
      "Epoch: 850\tTrain loss: 788031.0130208334\n",
      "Epoch: 900\tTrain loss: 754907.1510416666\n",
      "Epoch: 950\tTrain loss: 723481.4736328125\n",
      "Epoch: 1000\tTrain loss: 693612.1979166666\n",
      "Epoch: 1050\tTrain loss: 665274.2600097656\n",
      "Epoch: 1100\tTrain loss: 638474.8958333334\n",
      "Epoch: 1150\tTrain loss: 613130.6666666666\n",
      "Epoch: 1200\tTrain loss: 589253.9103597006\n",
      "Epoch: 1250\tTrain loss: 566507.6432291666\n",
      "Epoch: 1300\tTrain loss: 545147.7591145834\n",
      "Epoch: 1350\tTrain loss: 524967.6217447916\n",
      "Epoch: 1400\tTrain loss: 506000.3961588542\n",
      "Epoch: 1450\tTrain loss: 488243.1653645833\n",
      "Epoch: 1500\tTrain loss: 471617.7734375\n",
      "Epoch: 1550\tTrain loss: 456181.2786458333\n",
      "Epoch: 1600\tTrain loss: 441756.3307291667\n",
      "Epoch: 1650\tTrain loss: 428290.8541666667\n",
      "Epoch: 1700\tTrain loss: 415892.7376302083\n",
      "Epoch: 1750\tTrain loss: 404316.818359375\n",
      "Epoch: 1800\tTrain loss: 393809.5572916667\n",
      "Epoch: 1850\tTrain loss: 384097.6796875\n",
      "Epoch: 1900\tTrain loss: 375113.1614583333\n",
      "Epoch: 1950\tTrain loss: 367037.8880208333\n",
      "Epoch: 2000\tTrain loss: 359711.7395833333\n",
      "Epoch: 2050\tTrain loss: 352975.04296875\n",
      "Epoch: 2100\tTrain loss: 347004.2337239583\n",
      "Epoch: 2150\tTrain loss: 341544.1084798177\n",
      "Epoch: 2200\tTrain loss: 336659.0768229167\n",
      "Epoch: 2250\tTrain loss: 332312.3229166667\n",
      "Epoch: 2300\tTrain loss: 328364.7838541667\n",
      "Epoch: 2350\tTrain loss: 296137.087890625\n",
      "Epoch: 2400\tTrain loss: 257421.57299804688\n",
      "Epoch: 2450\tTrain loss: 245409.30255889893\n",
      "Epoch: 2500\tTrain loss: 233788.33737182617\n",
      "Epoch: 2550\tTrain loss: 222782.50303141275\n",
      "Epoch: 2600\tTrain loss: 211887.6045074463\n",
      "Epoch: 2650\tTrain loss: 201563.70172627768\n",
      "Epoch: 2700\tTrain loss: 191475.5485636393\n",
      "Epoch: 2750\tTrain loss: 181763.03665924072\n",
      "Epoch: 2800\tTrain loss: 172356.20114898682\n",
      "Epoch: 2850\tTrain loss: 163298.038907369\n",
      "Epoch: 2900\tTrain loss: 154544.0552037557\n",
      "Epoch: 2950\tTrain loss: 146052.2702585856\n",
      "Epoch: 3000\tTrain loss: 137999.6935373942\n",
      "Epoch: 3050\tTrain loss: 130046.7835667928\n",
      "Epoch: 3100\tTrain loss: 122573.53270467122\n",
      "Epoch: 3150\tTrain loss: 115221.62354532878\n",
      "Epoch: 3200\tTrain loss: 108246.0183283488\n",
      "Epoch: 3250\tTrain loss: 101519.01221720378\n",
      "Epoch: 3300\tTrain loss: 95140.98674519856\n",
      "Epoch: 3350\tTrain loss: 88967.7898127238\n",
      "Epoch: 3400\tTrain loss: 83059.47546005249\n",
      "Epoch: 3450\tTrain loss: 77470.53226343791\n",
      "Epoch: 3500\tTrain loss: 72117.466091156\n",
      "Epoch: 3550\tTrain loss: 66982.78904469807\n",
      "Epoch: 3600\tTrain loss: 62139.58618736267\n",
      "Epoch: 3650\tTrain loss: 57622.344568888344\n",
      "Epoch: 3700\tTrain loss: 53215.372634887695\n",
      "Epoch: 3750\tTrain loss: 49170.42079671224\n",
      "Epoch: 3800\tTrain loss: 45273.54210154215\n",
      "Epoch: 3850\tTrain loss: 41672.44525400797\n",
      "Epoch: 3900\tTrain loss: 38260.77527618408\n",
      "Epoch: 3950\tTrain loss: 35100.03617477417\n",
      "Epoch: 4000\tTrain loss: 32154.370985666912\n",
      "Epoch: 4050\tTrain loss: 29420.652605692547\n",
      "Epoch: 4100\tTrain loss: 26878.68198267619\n",
      "Epoch: 4150\tTrain loss: 24537.536571502686\n",
      "Epoch: 4200\tTrain loss: 22385.154956817627\n",
      "Epoch: 4250\tTrain loss: 20446.470989227295\n",
      "Epoch: 4300\tTrain loss: 18657.837056477863\n",
      "Epoch: 4350\tTrain loss: 17053.48203531901\n",
      "Epoch: 4400\tTrain loss: 15697.372385660807\n",
      "Epoch: 4450\tTrain loss: 14292.73864110311\n",
      "Epoch: 4500\tTrain loss: 13107.266705195108\n",
      "Epoch: 4550\tTrain loss: 12084.501597086588\n",
      "Epoch: 4600\tTrain loss: 11175.171069463095\n",
      "Epoch: 4650\tTrain loss: 10397.15027999878\n",
      "Epoch: 4700\tTrain loss: 9686.22675704956\n",
      "Epoch: 4750\tTrain loss: 8603.435265858969\n",
      "Epoch: 4800\tTrain loss: 7780.782178243001\n",
      "Epoch: 4850\tTrain loss: 6925.048175811768\n",
      "Epoch: 4900\tTrain loss: 6181.367114384969\n",
      "Epoch: 4950\tTrain loss: 5486.355312347412\n",
      "Epoch: 5000\tTrain loss: 4835.556023279826\n",
      "Epoch: 5050\tTrain loss: 4247.21595509847\n",
      "Epoch: 5100\tTrain loss: 3700.8256549835205\n",
      "Epoch: 5150\tTrain loss: 3204.5235691070557\n",
      "Epoch: 5200\tTrain loss: 2831.017583211263\n",
      "Epoch: 5250\tTrain loss: 2388.2880859375\n",
      "Epoch: 5300\tTrain loss: 2004.3568026224773\n",
      "Epoch: 5350\tTrain loss: 1682.2496795654297\n",
      "Epoch: 5400\tTrain loss: 1400.8410024642944\n",
      "Epoch: 5450\tTrain loss: 1160.9506397247314\n",
      "Epoch: 5500\tTrain loss: 954.951068242391\n",
      "Epoch: 5550\tTrain loss: 774.3474828402201\n",
      "Epoch: 5600\tTrain loss: 674.0310618082682\n",
      "Epoch: 5650\tTrain loss: 539.7969462076823\n",
      "Epoch: 5700\tTrain loss: 402.1578420003255\n",
      "Epoch: 5750\tTrain loss: 320.15728251139325\n",
      "Epoch: 5800\tTrain loss: 262.644079208374\n",
      "Epoch: 5850\tTrain loss: 204.29983774820963\n",
      "Epoch: 5900\tTrain loss: 164.7742830912272\n",
      "Epoch: 5950\tTrain loss: 146.45230356852213\n",
      "Epoch: 6000\tTrain loss: 114.59609031677246\n",
      "Epoch: 6050\tTrain loss: 98.72806040445964\n",
      "Epoch: 6100\tTrain loss: 192.44442749023438\n",
      "Epoch: 6150\tTrain loss: 82.25805314381917\n",
      "Epoch: 6200\tTrain loss: 74.54648971557617\n",
      "Epoch: 6250\tTrain loss: 77.87149842580159\n",
      "Epoch: 6300\tTrain loss: 151.13089497884116\n",
      "Epoch: 6350\tTrain loss: 63.115738232930504\n",
      "Epoch: 6400\tTrain loss: 60.69093449910482\n",
      "Epoch: 6450\tTrain loss: 58.43016274770101\n",
      "Epoch: 6500\tTrain loss: 88.68716939290364\n",
      "Epoch: 6550\tTrain loss: 55.76414489746094\n",
      "Epoch: 6600\tTrain loss: 52.27713394165039\n",
      "Epoch: 6650\tTrain loss: 50.21926975250244\n",
      "Epoch: 6700\tTrain loss: 91.7558364868164\n",
      "Epoch: 6750\tTrain loss: 46.92569224039713\n",
      "Epoch: 6800\tTrain loss: 60.081746419270836\n",
      "Epoch: 6850\tTrain loss: 56.13874944051107\n",
      "Epoch: 6900\tTrain loss: 40.665544191996254\n",
      "Epoch: 6950\tTrain loss: 38.516242345174156\n",
      "Epoch: 7000\tTrain loss: 36.67259089152018\n",
      "Epoch: 7050\tTrain loss: 34.823595682779946\n",
      "Epoch: 7100\tTrain loss: 36.48781522115072\n",
      "Epoch: 7150\tTrain loss: 32.214585622151695\n",
      "Epoch: 7200\tTrain loss: 35.71324857076009\n",
      "Epoch: 7250\tTrain loss: 89.11109606424968\n",
      "Epoch: 7300\tTrain loss: 31.38471221923828\n",
      "Epoch: 7350\tTrain loss: 25.3729461034139\n",
      "Epoch: 7400\tTrain loss: 24.05732997258504\n",
      "Epoch: 7450\tTrain loss: 22.85541025797526\n",
      "Epoch: 7500\tTrain loss: 21.80715258916219\n",
      "Epoch: 7550\tTrain loss: 21.425683975219727\n",
      "Epoch: 7600\tTrain loss: 28.51185131072998\n",
      "Epoch: 7650\tTrain loss: 60.63839022318522\n",
      "Epoch: 7700\tTrain loss: 18.885599613189697\n",
      "Epoch: 7750\tTrain loss: 17.802366097768147\n",
      "Epoch: 7800\tTrain loss: 17.2384618918101\n",
      "Epoch: 7850\tTrain loss: 16.724406401316326\n",
      "Epoch: 7900\tTrain loss: 16.24688919385274\n",
      "Epoch: 7950\tTrain loss: 15.897466818491617\n",
      "Epoch: 8000\tTrain loss: 15.911093552907309\n",
      "Epoch: 8050\tTrain loss: 15.627410014470419\n",
      "Epoch: 8100\tTrain loss: 20.524083614349365\n",
      "Epoch: 8150\tTrain loss: 16.598634680112202\n",
      "Epoch: 8200\tTrain loss: 16.059342622756958\n",
      "Epoch: 8250\tTrain loss: 92.05101140340169\n",
      "Epoch: 8300\tTrain loss: 14.13868522644043\n",
      "Epoch: 8350\tTrain loss: 17.694172382354736\n",
      "Epoch: 8400\tTrain loss: 13.585983792940775\n",
      "Epoch: 8450\tTrain loss: 13.334522068500519\n",
      "Epoch: 8500\tTrain loss: 13.124140620231628\n",
      "Epoch: 8550\tTrain loss: 13.166166973300278\n",
      "Epoch: 8600\tTrain loss: 19.211244225502014\n",
      "Epoch: 8650\tTrain loss: 12.519229595238963\n",
      "Epoch: 8700\tTrain loss: 14.031948924064636\n",
      "Epoch: 8750\tTrain loss: 23.800639788309734\n",
      "Epoch: 8800\tTrain loss: 15.037915276984373\n",
      "Epoch: 8850\tTrain loss: 11.723081338956641\n",
      "Epoch: 8900\tTrain loss: 11.525663534800211\n",
      "Epoch: 8950\tTrain loss: 11.318962395191193\n",
      "Epoch: 9000\tTrain loss: 11.111810346444448\n",
      "Epoch: 9050\tTrain loss: 10.915594842711775\n",
      "Epoch: 9100\tTrain loss: 10.686116615931192\n",
      "Epoch: 9150\tTrain loss: 10.461083233356476\n",
      "Epoch: 9200\tTrain loss: 85.44177309672038\n",
      "Epoch: 9250\tTrain loss: 10.058473765850067\n",
      "Epoch: 9300\tTrain loss: 9.839799056450525\n",
      "Epoch: 9350\tTrain loss: 9.916884263356527\n",
      "Epoch: 9400\tTrain loss: 9.475621355503486\n",
      "Epoch: 9450\tTrain loss: 9.33200075229009\n",
      "Epoch: 9500\tTrain loss: 36.846322456995644\n",
      "Epoch: 9550\tTrain loss: 8.925958196322123\n",
      "Epoch: 9600\tTrain loss: 8.642001430193583\n",
      "Epoch: 9650\tTrain loss: 8.402084320783615\n",
      "Epoch: 9700\tTrain loss: 8.219580010821423\n",
      "Epoch: 9750\tTrain loss: 10.008819222450256\n",
      "Epoch: 9800\tTrain loss: 46.5795046488444\n",
      "Epoch: 9850\tTrain loss: 7.960587978363037\n",
      "Epoch: 9900\tTrain loss: 7.48907196521759\n",
      "Epoch: 9950\tTrain loss: 7.1594706773757935\n",
      "Training CFRNN\n",
      "Epoch: 0\tTrain loss: 4748324.34375\n",
      "Epoch: 50\tTrain loss: 4658193.604166667\n",
      "Epoch: 100\tTrain loss: 4575948.0625\n",
      "Epoch: 150\tTrain loss: 4497063.666666667\n",
      "Epoch: 200\tTrain loss: 4420887.354166667\n",
      "Epoch: 250\tTrain loss: 4347418.822916667\n",
      "Epoch: 300\tTrain loss: 4276133.447916667\n",
      "Epoch: 350\tTrain loss: 4206814.552083333\n",
      "Epoch: 400\tTrain loss: 4139971.1875\n",
      "Epoch: 450\tTrain loss: 4074893.3229166665\n",
      "Epoch: 500\tTrain loss: 4010981.9895833335\n",
      "Epoch: 550\tTrain loss: 3949575.8723958335\n",
      "Epoch: 600\tTrain loss: 3889805.2630208335\n",
      "Epoch: 650\tTrain loss: 3831432.65625\n",
      "Epoch: 700\tTrain loss: 3774662.3567708335\n",
      "Epoch: 750\tTrain loss: 3719984.1666666665\n",
      "Epoch: 800\tTrain loss: 3666516.390625\n",
      "Epoch: 850\tTrain loss: 3614775.75\n",
      "Epoch: 900\tTrain loss: 3564717.3046875\n",
      "Epoch: 950\tTrain loss: 3515968.4114583335\n",
      "Epoch: 1000\tTrain loss: 3468738.6295572915\n",
      "Epoch: 1050\tTrain loss: 3422547.0520833335\n",
      "Epoch: 1100\tTrain loss: 3378228.1319986978\n",
      "Epoch: 1150\tTrain loss: 3335518.4401041665\n",
      "Epoch: 1200\tTrain loss: 3293660.8723958335\n",
      "Epoch: 1250\tTrain loss: 3253210.1588541665\n",
      "Epoch: 1300\tTrain loss: 3214342.7213541665\n",
      "Epoch: 1350\tTrain loss: 3176642.289754232\n",
      "Epoch: 1400\tTrain loss: 3139883.1328125\n",
      "Epoch: 1450\tTrain loss: 3104952.9055989585\n",
      "Epoch: 1500\tTrain loss: 3070570.4407552085\n",
      "Epoch: 1550\tTrain loss: 3037489.489908854\n",
      "Epoch: 1600\tTrain loss: 3006006.159830729\n",
      "Epoch: 1650\tTrain loss: 2975163.7747395835\n",
      "Epoch: 1700\tTrain loss: 2945442.671875\n",
      "Epoch: 1750\tTrain loss: 2917157.1080729165\n",
      "Epoch: 1800\tTrain loss: 2889792.2220052085\n",
      "Epoch: 1850\tTrain loss: 2863308.814453125\n",
      "Epoch: 1900\tTrain loss: 2837851.1106770835\n",
      "Epoch: 1950\tTrain loss: 2813378.3424479165\n",
      "Epoch: 2000\tTrain loss: 2790075.6614583335\n",
      "Epoch: 2050\tTrain loss: 2767400.42578125\n",
      "Epoch: 2100\tTrain loss: 2745817.4361979165\n",
      "Epoch: 2150\tTrain loss: 2725260.8618164062\n",
      "Epoch: 2200\tTrain loss: 2705251.6888020835\n",
      "Epoch: 2250\tTrain loss: 2686382.851969401\n",
      "Epoch: 2300\tTrain loss: 2667952.7369791665\n",
      "Epoch: 2350\tTrain loss: 2650732.4680989585\n",
      "Epoch: 2400\tTrain loss: 2634104.9270833335\n",
      "Epoch: 2450\tTrain loss: 2618244.4817708335\n",
      "Epoch: 2500\tTrain loss: 2603269.1243489585\n",
      "Epoch: 2550\tTrain loss: 2588752.5286458335\n",
      "Epoch: 2600\tTrain loss: 2561376.220703125\n",
      "Epoch: 2650\tTrain loss: 2521008.3307291665\n",
      "Epoch: 2700\tTrain loss: 2464152.465372721\n",
      "Epoch: 2750\tTrain loss: 2437963.3559570312\n",
      "Epoch: 2800\tTrain loss: 2413582.269897461\n",
      "Epoch: 2850\tTrain loss: 2389983.871632894\n",
      "Epoch: 2900\tTrain loss: 2366859.8290405273\n",
      "Epoch: 2950\tTrain loss: 2344277.055948893\n",
      "Epoch: 3000\tTrain loss: 2321355.557535807\n",
      "Epoch: 3050\tTrain loss: 2299191.537902832\n",
      "Epoch: 3100\tTrain loss: 2277248.9557902017\n",
      "Epoch: 3150\tTrain loss: 2255472.4990844727\n",
      "Epoch: 3200\tTrain loss: 2233854.7267608643\n",
      "Epoch: 3250\tTrain loss: 2212340.36689504\n",
      "Epoch: 3300\tTrain loss: 2191000.780995687\n",
      "Epoch: 3350\tTrain loss: 2169799.944883982\n",
      "Epoch: 3400\tTrain loss: 2148640.282928467\n",
      "Epoch: 3450\tTrain loss: 2127689.552670797\n",
      "Epoch: 3500\tTrain loss: 2106827.3185628257\n",
      "Epoch: 3550\tTrain loss: 2085907.5150871277\n",
      "Epoch: 3600\tTrain loss: 2065253.0325826008\n",
      "Epoch: 3650\tTrain loss: 2044495.9238001506\n",
      "Epoch: 3700\tTrain loss: 2024239.7313041687\n",
      "Epoch: 3750\tTrain loss: 2003753.4069595337\n",
      "Epoch: 3800\tTrain loss: 1983611.3723169963\n",
      "Epoch: 3850\tTrain loss: 1963338.9465332031\n",
      "Epoch: 3900\tTrain loss: 1943364.123615265\n",
      "Epoch: 3950\tTrain loss: 1923222.5597279866\n",
      "Epoch: 4000\tTrain loss: 1903709.0015920002\n",
      "Epoch: 4050\tTrain loss: 1883812.0259780884\n",
      "Epoch: 4100\tTrain loss: 1864206.5026117961\n",
      "Epoch: 4150\tTrain loss: 1844594.8933321636\n",
      "Epoch: 4200\tTrain loss: 1825256.0319391887\n",
      "Epoch: 4250\tTrain loss: 1805866.586429596\n",
      "Epoch: 4300\tTrain loss: 1786905.0892028809\n",
      "Epoch: 4350\tTrain loss: 1767521.0960591633\n",
      "Epoch: 4400\tTrain loss: 1748928.4051717122\n",
      "Epoch: 4450\tTrain loss: 1729644.789738973\n",
      "Epoch: 4500\tTrain loss: 1711014.881708781\n",
      "Epoch: 4550\tTrain loss: 1692340.856956482\n",
      "Epoch: 4600\tTrain loss: 1673566.1634775798\n",
      "Epoch: 4650\tTrain loss: 1655317.6728057861\n",
      "Epoch: 4700\tTrain loss: 1636784.1411743164\n",
      "Epoch: 4750\tTrain loss: 1618432.400941213\n",
      "Epoch: 4800\tTrain loss: 1600473.641904195\n",
      "Epoch: 4850\tTrain loss: 1582251.316664378\n",
      "Epoch: 4900\tTrain loss: 1564560.626876831\n",
      "Epoch: 4950\tTrain loss: 1546628.558458964\n",
      "Epoch: 5000\tTrain loss: 1528856.1013641357\n",
      "Epoch: 5050\tTrain loss: 1511461.6354662578\n",
      "Epoch: 5100\tTrain loss: 1493806.9219411213\n",
      "Epoch: 5150\tTrain loss: 1476227.059890747\n",
      "Epoch: 5200\tTrain loss: 1459119.8539263408\n",
      "Epoch: 5250\tTrain loss: 1441796.0379066467\n",
      "Epoch: 5300\tTrain loss: 1424917.849609375\n",
      "Epoch: 5350\tTrain loss: 1407894.1907755535\n",
      "Epoch: 5400\tTrain loss: 1390981.598350525\n",
      "Epoch: 5450\tTrain loss: 1374123.5463790894\n",
      "Epoch: 5500\tTrain loss: 1357555.68309021\n",
      "Epoch: 5550\tTrain loss: 1340903.7164764404\n",
      "Epoch: 5600\tTrain loss: 1324483.3579190571\n",
      "Epoch: 5650\tTrain loss: 1308601.9104614258\n",
      "Epoch: 5700\tTrain loss: 1291995.2763010662\n",
      "Epoch: 5750\tTrain loss: 1275809.4871699016\n",
      "Epoch: 5800\tTrain loss: 1259783.2214914958\n",
      "Epoch: 5850\tTrain loss: 1243914.5540542603\n",
      "Epoch: 5900\tTrain loss: 1228012.5864639282\n",
      "Epoch: 5950\tTrain loss: 1212354.3655751545\n",
      "Epoch: 6000\tTrain loss: 1196670.1585273743\n",
      "Epoch: 6050\tTrain loss: 1181250.2089970906\n",
      "Epoch: 6100\tTrain loss: 1165938.5834503174\n",
      "Epoch: 6150\tTrain loss: 1150604.075106303\n",
      "Epoch: 6200\tTrain loss: 1135223.0690612793\n",
      "Epoch: 6250\tTrain loss: 1120365.2449913025\n",
      "Epoch: 6300\tTrain loss: 1105265.5309435527\n",
      "Epoch: 6350\tTrain loss: 1090400.5688756306\n",
      "Epoch: 6400\tTrain loss: 1075723.3475812275\n",
      "Epoch: 6450\tTrain loss: 1060988.6723136902\n",
      "Epoch: 6500\tTrain loss: 1046446.0616111755\n",
      "Epoch: 6550\tTrain loss: 1031939.8377628326\n",
      "Epoch: 6600\tTrain loss: 1017755.1880836487\n",
      "Epoch: 6650\tTrain loss: 1003646.4465789795\n",
      "Epoch: 6700\tTrain loss: 989479.0311381022\n",
      "Epoch: 6750\tTrain loss: 975481.7182312012\n",
      "Epoch: 6800\tTrain loss: 961492.7281239828\n",
      "Epoch: 6850\tTrain loss: 947673.2082138062\n",
      "Epoch: 6900\tTrain loss: 933830.4257074991\n",
      "Epoch: 6950\tTrain loss: 920348.6897481283\n",
      "Epoch: 7000\tTrain loss: 906801.6212158203\n",
      "Epoch: 7050\tTrain loss: 893266.8539937338\n",
      "Epoch: 7100\tTrain loss: 879964.9893035889\n",
      "Epoch: 7150\tTrain loss: 866801.5228850046\n",
      "Epoch: 7200\tTrain loss: 853622.6205571493\n",
      "Epoch: 7250\tTrain loss: 840843.8224722544\n",
      "Epoch: 7300\tTrain loss: 827809.1264470419\n",
      "Epoch: 7350\tTrain loss: 815160.5915336609\n",
      "Epoch: 7400\tTrain loss: 802458.2476730347\n",
      "Epoch: 7450\tTrain loss: 789840.3298746744\n",
      "Epoch: 7500\tTrain loss: 777454.2304903666\n",
      "Epoch: 7550\tTrain loss: 764950.3234926859\n",
      "Epoch: 7600\tTrain loss: 752801.957291921\n",
      "Epoch: 7650\tTrain loss: 740633.503625234\n",
      "Epoch: 7700\tTrain loss: 728571.7025235494\n",
      "Epoch: 7750\tTrain loss: 716548.5135389963\n",
      "Epoch: 7800\tTrain loss: 704830.3643277487\n",
      "Epoch: 7850\tTrain loss: 692928.5934931437\n",
      "Epoch: 7900\tTrain loss: 681346.5184396108\n",
      "Epoch: 7950\tTrain loss: 669908.0044924418\n",
      "Epoch: 8000\tTrain loss: 658367.9621454874\n",
      "Epoch: 8050\tTrain loss: 647106.130780538\n",
      "Epoch: 8100\tTrain loss: 635947.1414680481\n",
      "Epoch: 8150\tTrain loss: 624714.5905520121\n",
      "Epoch: 8200\tTrain loss: 613801.0337231954\n",
      "Epoch: 8250\tTrain loss: 602831.5409742991\n",
      "Epoch: 8300\tTrain loss: 592008.1144892374\n",
      "Epoch: 8350\tTrain loss: 581383.6331075033\n",
      "Epoch: 8400\tTrain loss: 570652.0797055563\n",
      "Epoch: 8450\tTrain loss: 560284.3567797343\n",
      "Epoch: 8500\tTrain loss: 549808.6418660482\n",
      "Epoch: 8550\tTrain loss: 539515.9208068848\n",
      "Epoch: 8600\tTrain loss: 529460.6007194519\n",
      "Epoch: 8650\tTrain loss: 519229.36800893146\n",
      "Epoch: 8700\tTrain loss: 509321.1420796712\n",
      "Epoch: 8750\tTrain loss: 499508.02936935425\n",
      "Epoch: 8800\tTrain loss: 489590.9732055664\n",
      "Epoch: 8850\tTrain loss: 479925.2845687866\n",
      "Epoch: 8900\tTrain loss: 470386.6477775574\n",
      "Epoch: 8950\tTrain loss: 461036.1748936971\n",
      "Epoch: 9000\tTrain loss: 451656.30383936566\n",
      "Epoch: 9050\tTrain loss: 442276.48879305524\n",
      "Epoch: 9100\tTrain loss: 433215.0513000488\n",
      "Epoch: 9150\tTrain loss: 424159.85399882\n",
      "Epoch: 9200\tTrain loss: 415225.3630739848\n",
      "Epoch: 9250\tTrain loss: 406352.0010782878\n",
      "Epoch: 9300\tTrain loss: 397798.26696777344\n",
      "Epoch: 9350\tTrain loss: 388882.3127276103\n",
      "Epoch: 9400\tTrain loss: 380367.03209431964\n",
      "Epoch: 9450\tTrain loss: 371990.23062388104\n",
      "Epoch: 9500\tTrain loss: 363622.714205424\n",
      "Epoch: 9550\tTrain loss: 355279.2051722209\n",
      "Epoch: 9600\tTrain loss: 347209.1753298442\n",
      "Epoch: 9650\tTrain loss: 339111.0672607422\n",
      "Epoch: 9700\tTrain loss: 331239.86705271405\n",
      "Epoch: 9750\tTrain loss: 323418.7190761566\n",
      "Epoch: 9800\tTrain loss: 315683.9804356893\n",
      "Epoch: 9850\tTrain loss: 308056.4541130066\n",
      "Epoch: 9900\tTrain loss: 300459.84358342487\n",
      "Epoch: 9950\tTrain loss: 293072.7445119222\n"
     ]
    }
   ],
   "source": [
    "for baseline in CONFORMAL_BASELINES:\n",
    "    for seed in range(5):\n",
    "        run_medical_experiments(dataset='electricity', \n",
    "                                baseline=baseline,\n",
    "                                save_model=True, \n",
    "                                save_results=True,\n",
    "                                seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a45a98a5-3175-40ce-9c70-5c0b236300b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CFRNN\n",
      "50.0 \\(\\pm\\) 44.7\\%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for baseline in CONFORMAL_BASELINES:\n",
    "    print(baseline)\n",
    "    coverages_mean, coverages_std = get_joint_medical_coverages(baseline, 'electricity', seeds=range(5))\n",
    "    \n",
    "    print('{:.1f} \\\\(\\\\pm\\\\) {:.1f}\\\\%'.format(coverages_mean, coverages_std))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4074c07c-9cca-49c8-83b3-156d108530a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CFRNN\n",
      "1907.98449605306\n",
      "2278.095634475421\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for baseline in CONFORMAL_BASELINES:\n",
    "    print(baseline)\n",
    "    widths_mean, widths_std = get_medical_interval_widths(baseline, 'electricity', seeds=range(5))\n",
    "    \n",
    "    print(widths_mean)\n",
    "    print(widths_std)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b6d7af-a7c6-46eb-aead-23336fa9a665",
   "metadata": {},
   "source": [
    "## Ablation: Uncorrected calibration scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c58171e-7675-4983-851c-5a963d25aeec",
   "metadata": {},
   "source": [
    "#### Electricity Consumption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "752fa9ad-f6cc-43d3-9fb5-5bfdb071888e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50.0 \\(\\pm\\) 44.7\\%\n"
     ]
    }
   ],
   "source": [
    "coverages_mean, coverages_std = get_joint_medical_coverages('CFRNN', 'electricity', seeds=range(5), correct_conformal=True)\n",
    "    \n",
    "print('{:.1f} \\\\(\\\\pm\\\\) {:.1f}\\\\%'.format(coverages_mean, coverages_std))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "86d905e4-dc31-47ae-9b47-bda58c4c59aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50.0 \\(\\pm\\) 44.7\\%\n"
     ]
    }
   ],
   "source": [
    "coverages_mean, coverages_std = get_joint_medical_coverages('CFRNN', 'electricity', seeds=range(5), correct_conformal=False)\n",
    "    \n",
    "print('{:.1f} \\\\(\\\\pm\\\\) {:.1f}\\\\%'.format(coverages_mean, coverages_std))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "66c5f5a2-4423-4aff-b2e4-91cc7a3ce3be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100.0\\%, 100.0\\%]\n",
      "[0.0\\%, 0.0\\%]\n",
      "[0.0\\%, 50.0\\%]\n",
      "[50.0\\%, 50.0\\%]\n",
      "[100.0\\%, 100.0\\%]\n"
     ]
    }
   ],
   "source": [
    "for seed in range(5):\n",
    "    results = load_medical_results(dataset='electricity', baseline='CFRNN', seed=seed)\n",
    "    independent_coverages = results['Mean independent coverage']\n",
    "    print('[{:.1f}\\\\%, {:.1f}\\\\%]'.format(independent_coverages.min() * 100, independent_coverages.max() * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ab9b269d-13d6-4f21-8848-d6eaeb6bc8cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100.0\\%, 100.0\\%]\n",
      "[0.0\\%, 0.0\\%]\n",
      "[0.0\\%, 50.0\\%]\n",
      "[50.0\\%, 50.0\\%]\n",
      "[100.0\\%, 100.0\\%]\n"
     ]
    }
   ],
   "source": [
    "for seed in range(5):\n",
    "    uncorrected_mimic_results = get_uncorrected_medical_results(dataset='electricity', seed=seed)\n",
    "    independent_coverages = uncorrected_mimic_results['Mean independent coverage']\n",
    "    print('[{:.1f}\\\\%, {:.1f}\\\\%]'.format(independent_coverages.min() * 100, independent_coverages.max() * 100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('lab')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "55cd0a945cf9041a238b2dc7f6f29da44cde9fabaada4bd04363a8e3e2e8be13"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
