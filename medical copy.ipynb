{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bfaf9e81-c095-4d66-812e-3459dbc93c65",
   "metadata": {},
   "source": [
    "# Experiments on real-world data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8602d8b-cb7c-488b-9787-1e624204f8f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from utils.train_medical import run_medical_experiments\n",
    "from utils.results import (\n",
    "    get_joint_medical_coverages, \n",
    "    get_medical_interval_widths, \n",
    "    load_medical_results, \n",
    "    get_uncorrected_medical_results\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f13c3ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFORMAL_BASELINES = [\"CFRNN\", \"AdaptiveCFRNN\"]\n",
    "CONFORMAL_BASELINES = [\"CFRNN\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9455748f-ff7b-4fa6-8c02-45dfd5509810",
   "metadata": {},
   "source": [
    "To obtain the results as presented in the paper, run the following three sections."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20370c7c-eaca-49aa-8e45-7f91c01c4faa",
   "metadata": {},
   "source": [
    "## Electricity Consumption dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "84cb28a9-05b8-4be6-9656-44cc61953beb",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training CFRNN\n",
      "Epoch: 0\tTrain loss: 1763442.125\n",
      "Epoch: 50\tTrain loss: 1682647.5833333333\n",
      "Epoch: 100\tTrain loss: 1609232.8333333333\n",
      "Epoch: 150\tTrain loss: 1539459.90625\n",
      "Epoch: 200\tTrain loss: 1472759.2083333333\n",
      "Epoch: 250\tTrain loss: 1408601.71875\n",
      "Epoch: 300\tTrain loss: 1347093.625\n",
      "Epoch: 350\tTrain loss: 1287898.21875\n",
      "Epoch: 400\tTrain loss: 1230931.375\n",
      "Epoch: 450\tTrain loss: 1176256.7708333333\n",
      "Epoch: 500\tTrain loss: 1123881.6875\n",
      "Epoch: 550\tTrain loss: 1073498.0\n",
      "Epoch: 600\tTrain loss: 1025082.15625\n",
      "Epoch: 650\tTrain loss: 978578.1458333334\n",
      "Epoch: 700\tTrain loss: 934074.1458333334\n",
      "Epoch: 750\tTrain loss: 891352.4375\n",
      "Epoch: 800\tTrain loss: 850535.7708333334\n",
      "Epoch: 850\tTrain loss: 811332.1328125\n",
      "Epoch: 900\tTrain loss: 773878.6223958334\n",
      "Epoch: 950\tTrain loss: 737936.1041666666\n",
      "Epoch: 1000\tTrain loss: 703735.1640625\n",
      "Epoch: 1050\tTrain loss: 671071.0247395834\n",
      "Epoch: 1100\tTrain loss: 639966.0390625\n",
      "Epoch: 1150\tTrain loss: 610266.9270833334\n",
      "Epoch: 1200\tTrain loss: 582054.7122395834\n",
      "Epoch: 1250\tTrain loss: 555350.5677083334\n",
      "Epoch: 1300\tTrain loss: 530034.7578125\n",
      "Epoch: 1350\tTrain loss: 506167.65625\n",
      "Epoch: 1400\tTrain loss: 483582.1881510417\n",
      "Epoch: 1450\tTrain loss: 462239.4850260417\n",
      "Epoch: 1500\tTrain loss: 442157.1070963542\n",
      "Epoch: 1550\tTrain loss: 423325.4270833333\n",
      "Epoch: 1600\tTrain loss: 405735.36328125\n",
      "Epoch: 1650\tTrain loss: 389196.7291666667\n",
      "Epoch: 1700\tTrain loss: 373758.3541666667\n",
      "Epoch: 1750\tTrain loss: 359416.3518880208\n",
      "Epoch: 1800\tTrain loss: 346137.15120442706\n",
      "Epoch: 1850\tTrain loss: 333930.4453125\n",
      "Epoch: 1900\tTrain loss: 322568.4817708333\n",
      "Epoch: 1950\tTrain loss: 312190.4677734375\n",
      "Epoch: 2000\tTrain loss: 302624.2903645833\n",
      "Epoch: 2050\tTrain loss: 293900.53515625\n",
      "Epoch: 2100\tTrain loss: 286069.7526041667\n",
      "Epoch: 2150\tTrain loss: 278824.0755208333\n",
      "Epoch: 2200\tTrain loss: 272246.5911458333\n",
      "Epoch: 2250\tTrain loss: 266436.26171875\n",
      "Epoch: 2300\tTrain loss: 261192.64322916666\n",
      "Epoch: 2350\tTrain loss: 256536.91015625\n",
      "Epoch: 2400\tTrain loss: 245533.65234375\n",
      "Epoch: 2450\tTrain loss: 205991.42578125\n",
      "Epoch: 2500\tTrain loss: 188305.92122395834\n",
      "Epoch: 2550\tTrain loss: 176337.33528645834\n",
      "Epoch: 2600\tTrain loss: 165379.953125\n",
      "Epoch: 2650\tTrain loss: 154997.34822591147\n",
      "Epoch: 2700\tTrain loss: 145274.2158203125\n",
      "Epoch: 2750\tTrain loss: 136067.451171875\n",
      "Epoch: 2800\tTrain loss: 127461.59790039062\n",
      "Epoch: 2850\tTrain loss: 119332.78108723958\n",
      "Epoch: 2900\tTrain loss: 111723.35791015625\n",
      "Epoch: 2950\tTrain loss: 104565.88631184895\n",
      "Epoch: 3000\tTrain loss: 97953.35237630208\n",
      "Epoch: 3050\tTrain loss: 91537.5176595052\n",
      "Epoch: 3100\tTrain loss: 85698.9217936198\n",
      "Epoch: 3150\tTrain loss: 80257.30570475261\n",
      "Epoch: 3200\tTrain loss: 75207.73502604167\n",
      "Epoch: 3250\tTrain loss: 70560.71614583333\n",
      "Epoch: 3300\tTrain loss: 65832.81072998047\n",
      "Epoch: 3350\tTrain loss: 60519.34802246094\n",
      "Epoch: 3400\tTrain loss: 55882.281412760414\n",
      "Epoch: 3450\tTrain loss: 51469.45422363281\n",
      "Epoch: 3500\tTrain loss: 47321.324055989586\n",
      "Epoch: 3550\tTrain loss: 43360.60721842448\n",
      "Epoch: 3600\tTrain loss: 39644.03684488932\n",
      "Epoch: 3650\tTrain loss: 36154.9600016276\n",
      "Epoch: 3700\tTrain loss: 32878.24853515625\n",
      "Epoch: 3750\tTrain loss: 29815.099609375\n",
      "Epoch: 3800\tTrain loss: 26947.719401041668\n",
      "Epoch: 3850\tTrain loss: 24266.770385742188\n",
      "Epoch: 3900\tTrain loss: 21805.140462239582\n",
      "Epoch: 3950\tTrain loss: 19523.929524739582\n",
      "Epoch: 4000\tTrain loss: 17413.477294921875\n",
      "Epoch: 4050\tTrain loss: 15485.508626302084\n",
      "Epoch: 4100\tTrain loss: 13724.712280273438\n",
      "Epoch: 4150\tTrain loss: 12129.309753417969\n",
      "Epoch: 4200\tTrain loss: 10696.80859375\n",
      "Epoch: 4250\tTrain loss: 9397.880940755209\n",
      "Epoch: 4300\tTrain loss: 8240.787414550781\n",
      "Epoch: 4350\tTrain loss: 7225.3548583984375\n",
      "Epoch: 4400\tTrain loss: 6306.802225748698\n",
      "Epoch: 4450\tTrain loss: 5515.567118326823\n",
      "Epoch: 4500\tTrain loss: 4820.1816813151045\n",
      "Epoch: 4550\tTrain loss: 4225.1179606119795\n",
      "Epoch: 4600\tTrain loss: 3714.0636393229165\n",
      "Epoch: 4650\tTrain loss: 3276.9317830403647\n",
      "Epoch: 4700\tTrain loss: 2908.3397216796875\n",
      "Epoch: 4750\tTrain loss: 2598.0559895833335\n",
      "Epoch: 4800\tTrain loss: 2338.9962158203125\n",
      "Epoch: 4850\tTrain loss: 2123.553955078125\n",
      "Epoch: 4900\tTrain loss: 1979.3631998697917\n",
      "Epoch: 4950\tTrain loss: 1815.3560994466145\n",
      "Epoch: 5000\tTrain loss: 1696.7433471679688\n",
      "Epoch: 5050\tTrain loss: 1599.8523356119792\n",
      "Epoch: 5100\tTrain loss: 1519.1558430989583\n",
      "Epoch: 5150\tTrain loss: 1452.2050170898438\n",
      "Epoch: 5200\tTrain loss: 1396.3194376627605\n",
      "Epoch: 5250\tTrain loss: 1348.5050252278645\n",
      "Epoch: 5300\tTrain loss: 1307.4634195963542\n",
      "Epoch: 5350\tTrain loss: 1271.1331990559895\n",
      "Epoch: 5400\tTrain loss: 1238.7488403320312\n",
      "Epoch: 5450\tTrain loss: 1209.397684733073\n",
      "Epoch: 5500\tTrain loss: 1182.3560994466145\n",
      "Epoch: 5550\tTrain loss: 1156.7285970052083\n",
      "Epoch: 5600\tTrain loss: 1132.6405639648438\n",
      "Epoch: 5650\tTrain loss: 1107.8351033528645\n",
      "Epoch: 5700\tTrain loss: 1084.480448404948\n",
      "Epoch: 5750\tTrain loss: 1059.3166097005208\n",
      "Epoch: 5800\tTrain loss: 1038.815165201823\n",
      "Epoch: 5850\tTrain loss: 1014.9562784830729\n",
      "Epoch: 5900\tTrain loss: 997.2095133463541\n",
      "Epoch: 5950\tTrain loss: 976.3630574544271\n",
      "Epoch: 6000\tTrain loss: 6732.739827473958\n",
      "Epoch: 6050\tTrain loss: 2869.444091796875\n",
      "Epoch: 6100\tTrain loss: 2173.372111002604\n",
      "Epoch: 6150\tTrain loss: 1235.5914306640625\n",
      "Epoch: 6200\tTrain loss: 1205.4176025390625\n",
      "Epoch: 6250\tTrain loss: 1196.121358235677\n",
      "Epoch: 6300\tTrain loss: 1150.2450154622395\n",
      "Epoch: 6350\tTrain loss: 1125.3870035807292\n",
      "Epoch: 6400\tTrain loss: 1102.2530924479167\n",
      "Epoch: 6450\tTrain loss: 1212.8291625976562\n",
      "Epoch: 6500\tTrain loss: 1079.4492797851562\n",
      "Epoch: 6550\tTrain loss: 1060.2296346028645\n",
      "Epoch: 6600\tTrain loss: 1041.4964192708333\n",
      "Epoch: 6650\tTrain loss: 1013.3416239420573\n",
      "Epoch: 6700\tTrain loss: 999.7827351888021\n",
      "Epoch: 6750\tTrain loss: 1051.4349365234375\n",
      "Epoch: 6800\tTrain loss: 1155.4175618489583\n",
      "Epoch: 6850\tTrain loss: 953.1778157552084\n",
      "Epoch: 6900\tTrain loss: 928.3737386067709\n",
      "Epoch: 6950\tTrain loss: 911.2528686523438\n",
      "Epoch: 7000\tTrain loss: 887.8614095052084\n",
      "Epoch: 7050\tTrain loss: 29904.0205078125\n",
      "Epoch: 7100\tTrain loss: 19924.907084147137\n",
      "Epoch: 7150\tTrain loss: 2034.9166666666667\n",
      "Epoch: 7200\tTrain loss: 5827.436767578125\n",
      "Epoch: 7250\tTrain loss: 989.4779866536459\n",
      "Epoch: 7300\tTrain loss: 973.1017862955729\n",
      "Epoch: 7350\tTrain loss: 940.4073893229166\n",
      "Epoch: 7400\tTrain loss: 997.4630839029948\n",
      "Epoch: 7450\tTrain loss: 955.9246622721354\n",
      "Epoch: 7500\tTrain loss: 938.3880615234375\n",
      "Epoch: 7550\tTrain loss: 920.0695190429688\n",
      "Epoch: 7600\tTrain loss: 891.0903930664062\n",
      "Epoch: 7650\tTrain loss: 812.4121297200521\n",
      "Epoch: 7700\tTrain loss: 799.6483764648438\n",
      "Epoch: 7750\tTrain loss: 788.4277140299479\n",
      "Epoch: 7800\tTrain loss: 779.3761800130209\n",
      "Epoch: 7850\tTrain loss: 770.5612386067709\n",
      "Epoch: 7900\tTrain loss: 761.8432413736979\n",
      "Epoch: 7950\tTrain loss: 751.44140625\n",
      "Epoch: 8000\tTrain loss: 743.4150797526041\n",
      "Epoch: 8050\tTrain loss: 735.943837483724\n",
      "Epoch: 8100\tTrain loss: 729.9358113606771\n",
      "Epoch: 8150\tTrain loss: 725.0251057942709\n",
      "Epoch: 8200\tTrain loss: 716.8563435872396\n",
      "Epoch: 8250\tTrain loss: 710.6226908365885\n",
      "Epoch: 8300\tTrain loss: 704.9947713216146\n",
      "Epoch: 8350\tTrain loss: 699.3270467122396\n",
      "Epoch: 8400\tTrain loss: 693.7182210286459\n",
      "Epoch: 8450\tTrain loss: 687.9324747721354\n",
      "Epoch: 8500\tTrain loss: 683.7163696289062\n",
      "Epoch: 8550\tTrain loss: 728.2281494140625\n",
      "Epoch: 8600\tTrain loss: 691.3391316731771\n",
      "Epoch: 8650\tTrain loss: 678.2109781901041\n",
      "Epoch: 8700\tTrain loss: 669.1851603190104\n",
      "Epoch: 8750\tTrain loss: 663.7118631998698\n",
      "Epoch: 8800\tTrain loss: 657.6146443684896\n",
      "Epoch: 8850\tTrain loss: 652.7360026041666\n",
      "Epoch: 8900\tTrain loss: 648.8038024902344\n",
      "Epoch: 8950\tTrain loss: 646.1325480143229\n",
      "Epoch: 9000\tTrain loss: 639.4861450195312\n",
      "Epoch: 9050\tTrain loss: 634.8404337565104\n",
      "Epoch: 9100\tTrain loss: 632.5313720703125\n",
      "Epoch: 9150\tTrain loss: 627.6886088053385\n",
      "Epoch: 9200\tTrain loss: 622.9173685709635\n",
      "Epoch: 9250\tTrain loss: 619.5127360026041\n",
      "Epoch: 9300\tTrain loss: 615.1905822753906\n",
      "Epoch: 9350\tTrain loss: 611.5447998046875\n",
      "Epoch: 9400\tTrain loss: 608.3857421875\n",
      "Epoch: 9450\tTrain loss: 604.5834554036459\n",
      "Epoch: 9500\tTrain loss: 600.4462890625\n",
      "Epoch: 9550\tTrain loss: 597.5155220031738\n",
      "Epoch: 9600\tTrain loss: 593.1652119954427\n",
      "Epoch: 9650\tTrain loss: 589.463877360026\n",
      "Epoch: 9700\tTrain loss: 586.7150166829427\n",
      "Epoch: 9750\tTrain loss: 583.6122792561849\n",
      "Epoch: 9800\tTrain loss: 579.7082316080729\n",
      "Epoch: 9850\tTrain loss: 576.58984375\n",
      "Epoch: 9900\tTrain loss: 583.673105875651\n",
      "Epoch: 9950\tTrain loss: 559.5136311848959\n",
      "Epoch: 10000\tTrain loss: 552.3506876627604\n",
      "Epoch: 10050\tTrain loss: 547.1423136393229\n",
      "Epoch: 10100\tTrain loss: 542.8300037384033\n",
      "Epoch: 10150\tTrain loss: 544.0402119954427\n",
      "Epoch: 10200\tTrain loss: 535.7257486979166\n",
      "Epoch: 10250\tTrain loss: 532.5918172200521\n",
      "Epoch: 10300\tTrain loss: 526.7982076009115\n",
      "Epoch: 10350\tTrain loss: 553.0709940592448\n",
      "Epoch: 10400\tTrain loss: 1719.4150797526042\n",
      "Epoch: 10450\tTrain loss: 517.5713602701823\n",
      "Epoch: 10500\tTrain loss: 509.3506571451823\n",
      "Epoch: 10550\tTrain loss: 501.16278076171875\n",
      "Epoch: 10600\tTrain loss: 492.4224141438802\n",
      "Epoch: 10650\tTrain loss: 480.8466634750366\n",
      "Epoch: 10700\tTrain loss: 418.57985432942706\n",
      "Epoch: 10750\tTrain loss: 391.9349670410156\n",
      "Epoch: 10800\tTrain loss: 379.39906819661456\n",
      "Epoch: 10850\tTrain loss: 1938.5127766927083\n",
      "Epoch: 10900\tTrain loss: 942.7740478515625\n",
      "Epoch: 10950\tTrain loss: 832.2883504231771\n",
      "Epoch: 11000\tTrain loss: 823.9692586263021\n",
      "Epoch: 11050\tTrain loss: 9292.855326334635\n",
      "Epoch: 11100\tTrain loss: 30508.14365641276\n",
      "Epoch: 11150\tTrain loss: 67021.45009358723\n",
      "Epoch: 11200\tTrain loss: 54842.54528808594\n",
      "Epoch: 11250\tTrain loss: 46019.899322509766\n",
      "Epoch: 11300\tTrain loss: 39197.443450927734\n",
      "Epoch: 11350\tTrain loss: 33636.64666748047\n",
      "Epoch: 11400\tTrain loss: 28801.59614054362\n",
      "Epoch: 11450\tTrain loss: 24758.787027994793\n",
      "Epoch: 11500\tTrain loss: 21246.775817871094\n",
      "Epoch: 11550\tTrain loss: 18237.320882161457\n",
      "Epoch: 11600\tTrain loss: 15616.659362792969\n",
      "Epoch: 11650\tTrain loss: 13329.28847249349\n",
      "Epoch: 11700\tTrain loss: 11403.715993245443\n",
      "Epoch: 11750\tTrain loss: 9675.632598876953\n",
      "Epoch: 11800\tTrain loss: 8181.382141113281\n",
      "Epoch: 11850\tTrain loss: 6906.199513753255\n",
      "Epoch: 11900\tTrain loss: 5809.146413167317\n",
      "Epoch: 11950\tTrain loss: 41326.19508870443\n",
      "Epoch: 12000\tTrain loss: 32842.12357584635\n",
      "Epoch: 12050\tTrain loss: 27436.71160888672\n",
      "Epoch: 12100\tTrain loss: 23146.02060953776\n",
      "Epoch: 12150\tTrain loss: 19487.729014078777\n",
      "Epoch: 12200\tTrain loss: 16455.445454915363\n",
      "Epoch: 12250\tTrain loss: 13891.541056315104\n",
      "Epoch: 12300\tTrain loss: 1240.5750427246094\n",
      "Epoch: 12350\tTrain loss: 1152.8969217936199\n",
      "Epoch: 12400\tTrain loss: 1095.8367309570312\n",
      "Epoch: 12450\tTrain loss: 1024.1110127766926\n",
      "Epoch: 12500\tTrain loss: 984.6804809570312\n",
      "Epoch: 12550\tTrain loss: 947.1175130208334\n",
      "Epoch: 12600\tTrain loss: 914.3238220214844\n",
      "Epoch: 12650\tTrain loss: 888.3179168701172\n",
      "Epoch: 12700\tTrain loss: 941.3532409667969\n",
      "Epoch: 12750\tTrain loss: 779.7828877766927\n",
      "Epoch: 12800\tTrain loss: 679.6704711914062\n",
      "Epoch: 12850\tTrain loss: 643.2470296223959\n",
      "Epoch: 12900\tTrain loss: 626.3156026204427\n",
      "Epoch: 12950\tTrain loss: 614.79541015625\n",
      "Epoch: 13000\tTrain loss: 604.8365173339844\n",
      "Epoch: 13050\tTrain loss: 595.8917439778646\n",
      "Epoch: 13100\tTrain loss: 615.701182047526\n",
      "Epoch: 13150\tTrain loss: 597.1908569335938\n",
      "Epoch: 13200\tTrain loss: 585.0863749186198\n",
      "Epoch: 13250\tTrain loss: 8080.729848225911\n",
      "Epoch: 13300\tTrain loss: 689.6234842936198\n",
      "Epoch: 13350\tTrain loss: 579.7303568522135\n",
      "Epoch: 13400\tTrain loss: 544.9400329589844\n",
      "Epoch: 13450\tTrain loss: 521.7269694010416\n",
      "Epoch: 13500\tTrain loss: 502.4968744913737\n",
      "Epoch: 13550\tTrain loss: 496.2386474609375\n",
      "Epoch: 13600\tTrain loss: 475.1410903930664\n",
      "Epoch: 13650\tTrain loss: 478.58164469401044\n",
      "Epoch: 13700\tTrain loss: 462.4839274088542\n",
      "Epoch: 13750\tTrain loss: 556.4741923014323\n",
      "Epoch: 13800\tTrain loss: 539.26513671875\n",
      "Epoch: 13850\tTrain loss: 522.7553507486979\n",
      "Epoch: 13900\tTrain loss: 506.3013000488281\n",
      "Epoch: 13950\tTrain loss: 499.0242614746094\n",
      "Epoch: 14000\tTrain loss: 487.3936055501302\n",
      "Epoch: 14050\tTrain loss: 511.4750061035156\n",
      "Epoch: 14100\tTrain loss: 497.1129150390625\n",
      "Epoch: 14150\tTrain loss: 503.32119369506836\n",
      "Epoch: 14200\tTrain loss: 490.2272033691406\n",
      "Epoch: 14250\tTrain loss: 446.87583859761554\n",
      "Epoch: 14300\tTrain loss: 442.4255777994792\n",
      "Epoch: 14350\tTrain loss: 655.4053141276041\n",
      "Epoch: 14400\tTrain loss: 531.3532409667969\n",
      "Epoch: 14450\tTrain loss: 524.9113464355469\n",
      "Epoch: 14500\tTrain loss: 516.3044942220052\n",
      "Epoch: 14550\tTrain loss: 506.1953837076823\n",
      "Epoch: 14600\tTrain loss: 473.4398905436198\n",
      "Epoch: 14650\tTrain loss: 449.8138936360677\n",
      "Epoch: 14700\tTrain loss: 433.3331705729167\n",
      "Epoch: 14750\tTrain loss: 421.6416530609131\n",
      "Epoch: 14800\tTrain loss: 407.5775858561198\n",
      "Epoch: 14850\tTrain loss: 398.44134521484375\n",
      "Epoch: 14900\tTrain loss: 386.99476623535156\n",
      "Epoch: 14950\tTrain loss: 27646.869873046875\n",
      "Epoch: 15000\tTrain loss: 1125.3123168945312\n",
      "Epoch: 15050\tTrain loss: 826.9007466634115\n",
      "Epoch: 15100\tTrain loss: 746.7555440266927\n",
      "Epoch: 15150\tTrain loss: 694.5828857421875\n",
      "Epoch: 15200\tTrain loss: 650.915283203125\n",
      "Epoch: 15250\tTrain loss: 623.3063049316406\n",
      "Epoch: 15300\tTrain loss: 593.0553029378256\n",
      "Epoch: 15350\tTrain loss: 568.803965250651\n",
      "Epoch: 15400\tTrain loss: 555.7113952636719\n",
      "Epoch: 15450\tTrain loss: 516.7990010579427\n",
      "Epoch: 15500\tTrain loss: 535.370839436849\n",
      "Epoch: 15550\tTrain loss: 512.4073079427084\n",
      "Epoch: 15600\tTrain loss: 516.7613627115885\n",
      "Epoch: 15650\tTrain loss: 503.2666524251302\n",
      "Epoch: 15700\tTrain loss: 480.7515869140625\n",
      "Epoch: 15750\tTrain loss: 1478.7394409179688\n",
      "Epoch: 15800\tTrain loss: 1286.977762858073\n",
      "Epoch: 15850\tTrain loss: 1189.2914225260417\n",
      "Epoch: 15900\tTrain loss: 1106.842793782552\n",
      "Epoch: 15950\tTrain loss: 1028.7598876953125\n",
      "Epoch: 16000\tTrain loss: 1100.9615275065105\n",
      "Epoch: 16050\tTrain loss: 929.5771179199219\n",
      "Epoch: 16100\tTrain loss: 877.8009033203125\n",
      "Epoch: 16150\tTrain loss: 814.3839518229166\n",
      "Epoch: 16200\tTrain loss: 846.1541951497396\n",
      "Epoch: 16250\tTrain loss: 776.3917643229166\n",
      "Epoch: 16300\tTrain loss: 676.833740234375\n",
      "Epoch: 16350\tTrain loss: 566.9638163248698\n",
      "Epoch: 16400\tTrain loss: 531.3949788411459\n",
      "Epoch: 16450\tTrain loss: 531.134287516276\n",
      "Epoch: 16500\tTrain loss: 506.68251546223956\n",
      "Epoch: 16550\tTrain loss: 1289.3250020345051\n",
      "Epoch: 16600\tTrain loss: 688.4972941080729\n",
      "Epoch: 16650\tTrain loss: 641.6048482259115\n",
      "Epoch: 16700\tTrain loss: 662.569101969401\n",
      "Epoch: 16750\tTrain loss: 576.7154846191406\n",
      "Epoch: 16800\tTrain loss: 567.6888224283854\n",
      "Epoch: 16850\tTrain loss: 516.4340667724609\n",
      "Epoch: 16900\tTrain loss: 482.0816243489583\n",
      "Epoch: 16950\tTrain loss: 457.9111836751302\n",
      "Epoch: 17000\tTrain loss: 752.3320007324219\n",
      "Epoch: 17050\tTrain loss: 550.1370544433594\n",
      "Epoch: 17100\tTrain loss: 532.2656656901041\n",
      "Epoch: 17150\tTrain loss: 519.5325826009115\n",
      "Epoch: 17200\tTrain loss: 508.84962972005206\n",
      "Epoch: 17250\tTrain loss: 499.9019470214844\n",
      "Epoch: 17300\tTrain loss: 491.35408782958984\n",
      "Epoch: 17350\tTrain loss: 482.87779744466144\n",
      "Epoch: 17400\tTrain loss: 476.34949747721356\n",
      "Epoch: 17450\tTrain loss: 469.32586669921875\n",
      "Epoch: 17500\tTrain loss: 462.59788004557294\n",
      "Epoch: 17550\tTrain loss: 455.26439539591473\n",
      "Epoch: 17600\tTrain loss: 449.484619140625\n",
      "Epoch: 17650\tTrain loss: 442.07938893636066\n",
      "Epoch: 17700\tTrain loss: 436.24774169921875\n",
      "Epoch: 17750\tTrain loss: 434.1607666015625\n",
      "Epoch: 17800\tTrain loss: 420.0482991536458\n",
      "Epoch: 17850\tTrain loss: 410.09360758463544\n",
      "Epoch: 17900\tTrain loss: 398.0285237630208\n",
      "Epoch: 17950\tTrain loss: 577.0220489501953\n",
      "Epoch: 18000\tTrain loss: 552.3697992960612\n",
      "Epoch: 18050\tTrain loss: 322.44515736897785\n",
      "Epoch: 18100\tTrain loss: 489.31653849283856\n",
      "Epoch: 18150\tTrain loss: 309.87977600097656\n",
      "Epoch: 18200\tTrain loss: 320.06304931640625\n",
      "Epoch: 18250\tTrain loss: 232.96090189615884\n",
      "Epoch: 18300\tTrain loss: 373.87803649902344\n",
      "Epoch: 18350\tTrain loss: 314.01399993896484\n",
      "Epoch: 18400\tTrain loss: 372.8538411458333\n",
      "Epoch: 18450\tTrain loss: 245.99849955240884\n",
      "Epoch: 18500\tTrain loss: 470.57690175374347\n",
      "Epoch: 18550\tTrain loss: 243.93895467122397\n",
      "Epoch: 18600\tTrain loss: 164.30782572428384\n",
      "Epoch: 18650\tTrain loss: 149.1655731201172\n",
      "Epoch: 18700\tTrain loss: 135.5819295247396\n",
      "Epoch: 18750\tTrain loss: 123.1777572631836\n",
      "Epoch: 18800\tTrain loss: 112.93521881103516\n",
      "Epoch: 18850\tTrain loss: 104.1845219930013\n",
      "Epoch: 18900\tTrain loss: 231.55090967814127\n",
      "Epoch: 18950\tTrain loss: 206.37212117513022\n",
      "Epoch: 19000\tTrain loss: 186.88239669799805\n",
      "Epoch: 19050\tTrain loss: 168.2287394205729\n",
      "Epoch: 19100\tTrain loss: 118.57340240478516\n",
      "Epoch: 19150\tTrain loss: 147.52481333414713\n",
      "Epoch: 19200\tTrain loss: 131.52773920694986\n",
      "Epoch: 19250\tTrain loss: 111.8616943359375\n",
      "Epoch: 19300\tTrain loss: 74.2537612915039\n",
      "Epoch: 19350\tTrain loss: 61.91315841674805\n",
      "Epoch: 19400\tTrain loss: 54.66866938273112\n",
      "Epoch: 19450\tTrain loss: 49.48891576131185\n",
      "Epoch: 19500\tTrain loss: 927.5823465983073\n",
      "Epoch: 19550\tTrain loss: 453.56781514485675\n",
      "Epoch: 19600\tTrain loss: 525.7579650878906\n",
      "Epoch: 19650\tTrain loss: 354.87098185221356\n",
      "Epoch: 19700\tTrain loss: 341.49352773030597\n",
      "Epoch: 19750\tTrain loss: 318.39427439371747\n",
      "Epoch: 19800\tTrain loss: 297.6479136149089\n",
      "Epoch: 19850\tTrain loss: 298.12757364908856\n",
      "Epoch: 19900\tTrain loss: 265.68882242838544\n",
      "Epoch: 19950\tTrain loss: 219.4182917277018\n",
      "Epoch: 20000\tTrain loss: 299.5481185913086\n",
      "Epoch: 20050\tTrain loss: 251.2136662801107\n",
      "Epoch: 20100\tTrain loss: 253.7924919128418\n",
      "Epoch: 20150\tTrain loss: 378.8907521565755\n",
      "Epoch: 20200\tTrain loss: 271.4688186645508\n",
      "Epoch: 20250\tTrain loss: 250.2924016316732\n",
      "Epoch: 20300\tTrain loss: 231.77974573771158\n",
      "Epoch: 20350\tTrain loss: 213.14525985717773\n",
      "Epoch: 20400\tTrain loss: 151.99526500701904\n",
      "Epoch: 20450\tTrain loss: 105.76838302612305\n",
      "Epoch: 20500\tTrain loss: 85.16394106547038\n",
      "Epoch: 20550\tTrain loss: 77.5043551127116\n",
      "Epoch: 20600\tTrain loss: 222.93874168395996\n",
      "Epoch: 20650\tTrain loss: 429.20341237386066\n",
      "Epoch: 20700\tTrain loss: 388.4508641560872\n",
      "Epoch: 20750\tTrain loss: 341.8750712076823\n",
      "Epoch: 20800\tTrain loss: 304.0196024576823\n",
      "Epoch: 20850\tTrain loss: 278.8346761067708\n",
      "Epoch: 20900\tTrain loss: 245.1573944091797\n",
      "Epoch: 20950\tTrain loss: 223.96488316853842\n",
      "Epoch: 21000\tTrain loss: 207.0470759073893\n",
      "Epoch: 21050\tTrain loss: 187.56883112589517\n",
      "Epoch: 21100\tTrain loss: 279.409610748291\n",
      "Epoch: 21150\tTrain loss: 261.9942804972331\n",
      "Epoch: 21200\tTrain loss: 248.7639478047689\n",
      "Epoch: 21250\tTrain loss: 203.5546137491862\n",
      "Epoch: 21300\tTrain loss: 184.56475321451822\n",
      "Epoch: 21350\tTrain loss: 172.90793736775717\n",
      "Epoch: 21400\tTrain loss: 165.30012385050455\n",
      "Epoch: 21450\tTrain loss: 157.93615849812826\n",
      "Epoch: 21500\tTrain loss: 148.88046391805014\n",
      "Epoch: 21550\tTrain loss: 139.10418891906738\n",
      "Epoch: 21600\tTrain loss: 128.8216298421224\n",
      "Epoch: 21650\tTrain loss: 562.5468571980795\n",
      "Epoch: 21700\tTrain loss: 327.31313133239746\n",
      "Epoch: 21750\tTrain loss: 321.39612197875977\n",
      "Epoch: 21800\tTrain loss: 397.9812876383464\n",
      "Epoch: 21850\tTrain loss: 342.93657302856445\n",
      "Epoch: 21900\tTrain loss: 342.78541310628253\n",
      "Epoch: 21950\tTrain loss: 343.9812761942546\n",
      "Epoch: 22000\tTrain loss: 329.4741096496582\n",
      "Epoch: 22050\tTrain loss: 272.51095549265546\n",
      "Epoch: 22100\tTrain loss: 240.1736272176107\n",
      "Epoch: 22150\tTrain loss: 2346.5480753580728\n",
      "Epoch: 22200\tTrain loss: 1609.6662902832031\n",
      "Epoch: 22250\tTrain loss: 1016.5219014485677\n",
      "Epoch: 22300\tTrain loss: 816.7592569986979\n",
      "Epoch: 22350\tTrain loss: 689.9959920247396\n",
      "Epoch: 22400\tTrain loss: 646.2582397460938\n",
      "Epoch: 22450\tTrain loss: 608.4938557942709\n",
      "Epoch: 22500\tTrain loss: 575.7567647298177\n",
      "Epoch: 22550\tTrain loss: 546.6310526529948\n",
      "Epoch: 22600\tTrain loss: 520.9525655110677\n",
      "Epoch: 22650\tTrain loss: 497.68365478515625\n",
      "Epoch: 22700\tTrain loss: 476.70098876953125\n",
      "Epoch: 22750\tTrain loss: 461.33476130167645\n",
      "Epoch: 22800\tTrain loss: 451.1581522623698\n",
      "Epoch: 22850\tTrain loss: 442.68751462300617\n",
      "Epoch: 22900\tTrain loss: 435.0420328776042\n",
      "Epoch: 22950\tTrain loss: 481.92822265625\n",
      "Epoch: 23000\tTrain loss: 466.97655232747394\n",
      "Epoch: 23050\tTrain loss: 451.99672253926593\n",
      "Epoch: 23100\tTrain loss: 443.2493591308594\n",
      "Epoch: 23150\tTrain loss: 437.0786437988281\n",
      "Epoch: 23200\tTrain loss: 432.6954091389974\n",
      "Epoch: 23250\tTrain loss: 429.1860860188802\n",
      "Epoch: 23300\tTrain loss: 426.1407775878906\n",
      "Epoch: 23350\tTrain loss: 423.1085713704427\n",
      "Epoch: 23400\tTrain loss: 420.5022627512614\n",
      "Epoch: 23450\tTrain loss: 417.2229410807292\n",
      "Epoch: 23500\tTrain loss: 414.98593584696454\n",
      "Epoch: 23550\tTrain loss: 412.19189580281574\n",
      "Epoch: 23600\tTrain loss: 409.83826700846356\n",
      "Epoch: 23650\tTrain loss: 407.8857421875\n",
      "Epoch: 23700\tTrain loss: 405.27362060546875\n",
      "Epoch: 23750\tTrain loss: 402.92946370442706\n",
      "Epoch: 23800\tTrain loss: 410.1605224609375\n",
      "Epoch: 23850\tTrain loss: 404.4176330566406\n",
      "Epoch: 23900\tTrain loss: 399.82375081380206\n",
      "Epoch: 23950\tTrain loss: 396.8112880388896\n",
      "Epoch: 24000\tTrain loss: 393.82350667317706\n",
      "Epoch: 24050\tTrain loss: 385.4924621582031\n",
      "Epoch: 24100\tTrain loss: 345.71045239766437\n",
      "Epoch: 24150\tTrain loss: 286.83595275878906\n",
      "Epoch: 24200\tTrain loss: 260.28691991170246\n",
      "Epoch: 24250\tTrain loss: 244.8511562347412\n",
      "Epoch: 24300\tTrain loss: 236.21330388387045\n",
      "Epoch: 24350\tTrain loss: 229.50234095255533\n",
      "Epoch: 24400\tTrain loss: 223.49665069580078\n",
      "Epoch: 24450\tTrain loss: 220.9762477874756\n",
      "Epoch: 24500\tTrain loss: 217.7214387257894\n",
      "Epoch: 24550\tTrain loss: 219.7151724497477\n",
      "Epoch: 24600\tTrain loss: 216.04679918289185\n",
      "Epoch: 24650\tTrain loss: 214.66022698084512\n",
      "Epoch: 24700\tTrain loss: 213.12963779767355\n",
      "Epoch: 24750\tTrain loss: 212.2060340245565\n",
      "Epoch: 24800\tTrain loss: 539.3581899007162\n",
      "Epoch: 24850\tTrain loss: 622.2916361490885\n",
      "Epoch: 24900\tTrain loss: 550.802106221517\n",
      "Epoch: 24950\tTrain loss: 1093.8345845540364\n",
      "Epoch: 25000\tTrain loss: 697.1234741210938\n",
      "Epoch: 25050\tTrain loss: 575.4160766601562\n",
      "Epoch: 25100\tTrain loss: 532.873524983724\n",
      "Epoch: 25150\tTrain loss: 516.153554280599\n",
      "Epoch: 25200\tTrain loss: 508.3454182942708\n",
      "Epoch: 25250\tTrain loss: 503.1169942220052\n",
      "Epoch: 25300\tTrain loss: 479.4589436848958\n",
      "Epoch: 25350\tTrain loss: 395.04510498046875\n",
      "Epoch: 25400\tTrain loss: 324.98106066385907\n",
      "Epoch: 25450\tTrain loss: 300.1124267578125\n",
      "Epoch: 25500\tTrain loss: 276.1085238456726\n",
      "Epoch: 25550\tTrain loss: 274.13978576660156\n",
      "Epoch: 25600\tTrain loss: 272.74291888872784\n",
      "Epoch: 25650\tTrain loss: 291.3511098225911\n",
      "Epoch: 25700\tTrain loss: 240.93437703450522\n",
      "Epoch: 25750\tTrain loss: 252.26294962565103\n",
      "Epoch: 25800\tTrain loss: 259.18666585286456\n",
      "Epoch: 25850\tTrain loss: 229.64078776041666\n",
      "Epoch: 25900\tTrain loss: 318.76282755533856\n",
      "Epoch: 25950\tTrain loss: 311.34032440185547\n",
      "Epoch: 26000\tTrain loss: 212.80067698160806\n",
      "Epoch: 26050\tTrain loss: 221.55279922485352\n",
      "Epoch: 26100\tTrain loss: 203.54327646891275\n",
      "Epoch: 26150\tTrain loss: 194.947239557902\n",
      "Epoch: 26200\tTrain loss: 337.0929463704427\n",
      "Epoch: 26250\tTrain loss: 221.24326197306314\n",
      "Epoch: 26300\tTrain loss: 191.39939753214517\n",
      "Epoch: 26350\tTrain loss: 184.9658546447754\n",
      "Epoch: 26400\tTrain loss: 201.34382120768228\n",
      "Epoch: 26450\tTrain loss: 246.07209142049155\n",
      "Epoch: 26500\tTrain loss: 173.0372772216797\n",
      "Epoch: 26550\tTrain loss: 166.80858103434244\n",
      "Epoch: 26600\tTrain loss: 187.93463961283365\n",
      "Epoch: 26650\tTrain loss: 168.55699030558267\n",
      "Epoch: 26700\tTrain loss: 148.0728905995687\n",
      "Epoch: 26750\tTrain loss: 254.53073755900064\n",
      "Epoch: 26800\tTrain loss: 205.51367060343424\n",
      "Epoch: 26850\tTrain loss: 163.0750773747762\n",
      "Epoch: 26900\tTrain loss: 156.5309518178304\n",
      "Epoch: 26950\tTrain loss: 222.66460418701172\n",
      "Epoch: 27000\tTrain loss: 211.99037488301596\n",
      "Epoch: 27050\tTrain loss: 144.2248166402181\n",
      "Epoch: 27100\tTrain loss: 330.5928560892741\n",
      "Epoch: 27150\tTrain loss: 167.86893208821616\n",
      "Epoch: 27200\tTrain loss: 371.2603454589844\n",
      "Epoch: 27250\tTrain loss: 222.9128688176473\n",
      "Epoch: 27300\tTrain loss: 176.23135058085123\n",
      "Epoch: 27350\tTrain loss: 148.70475133260092\n",
      "Epoch: 27400\tTrain loss: 116.586901028951\n",
      "Epoch: 27450\tTrain loss: 83.14845116933186\n",
      "Epoch: 27500\tTrain loss: 65.9527136484782\n",
      "Epoch: 27550\tTrain loss: 57.757490158081055\n",
      "Epoch: 27600\tTrain loss: 170.03074741363525\n",
      "Epoch: 27650\tTrain loss: 126.31799062093098\n",
      "Epoch: 27700\tTrain loss: 244.74322700500488\n",
      "Epoch: 27750\tTrain loss: 1893.5568593343098\n",
      "Epoch: 27800\tTrain loss: 64.71948623657227\n",
      "Epoch: 27850\tTrain loss: 38.547960917154946\n",
      "Epoch: 27900\tTrain loss: 48.365818659464516\n",
      "Epoch: 27950\tTrain loss: 28.378718694051106\n",
      "Epoch: 28000\tTrain loss: 21.017303148905437\n",
      "Epoch: 28050\tTrain loss: 39.47681172688802\n",
      "Epoch: 28100\tTrain loss: 23.115907669067383\n",
      "Epoch: 28150\tTrain loss: 73.98394711812337\n",
      "Epoch: 28200\tTrain loss: 49.31800397237142\n",
      "Epoch: 28250\tTrain loss: 194.93543275197348\n",
      "Epoch: 28300\tTrain loss: 53.645561854044594\n",
      "Epoch: 28350\tTrain loss: 42.45337740580241\n",
      "Epoch: 28400\tTrain loss: 11.136796633402506\n",
      "Epoch: 28450\tTrain loss: 19.549500981966656\n",
      "Epoch: 28500\tTrain loss: 11.10471518834432\n",
      "Epoch: 28550\tTrain loss: 44.15828959147135\n",
      "Epoch: 28600\tTrain loss: 11.629955927530924\n",
      "Epoch: 28650\tTrain loss: 379.57942454020184\n",
      "Epoch: 28700\tTrain loss: 111.85826269785564\n",
      "Epoch: 28750\tTrain loss: 101.79872385660808\n",
      "Epoch: 28800\tTrain loss: 97.4148284594218\n",
      "Epoch: 28850\tTrain loss: 26.660257816314697\n",
      "Epoch: 28900\tTrain loss: 11.242870012919107\n",
      "Epoch: 28950\tTrain loss: 22.15637954076131\n",
      "Epoch: 29000\tTrain loss: 8.24102520942688\n",
      "Epoch: 29050\tTrain loss: 5.079397916793823\n",
      "Epoch: 29100\tTrain loss: 7.171004851659139\n",
      "Epoch: 29150\tTrain loss: 2.998433748881022\n",
      "Epoch: 29200\tTrain loss: 36.46546618143717\n",
      "Epoch: 29250\tTrain loss: 25.715407530466717\n",
      "Epoch: 29300\tTrain loss: 10.167054255803427\n",
      "Epoch: 29350\tTrain loss: 5.000461657842\n",
      "Epoch: 29400\tTrain loss: 3.8992594877878823\n",
      "Epoch: 29450\tTrain loss: 44.49288775523504\n",
      "Epoch: 29500\tTrain loss: 8.016246795654297\n",
      "Epoch: 29550\tTrain loss: 3.092425594727198\n",
      "Epoch: 29600\tTrain loss: 2.26467235883077\n",
      "Epoch: 29650\tTrain loss: 15.466811080773672\n",
      "Epoch: 29700\tTrain loss: 12.693768401940664\n",
      "Epoch: 29750\tTrain loss: 1.96213894089063\n",
      "Epoch: 29800\tTrain loss: 1.3533579508463542\n",
      "Epoch: 29850\tTrain loss: 1.1621135870615642\n",
      "Epoch: 29900\tTrain loss: 58.80737612644831\n",
      "Epoch: 29950\tTrain loss: 1.0786955257256825\n",
      "Training CFRNN\n",
      "Epoch: 0\tTrain loss: 1461667.5833333333\n",
      "Epoch: 50\tTrain loss: 1387610.375\n",
      "Epoch: 100\tTrain loss: 1318287.375\n",
      "Epoch: 150\tTrain loss: 1253566.0208333333\n",
      "Epoch: 200\tTrain loss: 1192212.2916666667\n",
      "Epoch: 250\tTrain loss: 1133612.9375\n",
      "Epoch: 300\tTrain loss: 1077579.3645833333\n",
      "Epoch: 350\tTrain loss: 1024210.4375\n",
      "Epoch: 400\tTrain loss: 973302.1041666666\n",
      "Epoch: 450\tTrain loss: 924420.3229166666\n",
      "Epoch: 500\tTrain loss: 877765.0625\n",
      "Epoch: 550\tTrain loss: 833204.8072916666\n",
      "Epoch: 600\tTrain loss: 790589.765625\n",
      "Epoch: 650\tTrain loss: 749896.1927083334\n",
      "Epoch: 700\tTrain loss: 711112.3385416666\n",
      "Epoch: 750\tTrain loss: 674191.90625\n",
      "Epoch: 800\tTrain loss: 638849.7083333334\n",
      "Epoch: 850\tTrain loss: 605339.2395833334\n",
      "Epoch: 900\tTrain loss: 573458.0625\n",
      "Epoch: 950\tTrain loss: 543135.7604166666\n",
      "Epoch: 1000\tTrain loss: 514369.8541666667\n",
      "Epoch: 1050\tTrain loss: 487108.66796875\n",
      "Epoch: 1100\tTrain loss: 461285.3333333333\n",
      "Epoch: 1150\tTrain loss: 436964.8984375\n",
      "Epoch: 1200\tTrain loss: 414067.7369791667\n",
      "Epoch: 1250\tTrain loss: 392459.40625\n",
      "Epoch: 1300\tTrain loss: 372230.8541666667\n",
      "Epoch: 1350\tTrain loss: 353197.9583333333\n",
      "Epoch: 1400\tTrain loss: 335458.8053385417\n",
      "Epoch: 1450\tTrain loss: 318788.8170572917\n",
      "Epoch: 1500\tTrain loss: 303398.7298177083\n",
      "Epoch: 1550\tTrain loss: 288994.8307291667\n",
      "Epoch: 1600\tTrain loss: 275793.763671875\n",
      "Epoch: 1650\tTrain loss: 263439.56640625\n",
      "Epoch: 1700\tTrain loss: 252266.50130208334\n",
      "Epoch: 1750\tTrain loss: 241827.1251627604\n",
      "Epoch: 1800\tTrain loss: 232417.984375\n",
      "Epoch: 1850\tTrain loss: 223774.75911458334\n",
      "Epoch: 1900\tTrain loss: 215983.92578125\n",
      "Epoch: 1950\tTrain loss: 208850.37760416666\n",
      "Epoch: 2000\tTrain loss: 202554.390625\n",
      "Epoch: 2050\tTrain loss: 196784.05989583334\n",
      "Epoch: 2100\tTrain loss: 191694.0625\n",
      "Epoch: 2150\tTrain loss: 187212.05208333334\n",
      "Epoch: 2200\tTrain loss: 183140.75\n",
      "Epoch: 2250\tTrain loss: 179571.16145833334\n",
      "Epoch: 2300\tTrain loss: 176411.66145833334\n",
      "Epoch: 2350\tTrain loss: 173685.6328125\n",
      "Epoch: 2400\tTrain loss: 171300.39322916666\n",
      "Epoch: 2450\tTrain loss: 156148.15592447916\n",
      "Epoch: 2500\tTrain loss: 174787.00431315103\n",
      "Epoch: 2550\tTrain loss: 140714.953125\n",
      "Epoch: 2600\tTrain loss: 132080.85677083334\n",
      "Epoch: 2650\tTrain loss: 124191.31673177083\n",
      "Epoch: 2700\tTrain loss: 116769.62587483723\n",
      "Epoch: 2750\tTrain loss: 109915.77514648438\n",
      "Epoch: 2800\tTrain loss: 103561.04349772136\n",
      "Epoch: 2850\tTrain loss: 97681.94514973958\n",
      "Epoch: 2900\tTrain loss: 92252.51698811848\n",
      "Epoch: 2950\tTrain loss: 87200.03629557292\n",
      "Epoch: 3000\tTrain loss: 82502.1255493164\n",
      "Epoch: 3050\tTrain loss: 78236.02616373698\n",
      "Epoch: 3100\tTrain loss: 74285.08310953777\n",
      "Epoch: 3150\tTrain loss: 70644.93159993489\n",
      "Epoch: 3200\tTrain loss: 67306.61688232422\n",
      "Epoch: 3250\tTrain loss: 64270.96875\n",
      "Epoch: 3300\tTrain loss: 61517.42655436198\n",
      "Epoch: 3350\tTrain loss: 58995.95271809896\n",
      "Epoch: 3400\tTrain loss: 56739.72928873698\n",
      "Epoch: 3450\tTrain loss: 53823.905110677086\n",
      "Epoch: 3500\tTrain loss: 51072.19521077474\n",
      "Epoch: 3550\tTrain loss: 48426.652099609375\n",
      "Epoch: 3600\tTrain loss: 45985.231119791664\n",
      "Epoch: 3650\tTrain loss: 43680.88651529948\n",
      "Epoch: 3700\tTrain loss: 41492.77168782552\n",
      "Epoch: 3750\tTrain loss: 39356.13671875\n",
      "Epoch: 3800\tTrain loss: 39976.99344889323\n",
      "Epoch: 3850\tTrain loss: 35278.132161458336\n",
      "Epoch: 3900\tTrain loss: 33302.28348795573\n",
      "Epoch: 3950\tTrain loss: 31413.391153971355\n",
      "Epoch: 4000\tTrain loss: 29596.97833251953\n",
      "Epoch: 4050\tTrain loss: 27825.008951822918\n",
      "Epoch: 4100\tTrain loss: 26113.97928873698\n",
      "Epoch: 4150\tTrain loss: 24481.28857421875\n",
      "Epoch: 4200\tTrain loss: 23251.896728515625\n",
      "Epoch: 4250\tTrain loss: 21444.855204264324\n",
      "Epoch: 4300\tTrain loss: 20003.764607747395\n",
      "Epoch: 4350\tTrain loss: 18631.293986002605\n",
      "Epoch: 4400\tTrain loss: 17328.491048177082\n",
      "Epoch: 4450\tTrain loss: 16065.726725260416\n",
      "Epoch: 4500\tTrain loss: 14915.380696614584\n",
      "Epoch: 4550\tTrain loss: 13772.155253092447\n",
      "Epoch: 4600\tTrain loss: 12678.511271158854\n",
      "Epoch: 4650\tTrain loss: 11673.095418294271\n",
      "Epoch: 4700\tTrain loss: 10754.189331054688\n",
      "Epoch: 4750\tTrain loss: 9840.828735351562\n",
      "Epoch: 4800\tTrain loss: 9044.043863932291\n",
      "Epoch: 4850\tTrain loss: 8215.177408854166\n",
      "Epoch: 4900\tTrain loss: 7478.492268880208\n",
      "Epoch: 4950\tTrain loss: 6805.550537109375\n",
      "Epoch: 5000\tTrain loss: 6179.1514892578125\n",
      "Epoch: 5050\tTrain loss: 5591.8974609375\n",
      "Epoch: 5100\tTrain loss: 5058.105712890625\n",
      "Epoch: 5150\tTrain loss: 4581.263692220052\n",
      "Epoch: 5200\tTrain loss: 4123.968566894531\n",
      "Epoch: 5250\tTrain loss: 3720.2383829752603\n",
      "Epoch: 5300\tTrain loss: 3356.5289713541665\n",
      "Epoch: 5350\tTrain loss: 3024.8069661458335\n",
      "Epoch: 5400\tTrain loss: 2742.787801106771\n",
      "Epoch: 5450\tTrain loss: 2472.463582356771\n",
      "Epoch: 5500\tTrain loss: 2249.0789388020835\n",
      "Epoch: 5550\tTrain loss: 2044.4431966145833\n",
      "Epoch: 5600\tTrain loss: 8302.996704101562\n",
      "Epoch: 5650\tTrain loss: 1726.0370686848958\n",
      "Epoch: 5700\tTrain loss: 1591.7204182942708\n",
      "Epoch: 5750\tTrain loss: 1466.9716796875\n",
      "Epoch: 5800\tTrain loss: 1363.8997395833333\n",
      "Epoch: 5850\tTrain loss: 1276.7886555989583\n",
      "Epoch: 5900\tTrain loss: 1202.2474365234375\n",
      "Epoch: 5950\tTrain loss: 1137.9820963541667\n",
      "Epoch: 6000\tTrain loss: 1081.4141031901042\n",
      "Epoch: 6050\tTrain loss: 1032.507792154948\n",
      "Epoch: 6100\tTrain loss: 990.8790486653646\n",
      "Epoch: 6150\tTrain loss: 970.7745564778646\n",
      "Epoch: 6200\tTrain loss: 918.3922119140625\n",
      "Epoch: 6250\tTrain loss: 885.7482198079427\n",
      "Epoch: 6300\tTrain loss: 1663.5674641927083\n",
      "Epoch: 6350\tTrain loss: 1556.6201578776042\n",
      "Epoch: 6400\tTrain loss: 1472.2023111979167\n",
      "Epoch: 6450\tTrain loss: 1380.4094645182292\n",
      "Epoch: 6500\tTrain loss: 1265.0248006184895\n",
      "Epoch: 6550\tTrain loss: 898.1702473958334\n",
      "Epoch: 6600\tTrain loss: 809.2261352539062\n",
      "Epoch: 6650\tTrain loss: 783.9394327799479\n",
      "Epoch: 6700\tTrain loss: 802.6247762044271\n",
      "Epoch: 6750\tTrain loss: 755.6801554361979\n",
      "Epoch: 6800\tTrain loss: 727.8258056640625\n",
      "Epoch: 6850\tTrain loss: 708.6351013183594\n",
      "Epoch: 6900\tTrain loss: 694.3000055948893\n",
      "Epoch: 6950\tTrain loss: 683.1275431315104\n",
      "Epoch: 7000\tTrain loss: 673.8296012878418\n",
      "Epoch: 7050\tTrain loss: 665.0113805135092\n",
      "Epoch: 7100\tTrain loss: 657.2360636393229\n",
      "Epoch: 7150\tTrain loss: 649.8870747884115\n",
      "Epoch: 7200\tTrain loss: 643.2256774902344\n",
      "Epoch: 7250\tTrain loss: 636.5236206054688\n",
      "Epoch: 7300\tTrain loss: 629.8267332712809\n",
      "Epoch: 7350\tTrain loss: 649.1614017486572\n",
      "Epoch: 7400\tTrain loss: 619.5567502975464\n",
      "Epoch: 7450\tTrain loss: 613.1654103597006\n",
      "Epoch: 7500\tTrain loss: 607.3045043945312\n",
      "Epoch: 7550\tTrain loss: 601.1768493652344\n",
      "Epoch: 7600\tTrain loss: 600.857835928599\n",
      "Epoch: 7650\tTrain loss: 589.1072082519531\n",
      "Epoch: 7700\tTrain loss: 582.7175115744272\n",
      "Epoch: 7750\tTrain loss: 575.3829244772593\n",
      "Epoch: 7800\tTrain loss: 568.4294738769531\n",
      "Epoch: 7850\tTrain loss: 559.5939737955729\n",
      "Epoch: 7900\tTrain loss: 547.2907017469406\n",
      "Epoch: 7950\tTrain loss: 536.432373046875\n",
      "Epoch: 8000\tTrain loss: 523.0935770670573\n",
      "Epoch: 8050\tTrain loss: 497.9658508300781\n",
      "Epoch: 8100\tTrain loss: 475.03590901692706\n",
      "Epoch: 8150\tTrain loss: 464.1654357910156\n",
      "Epoch: 8200\tTrain loss: 438.2175547281901\n",
      "Epoch: 8250\tTrain loss: 524.4016621907552\n",
      "Epoch: 8300\tTrain loss: 423.2106170654297\n",
      "Epoch: 8350\tTrain loss: 406.7359263102214\n",
      "Epoch: 8400\tTrain loss: 392.456537882487\n",
      "Epoch: 8450\tTrain loss: 379.08502197265625\n",
      "Epoch: 8500\tTrain loss: 367.28611755371094\n",
      "Epoch: 8550\tTrain loss: 355.0154622395833\n",
      "Epoch: 8600\tTrain loss: 343.42735441525775\n",
      "Epoch: 8650\tTrain loss: 341.00404357910156\n",
      "Epoch: 8700\tTrain loss: 324.12733459472656\n",
      "Epoch: 8750\tTrain loss: 312.0471420288086\n",
      "Epoch: 8800\tTrain loss: 300.3438204129537\n",
      "Epoch: 8850\tTrain loss: 281.93516127268475\n",
      "Epoch: 8900\tTrain loss: 260.42440962791443\n",
      "Epoch: 8950\tTrain loss: 243.61137517293295\n",
      "Epoch: 9000\tTrain loss: 226.53745714823404\n",
      "Epoch: 9050\tTrain loss: 196.47982343037924\n",
      "Epoch: 9100\tTrain loss: 179.521550655365\n",
      "Epoch: 9150\tTrain loss: 164.23684438069662\n",
      "Epoch: 9200\tTrain loss: 151.5302480061849\n",
      "Epoch: 9250\tTrain loss: 139.6069761912028\n",
      "Epoch: 9300\tTrain loss: 129.7183609008789\n",
      "Epoch: 9350\tTrain loss: 120.14610417683919\n",
      "Epoch: 9400\tTrain loss: 115.68138249715169\n",
      "Epoch: 9450\tTrain loss: 105.95486990610759\n",
      "Epoch: 9500\tTrain loss: 98.7110759417216\n",
      "Epoch: 9550\tTrain loss: 94.56721146901448\n",
      "Epoch: 9600\tTrain loss: 88.03422427177429\n",
      "Epoch: 9650\tTrain loss: 83.09892018636067\n",
      "Epoch: 9700\tTrain loss: 78.27219692866008\n",
      "Epoch: 9750\tTrain loss: 73.4219942688942\n",
      "Epoch: 9800\tTrain loss: 2215.635052998861\n",
      "Epoch: 9850\tTrain loss: 1663.0443216959636\n",
      "Epoch: 9900\tTrain loss: 1628.8470255533855\n",
      "Epoch: 9950\tTrain loss: 472.14886983235675\n",
      "Epoch: 10000\tTrain loss: 328.76918029785156\n",
      "Epoch: 10050\tTrain loss: 13454.458312988281\n",
      "Epoch: 10100\tTrain loss: 8206.184814453125\n",
      "Epoch: 10150\tTrain loss: 7052.0105387369795\n",
      "Epoch: 10200\tTrain loss: 6356.867492675781\n",
      "Epoch: 10250\tTrain loss: 5893.7630615234375\n",
      "Epoch: 10300\tTrain loss: 5551.614318847656\n",
      "Epoch: 10350\tTrain loss: 5285.673400878906\n",
      "Epoch: 10400\tTrain loss: 5064.905049641927\n",
      "Epoch: 10450\tTrain loss: 4840.786142985026\n",
      "Epoch: 10500\tTrain loss: 3665.593953450521\n",
      "Epoch: 10550\tTrain loss: 3344.6013387044272\n",
      "Epoch: 10600\tTrain loss: 3214.7720540364585\n",
      "Epoch: 10650\tTrain loss: 3143.3437703450522\n",
      "Epoch: 10700\tTrain loss: 3045.7332560221353\n",
      "Epoch: 10750\tTrain loss: 2957.7996826171875\n",
      "Epoch: 10800\tTrain loss: 2874.8455403645835\n",
      "Epoch: 10850\tTrain loss: 2796.028279622396\n",
      "Epoch: 10900\tTrain loss: 2719.4110514322915\n",
      "Epoch: 10950\tTrain loss: 2646.3021240234375\n",
      "Epoch: 11000\tTrain loss: 2573.6756184895835\n",
      "Epoch: 11050\tTrain loss: 2502.685536702474\n",
      "Epoch: 11100\tTrain loss: 2432.6900634765625\n",
      "Epoch: 11150\tTrain loss: 2365.2015380859375\n",
      "Epoch: 11200\tTrain loss: 2298.3180541992188\n",
      "Epoch: 11250\tTrain loss: 2229.741200764974\n",
      "Epoch: 11300\tTrain loss: 2158.9077962239585\n",
      "Epoch: 11350\tTrain loss: 2082.4405924479165\n",
      "Epoch: 11400\tTrain loss: 1993.304443359375\n",
      "Epoch: 11450\tTrain loss: 1879.9086608886719\n",
      "Epoch: 11500\tTrain loss: 1715.656026204427\n",
      "Epoch: 11550\tTrain loss: 1466.6992594401042\n",
      "Epoch: 11600\tTrain loss: 1126.2505289713542\n",
      "Epoch: 11650\tTrain loss: 873.2811889648438\n",
      "Epoch: 11700\tTrain loss: 756.3562723795573\n",
      "Epoch: 11750\tTrain loss: 691.8379923502604\n",
      "Epoch: 11800\tTrain loss: 649.8350931803385\n",
      "Epoch: 11850\tTrain loss: 619.0701293945312\n",
      "Epoch: 11900\tTrain loss: 595.3492838541666\n",
      "Epoch: 11950\tTrain loss: 575.9928131103516\n",
      "Epoch: 12000\tTrain loss: 559.7048645019531\n",
      "Epoch: 12050\tTrain loss: 545.7993927001953\n",
      "Epoch: 12100\tTrain loss: 532.868398030599\n",
      "Epoch: 12150\tTrain loss: 522.5532480875651\n",
      "Epoch: 12200\tTrain loss: 510.4381917317708\n",
      "Epoch: 12250\tTrain loss: 501.1491978963216\n",
      "Epoch: 12300\tTrain loss: 490.9551289876302\n",
      "Epoch: 12350\tTrain loss: 484.2898763020833\n",
      "Epoch: 12400\tTrain loss: 479.69114176432294\n",
      "Epoch: 12450\tTrain loss: 465.8091532389323\n",
      "Epoch: 12500\tTrain loss: 456.47130330403644\n",
      "Epoch: 12550\tTrain loss: 511.13123575846356\n",
      "Epoch: 12600\tTrain loss: 443.3927307128906\n",
      "Epoch: 12650\tTrain loss: 435.6148376464844\n",
      "Epoch: 12700\tTrain loss: 438.7335917154948\n",
      "Epoch: 12750\tTrain loss: 418.1374003092448\n",
      "Epoch: 12800\tTrain loss: 406.34375\n",
      "Epoch: 12850\tTrain loss: 391.8605874379476\n",
      "Epoch: 12900\tTrain loss: 374.6502374013265\n",
      "Epoch: 12950\tTrain loss: 363.7942606608073\n",
      "Epoch: 13000\tTrain loss: 359.66712443033856\n",
      "Epoch: 13050\tTrain loss: 342.379638671875\n",
      "Epoch: 13100\tTrain loss: 388.5809275309245\n",
      "Epoch: 13150\tTrain loss: 312.11321512858075\n",
      "Epoch: 13200\tTrain loss: 332.5292714436849\n",
      "Epoch: 13250\tTrain loss: 377.1541353861491\n",
      "Epoch: 13300\tTrain loss: 316.8557586669922\n",
      "Epoch: 13350\tTrain loss: 293.998540242513\n",
      "Epoch: 13400\tTrain loss: 280.78041585286456\n",
      "Epoch: 13450\tTrain loss: 265.682861328125\n",
      "Epoch: 13500\tTrain loss: 247.88743591308594\n",
      "Epoch: 13550\tTrain loss: 769.7304077148438\n",
      "Epoch: 13600\tTrain loss: 522.6760711669922\n",
      "Epoch: 13650\tTrain loss: 493.9675547281901\n",
      "Epoch: 13700\tTrain loss: 474.99633026123047\n",
      "Epoch: 13750\tTrain loss: 460.2171452840169\n",
      "Epoch: 13800\tTrain loss: 447.54911041259766\n",
      "Epoch: 13850\tTrain loss: 435.96364339192706\n",
      "Epoch: 13900\tTrain loss: 425.05470275878906\n",
      "Epoch: 13950\tTrain loss: 414.70233154296875\n",
      "Epoch: 14000\tTrain loss: 404.17821248372394\n",
      "Epoch: 14050\tTrain loss: 392.98594665527344\n",
      "Epoch: 14100\tTrain loss: 378.2049204508464\n",
      "Epoch: 14150\tTrain loss: 360.4908752441406\n",
      "Epoch: 14200\tTrain loss: 342.3246663411458\n",
      "Epoch: 14250\tTrain loss: 324.86570231119794\n",
      "Epoch: 14300\tTrain loss: 314.1484832763672\n",
      "Epoch: 14350\tTrain loss: 311.08467356363934\n",
      "Epoch: 14400\tTrain loss: 296.9183069864909\n",
      "Epoch: 14450\tTrain loss: 282.2592233022054\n",
      "Epoch: 14500\tTrain loss: 275.6580759684245\n",
      "Epoch: 14550\tTrain loss: 265.0643666585286\n",
      "Epoch: 14600\tTrain loss: 259.8682810465495\n",
      "Epoch: 14650\tTrain loss: 253.23187383015951\n",
      "Epoch: 14700\tTrain loss: 262.20089785257977\n",
      "Epoch: 14750\tTrain loss: 244.5954386393229\n",
      "Epoch: 14800\tTrain loss: 281.8988444010417\n",
      "Epoch: 14850\tTrain loss: 240.45005798339844\n",
      "Epoch: 14900\tTrain loss: 230.87163543701172\n",
      "Epoch: 14950\tTrain loss: 225.2159595489502\n",
      "Epoch: 15000\tTrain loss: 227.74233309427896\n",
      "Epoch: 15050\tTrain loss: 216.36281458536783\n",
      "Epoch: 15100\tTrain loss: 211.75703811645508\n",
      "Epoch: 15150\tTrain loss: 209.37120532989502\n",
      "Epoch: 15200\tTrain loss: 205.3467229207357\n",
      "Epoch: 15250\tTrain loss: 214.28412755330405\n",
      "Epoch: 15300\tTrain loss: 199.74555587768555\n",
      "Epoch: 15350\tTrain loss: 198.55821402867636\n",
      "Epoch: 15400\tTrain loss: 213.93719991048178\n",
      "Epoch: 15450\tTrain loss: 193.99566586812338\n",
      "Epoch: 15500\tTrain loss: 242.68737729390463\n",
      "Epoch: 15550\tTrain loss: 199.2673740386963\n",
      "Epoch: 15600\tTrain loss: 190.02967174847922\n",
      "Epoch: 15650\tTrain loss: 187.94077491760254\n",
      "Epoch: 15700\tTrain loss: 186.0131376584371\n",
      "Epoch: 15750\tTrain loss: 185.2663278579712\n",
      "Epoch: 15800\tTrain loss: 183.41369795799255\n",
      "Epoch: 15850\tTrain loss: 182.214750289917\n",
      "Epoch: 15900\tTrain loss: 186.01149717966715\n",
      "Epoch: 15950\tTrain loss: 181.46093527475992\n",
      "Epoch: 16000\tTrain loss: 180.31189886728922\n",
      "Epoch: 16050\tTrain loss: 179.0453151067098\n",
      "Epoch: 16100\tTrain loss: 178.43858633438745\n",
      "Epoch: 16150\tTrain loss: 177.18994108835855\n",
      "Epoch: 16200\tTrain loss: 176.1660706202189\n",
      "Epoch: 16250\tTrain loss: 175.4179904361566\n",
      "Epoch: 16300\tTrain loss: 224.49081214269003\n",
      "Epoch: 16350\tTrain loss: 190.52123387654623\n",
      "Epoch: 16400\tTrain loss: 184.90058866143227\n",
      "Epoch: 16450\tTrain loss: 182.29334254314503\n",
      "Epoch: 16500\tTrain loss: 180.31976778805256\n",
      "Epoch: 16550\tTrain loss: 178.99336751302084\n",
      "Epoch: 16600\tTrain loss: 177.78547223409018\n",
      "Epoch: 16650\tTrain loss: 176.70404275258383\n",
      "Epoch: 16700\tTrain loss: 175.73359743754068\n",
      "Epoch: 16750\tTrain loss: 174.79279390970865\n",
      "Epoch: 16800\tTrain loss: 173.9478616953517\n",
      "Epoch: 16850\tTrain loss: 173.09670321146646\n",
      "Epoch: 16900\tTrain loss: 172.26524265731373\n",
      "Epoch: 16950\tTrain loss: 170.38894430796304\n",
      "Epoch: 17000\tTrain loss: 166.50454226136208\n",
      "Epoch: 17050\tTrain loss: 160.91424671808878\n",
      "Epoch: 17100\tTrain loss: 130.1993169784546\n",
      "Epoch: 17150\tTrain loss: 108.35433006286621\n",
      "Epoch: 17200\tTrain loss: 3153.7049560546875\n",
      "Epoch: 17250\tTrain loss: 1076.3899434407551\n",
      "Epoch: 17300\tTrain loss: 616.2737592061361\n",
      "Epoch: 17350\tTrain loss: 366.1316731770833\n",
      "Epoch: 17400\tTrain loss: 244.8355509440104\n",
      "Epoch: 17450\tTrain loss: 280.62724177042645\n",
      "Epoch: 17500\tTrain loss: 248.24671999613443\n",
      "Epoch: 17550\tTrain loss: 234.65451622009277\n",
      "Epoch: 17600\tTrain loss: 225.83605766296387\n",
      "Epoch: 17650\tTrain loss: 215.49093373616537\n",
      "Epoch: 17700\tTrain loss: 180.2007099787394\n",
      "Epoch: 17750\tTrain loss: 177.75824864705405\n",
      "Epoch: 17800\tTrain loss: 232.4575538635254\n",
      "Epoch: 17850\tTrain loss: 215.69813998540243\n",
      "Epoch: 17900\tTrain loss: 205.92398007710776\n",
      "Epoch: 17950\tTrain loss: 159.73773022492728\n",
      "Epoch: 18000\tTrain loss: 1134.621416727702\n",
      "Epoch: 18050\tTrain loss: 336.1732635498047\n",
      "Epoch: 18100\tTrain loss: 165.37020619710287\n",
      "Epoch: 18150\tTrain loss: 68.39698092142741\n",
      "Epoch: 18200\tTrain loss: 54.6136679649353\n",
      "Epoch: 18250\tTrain loss: 46.838752031326294\n",
      "Epoch: 18300\tTrain loss: 42.60682010650635\n",
      "Epoch: 18350\tTrain loss: 35.84790404637655\n",
      "Epoch: 18400\tTrain loss: 32.47043148676554\n",
      "Epoch: 18450\tTrain loss: 28.54274320602417\n",
      "Epoch: 18500\tTrain loss: 23.93252182006836\n",
      "Epoch: 18550\tTrain loss: 20.851660947004955\n",
      "Epoch: 18600\tTrain loss: 19.46263615290324\n",
      "Epoch: 18650\tTrain loss: 23.355170210202534\n",
      "Epoch: 18700\tTrain loss: 17.943638424078625\n",
      "Epoch: 18750\tTrain loss: 13.617773214975992\n",
      "Epoch: 18800\tTrain loss: 11.440402646859487\n",
      "Epoch: 18850\tTrain loss: 9.628619203964869\n",
      "Epoch: 18900\tTrain loss: 8.058165431022644\n",
      "Epoch: 18950\tTrain loss: 6.676651080449422\n",
      "Epoch: 19000\tTrain loss: 5.482156485319138\n",
      "Epoch: 19050\tTrain loss: 4.64235304792722\n",
      "Epoch: 19100\tTrain loss: 3.5889994899431863\n",
      "Epoch: 19150\tTrain loss: 2.850026865800222\n",
      "Epoch: 19200\tTrain loss: 2.2661730150381723\n",
      "Epoch: 19250\tTrain loss: 236.40372117360434\n",
      "Epoch: 19300\tTrain loss: 1.4304451147715251\n",
      "Epoch: 19350\tTrain loss: 1.0741263615588348\n",
      "Epoch: 19400\tTrain loss: 0.8145378356178602\n",
      "Epoch: 19450\tTrain loss: 0.6619743530948957\n",
      "Epoch: 19500\tTrain loss: 0.4686766651769479\n",
      "Epoch: 19550\tTrain loss: 0.3309169225394726\n",
      "Epoch: 19600\tTrain loss: 4.8764825115601225\n",
      "Epoch: 19650\tTrain loss: 0.16017899367337426\n",
      "Epoch: 19700\tTrain loss: 0.10621479401985805\n",
      "Epoch: 19750\tTrain loss: 0.0693678908670942\n",
      "Epoch: 19800\tTrain loss: 0.044063555309548974\n",
      "Epoch: 19850\tTrain loss: 0.027120911593859393\n",
      "Epoch: 19900\tTrain loss: 0.016724837439445157\n",
      "Epoch: 19950\tTrain loss: 0.009469559513187656\n",
      "Epoch: 20000\tTrain loss: 0.0065922936773858964\n",
      "Epoch: 20050\tTrain loss: 0.5006254681696495\n",
      "Epoch: 20100\tTrain loss: 33955.372395833336\n",
      "Epoch: 20150\tTrain loss: 2462.3479919433594\n",
      "Epoch: 20200\tTrain loss: 2159.1039428710938\n",
      "Epoch: 20250\tTrain loss: 1551.5955149332683\n",
      "Epoch: 20300\tTrain loss: 1357.5489298502605\n",
      "Epoch: 20350\tTrain loss: 1200.591044108073\n",
      "Epoch: 20400\tTrain loss: 1045.48095703125\n",
      "Epoch: 20450\tTrain loss: 441.8453420003255\n",
      "Epoch: 20500\tTrain loss: 332.42884318033856\n",
      "Epoch: 20550\tTrain loss: 7603.380289713542\n",
      "Epoch: 20600\tTrain loss: 1181.3390096028645\n",
      "Epoch: 20650\tTrain loss: 5121.081868489583\n",
      "Epoch: 20700\tTrain loss: 16145.280192057291\n",
      "Epoch: 20750\tTrain loss: 11468.677124023438\n",
      "Epoch: 20800\tTrain loss: 8749.524129231771\n",
      "Epoch: 20850\tTrain loss: 1950.5940348307292\n",
      "Epoch: 20900\tTrain loss: 1651.0133666992188\n",
      "Epoch: 20950\tTrain loss: 1274.9705708821614\n",
      "Epoch: 21000\tTrain loss: 2142.5252888997397\n",
      "Epoch: 21050\tTrain loss: 1099.9121500651042\n",
      "Epoch: 21100\tTrain loss: 926.178232828776\n",
      "Epoch: 21150\tTrain loss: 786.6875915527344\n",
      "Epoch: 21200\tTrain loss: 346.07609303792316\n",
      "Epoch: 21250\tTrain loss: 299.8064219156901\n",
      "Epoch: 21300\tTrain loss: 264.8664080301921\n",
      "Epoch: 21350\tTrain loss: 244.35971323649088\n",
      "Epoch: 21400\tTrain loss: 235.85267384847006\n",
      "Epoch: 21450\tTrain loss: 230.1273422241211\n",
      "Epoch: 21500\tTrain loss: 225.49859873453775\n",
      "Epoch: 21550\tTrain loss: 221.6254965464274\n",
      "Epoch: 21600\tTrain loss: 218.25848833719888\n",
      "Epoch: 21650\tTrain loss: 215.32660802205405\n",
      "Epoch: 21700\tTrain loss: 212.723113377889\n",
      "Epoch: 21750\tTrain loss: 210.4278106689453\n",
      "Epoch: 21800\tTrain loss: 208.39742708206177\n",
      "Epoch: 21850\tTrain loss: 206.5780200958252\n",
      "Epoch: 21900\tTrain loss: 205.01498317718506\n",
      "Epoch: 21950\tTrain loss: 203.5135529836019\n",
      "Epoch: 22000\tTrain loss: 202.2400337855021\n",
      "Epoch: 22050\tTrain loss: 200.99791558583578\n",
      "Epoch: 22100\tTrain loss: 199.90004316965738\n",
      "Epoch: 22150\tTrain loss: 198.87508789698282\n",
      "Epoch: 22200\tTrain loss: 197.88294712702432\n",
      "Epoch: 22250\tTrain loss: 197.13414414723715\n",
      "Epoch: 22300\tTrain loss: 196.20258498191833\n",
      "Epoch: 22350\tTrain loss: 195.51051394144693\n",
      "Epoch: 22400\tTrain loss: 194.71220608552298\n",
      "Epoch: 22450\tTrain loss: 194.61469976107279\n",
      "Epoch: 22500\tTrain loss: 248.71506118774414\n",
      "Epoch: 22550\tTrain loss: 195.92922973632812\n",
      "Epoch: 22600\tTrain loss: 196.3255648612976\n",
      "Epoch: 22650\tTrain loss: 193.47338803609213\n",
      "Epoch: 22700\tTrain loss: 194.29874197642008\n",
      "Epoch: 22750\tTrain loss: 187.5512217680613\n",
      "Epoch: 22800\tTrain loss: 185.29219463467598\n",
      "Epoch: 22850\tTrain loss: 182.97202626864114\n",
      "Epoch: 22900\tTrain loss: 199.06417948007584\n",
      "Epoch: 22950\tTrain loss: 167.95417833328247\n",
      "Epoch: 23000\tTrain loss: 156.78663778305054\n",
      "Epoch: 23050\tTrain loss: 149.67840898036957\n",
      "Epoch: 23100\tTrain loss: 142.82237084706625\n",
      "Epoch: 23150\tTrain loss: 134.98438819249472\n",
      "Epoch: 23200\tTrain loss: 127.10541582107544\n",
      "Epoch: 23250\tTrain loss: 121.81943632413943\n",
      "Epoch: 23300\tTrain loss: 114.35609086354573\n",
      "Epoch: 23350\tTrain loss: 124.84829457600911\n",
      "Epoch: 23400\tTrain loss: 103.98042885462444\n",
      "Epoch: 23450\tTrain loss: 99.14020776748657\n",
      "Epoch: 23500\tTrain loss: 94.15674789746602\n",
      "Epoch: 23550\tTrain loss: 89.22237237791221\n",
      "Epoch: 23600\tTrain loss: 84.43690482278664\n",
      "Epoch: 23650\tTrain loss: 79.44432488332193\n",
      "Epoch: 23700\tTrain loss: 91.61607535680135\n",
      "Epoch: 23750\tTrain loss: 49.43787240982056\n",
      "Epoch: 23800\tTrain loss: 179.94603983561197\n",
      "Epoch: 23850\tTrain loss: 71.31511688232422\n",
      "Epoch: 23900\tTrain loss: 46.78168805440267\n",
      "Epoch: 23950\tTrain loss: 37.01262410481771\n",
      "Epoch: 24000\tTrain loss: 31.491251627604168\n",
      "Epoch: 24050\tTrain loss: 27.439139048258465\n",
      "Epoch: 24100\tTrain loss: 24.040040810902912\n",
      "Epoch: 24150\tTrain loss: 21.153254985809326\n",
      "Epoch: 24200\tTrain loss: 27.791351954142254\n",
      "Epoch: 24250\tTrain loss: 74.06628195444743\n",
      "Epoch: 24300\tTrain loss: 19.71426026026408\n",
      "Epoch: 24350\tTrain loss: 17.071988185246784\n",
      "Epoch: 24400\tTrain loss: 14.770119031270346\n",
      "Epoch: 24450\tTrain loss: 12.713322162628174\n",
      "Epoch: 24500\tTrain loss: 10.932815710703531\n",
      "Epoch: 24550\tTrain loss: 29.927183945973713\n",
      "Epoch: 24600\tTrain loss: 12.465688864390055\n",
      "Epoch: 24650\tTrain loss: 10.606050809224447\n",
      "Epoch: 24700\tTrain loss: 9.326643625895182\n",
      "Epoch: 24750\tTrain loss: 8.296089967091879\n",
      "Epoch: 24800\tTrain loss: 7.367456356684367\n",
      "Epoch: 24850\tTrain loss: 6.557360092798869\n",
      "Epoch: 24900\tTrain loss: 5.884798924128215\n",
      "Epoch: 24950\tTrain loss: 5.153757572174072\n",
      "Epoch: 25000\tTrain loss: 4.6454160412152605\n",
      "Epoch: 25050\tTrain loss: 4.004847347736359\n",
      "Epoch: 25100\tTrain loss: 4.676712115605672\n",
      "Epoch: 25150\tTrain loss: 4.130403757095337\n",
      "Epoch: 25200\tTrain loss: 2.717445651690165\n",
      "Epoch: 25250\tTrain loss: 2.3481659293174744\n",
      "Epoch: 25300\tTrain loss: 2.0259224424759545\n",
      "Epoch: 25350\tTrain loss: 1.7415709098180134\n",
      "Epoch: 25400\tTrain loss: 1.4887230197588603\n",
      "Epoch: 25450\tTrain loss: 1.2633066425720851\n",
      "Epoch: 25500\tTrain loss: 1.0655478835105896\n",
      "Epoch: 25550\tTrain loss: 2.2715935657421746\n",
      "Epoch: 25600\tTrain loss: 0.7702730024854342\n",
      "Epoch: 25650\tTrain loss: 0.6113173936804136\n",
      "Epoch: 25700\tTrain loss: 0.494455707569917\n",
      "Epoch: 25750\tTrain loss: 0.39504516621430713\n",
      "Epoch: 25800\tTrain loss: 0.3126010075211525\n",
      "Epoch: 25850\tTrain loss: 0.2430324244002501\n",
      "Epoch: 25900\tTrain loss: 0.18609033649166426\n",
      "Epoch: 25950\tTrain loss: 0.1398489138421913\n",
      "Epoch: 26000\tTrain loss: 0.10346577937404315\n",
      "Epoch: 26050\tTrain loss: 0.07461401633918285\n",
      "Epoch: 26100\tTrain loss: 0.0524795763194561\n",
      "Epoch: 26150\tTrain loss: 0.035943320874745645\n",
      "Epoch: 26200\tTrain loss: 0.04785376135259867\n",
      "Epoch: 26250\tTrain loss: 8.217038949330648\n",
      "Epoch: 26300\tTrain loss: 358.69006474812824\n",
      "Epoch: 26350\tTrain loss: 303.1383927663167\n",
      "Epoch: 26400\tTrain loss: 276.5252227783203\n",
      "Epoch: 26450\tTrain loss: 251.4647356669108\n",
      "Epoch: 26500\tTrain loss: 241.95932070414224\n",
      "Epoch: 26550\tTrain loss: 234.40462112426758\n",
      "Epoch: 26600\tTrain loss: 232.70076115926108\n",
      "Epoch: 26650\tTrain loss: 216.87693027655283\n",
      "Epoch: 26700\tTrain loss: 582.7432861328125\n",
      "Epoch: 26750\tTrain loss: 283.9574940999349\n",
      "Epoch: 26800\tTrain loss: 272.4369911154111\n",
      "Epoch: 26850\tTrain loss: 264.89161247015\n",
      "Epoch: 26900\tTrain loss: 259.0125198364258\n",
      "Epoch: 26950\tTrain loss: 253.96594111124674\n",
      "Epoch: 27000\tTrain loss: 249.24755986531576\n",
      "Epoch: 27050\tTrain loss: 244.84621556599936\n",
      "Epoch: 27100\tTrain loss: 240.27501619358858\n",
      "Epoch: 27150\tTrain loss: 236.72851371765137\n",
      "Epoch: 27200\tTrain loss: 232.8281447092692\n",
      "Epoch: 27250\tTrain loss: 228.84460130209723\n",
      "Epoch: 27300\tTrain loss: 226.03886858870587\n",
      "Epoch: 27350\tTrain loss: 224.11237049102783\n",
      "Epoch: 27400\tTrain loss: 222.75641125626862\n",
      "Epoch: 27450\tTrain loss: 221.5705483754476\n",
      "Epoch: 27500\tTrain loss: 220.37133334220076\n",
      "Epoch: 27550\tTrain loss: 219.80253872120133\n",
      "Epoch: 27600\tTrain loss: 218.95549949010214\n",
      "Epoch: 27650\tTrain loss: 217.87186845143637\n",
      "Epoch: 27700\tTrain loss: 217.8206885655721\n",
      "Epoch: 27750\tTrain loss: 216.07555699845156\n",
      "Epoch: 27800\tTrain loss: 215.27165834108987\n",
      "Epoch: 27850\tTrain loss: 214.09496240127677\n",
      "Epoch: 27900\tTrain loss: 211.6694880706491\n",
      "Epoch: 27950\tTrain loss: 199.1392950216929\n",
      "Epoch: 28000\tTrain loss: 214.10839776694775\n",
      "Epoch: 28050\tTrain loss: 206.50726628303528\n",
      "Epoch: 28100\tTrain loss: 203.99165026346842\n",
      "Epoch: 28150\tTrain loss: 202.9063942039696\n",
      "Epoch: 28200\tTrain loss: 201.95715892439088\n",
      "Epoch: 28250\tTrain loss: 201.49341975338757\n",
      "Epoch: 28300\tTrain loss: 200.81533887951323\n",
      "Epoch: 28350\tTrain loss: 200.26505796114603\n",
      "Epoch: 28400\tTrain loss: 199.63982875940079\n",
      "Epoch: 28450\tTrain loss: 199.63560684521994\n",
      "Epoch: 28500\tTrain loss: 197.3788372774919\n",
      "Epoch: 28550\tTrain loss: 201.8958125114441\n",
      "Epoch: 28600\tTrain loss: 191.52917840669397\n",
      "Epoch: 28650\tTrain loss: 198.8115486999353\n",
      "Epoch: 28700\tTrain loss: 187.0998283997178\n",
      "Epoch: 28750\tTrain loss: 237.00939605633417\n",
      "Epoch: 28800\tTrain loss: 181.67444387078285\n",
      "Epoch: 28850\tTrain loss: 264.2734375\n",
      "Epoch: 28900\tTrain loss: 236.55013147989908\n",
      "Epoch: 28950\tTrain loss: 215.90951919555664\n",
      "Epoch: 29000\tTrain loss: 197.36352348327637\n",
      "Epoch: 29050\tTrain loss: 228.56869607294598\n",
      "Epoch: 29100\tTrain loss: 217.49284582166\n",
      "Epoch: 29150\tTrain loss: 207.29858525594076\n",
      "Epoch: 29200\tTrain loss: 199.83866214752197\n",
      "Epoch: 29250\tTrain loss: 184.05732084903866\n",
      "Epoch: 29300\tTrain loss: 206.4503095249335\n",
      "Epoch: 29350\tTrain loss: 214.16493667662144\n",
      "Epoch: 29400\tTrain loss: 194.0088140865167\n",
      "Epoch: 29450\tTrain loss: 219.7525291442871\n",
      "Epoch: 29500\tTrain loss: 167.51212088267008\n",
      "Epoch: 29550\tTrain loss: 188.90062157313028\n",
      "Epoch: 29600\tTrain loss: 183.32209945718446\n",
      "Epoch: 29650\tTrain loss: 120.75205167134602\n",
      "Epoch: 29700\tTrain loss: 5432.291018803914\n",
      "Epoch: 29750\tTrain loss: 4004.842524210612\n",
      "Epoch: 29800\tTrain loss: 2534.335215250651\n",
      "Epoch: 29850\tTrain loss: 2020.400599161784\n",
      "Epoch: 29900\tTrain loss: 1587.0737813313801\n",
      "Epoch: 29950\tTrain loss: 1264.8099416097004\n",
      "Training CFRNN\n",
      "Epoch: 0\tTrain loss: 3138863.5833333335\n",
      "Epoch: 50\tTrain loss: 3044050.1666666665\n",
      "Epoch: 100\tTrain loss: 2956115.2083333335\n",
      "Epoch: 150\tTrain loss: 2872297.0833333335\n",
      "Epoch: 200\tTrain loss: 2791972.4166666665\n",
      "Epoch: 250\tTrain loss: 2714104.9635416665\n",
      "Epoch: 300\tTrain loss: 2639277.25\n",
      "Epoch: 350\tTrain loss: 2566770.0\n",
      "Epoch: 400\tTrain loss: 2496125.2083333335\n",
      "Epoch: 450\tTrain loss: 2427721.5625\n",
      "Epoch: 500\tTrain loss: 2361576.5963541665\n",
      "Epoch: 550\tTrain loss: 2297074.7083333335\n",
      "Epoch: 600\tTrain loss: 2234776.5\n",
      "Epoch: 650\tTrain loss: 2174647.7591145835\n",
      "Epoch: 700\tTrain loss: 2116132.0833333335\n",
      "Epoch: 750\tTrain loss: 2059486.2604166667\n",
      "Epoch: 800\tTrain loss: 2004771.2369791667\n",
      "Epoch: 850\tTrain loss: 1951928.3958333333\n",
      "Epoch: 900\tTrain loss: 1900668.0305989583\n",
      "Epoch: 950\tTrain loss: 1850801.9026692708\n",
      "Epoch: 1000\tTrain loss: 1803230.5686848958\n",
      "Epoch: 1050\tTrain loss: 1756886.2395833333\n",
      "Epoch: 1100\tTrain loss: 1712199.4166666667\n",
      "Epoch: 1150\tTrain loss: 1669057.4140625\n",
      "Epoch: 1200\tTrain loss: 1627669.40625\n",
      "Epoch: 1250\tTrain loss: 1587809.3658854167\n",
      "Epoch: 1300\tTrain loss: 1548981.4791666667\n",
      "Epoch: 1350\tTrain loss: 1511958.2447916667\n",
      "Epoch: 1400\tTrain loss: 1476182.109375\n",
      "Epoch: 1450\tTrain loss: 1441870.5989583333\n",
      "Epoch: 1500\tTrain loss: 1408942.2395833333\n",
      "Epoch: 1550\tTrain loss: 1377229.0729166667\n",
      "Epoch: 1600\tTrain loss: 1347092.6380208333\n",
      "Epoch: 1650\tTrain loss: 1318133.2083333333\n",
      "Epoch: 1700\tTrain loss: 1290232.0833333333\n",
      "Epoch: 1750\tTrain loss: 1263760.1979166667\n",
      "Epoch: 1800\tTrain loss: 1238377.4479166667\n",
      "Epoch: 1850\tTrain loss: 1214161.6354166667\n",
      "Epoch: 1900\tTrain loss: 1191005.4895833333\n",
      "Epoch: 1950\tTrain loss: 1169110.9375\n",
      "Epoch: 2000\tTrain loss: 1148240.2604166667\n",
      "Epoch: 2050\tTrain loss: 1128300.0104166667\n",
      "Epoch: 2100\tTrain loss: 1109537.6875\n",
      "Epoch: 2150\tTrain loss: 1091597.3177083333\n",
      "Epoch: 2200\tTrain loss: 1074690.5104166667\n",
      "Epoch: 2250\tTrain loss: 1058810.3020833333\n",
      "Epoch: 2300\tTrain loss: 1043783.5052083334\n",
      "Epoch: 2350\tTrain loss: 1029709.5052083334\n",
      "Epoch: 2400\tTrain loss: 1016415.2552083334\n",
      "Epoch: 2450\tTrain loss: 1003831.546875\n",
      "Epoch: 2500\tTrain loss: 992282.5416666666\n",
      "Epoch: 2550\tTrain loss: 923180.4114583334\n",
      "Epoch: 2600\tTrain loss: 822553.1087239584\n",
      "Epoch: 2650\tTrain loss: 797855.9727783203\n",
      "Epoch: 2700\tTrain loss: 777190.1220703125\n",
      "Epoch: 2750\tTrain loss: 751950.3783365885\n",
      "Epoch: 2800\tTrain loss: 730646.4840494791\n",
      "Epoch: 2850\tTrain loss: 711167.7434895834\n",
      "Epoch: 2900\tTrain loss: 690578.6459147135\n",
      "Epoch: 2950\tTrain loss: 671963.8880208334\n",
      "Epoch: 3000\tTrain loss: 654053.9186197916\n",
      "Epoch: 3050\tTrain loss: 636963.6979166666\n",
      "Epoch: 3100\tTrain loss: 620706.3042602539\n",
      "Epoch: 3150\tTrain loss: 605136.6136474609\n",
      "Epoch: 3200\tTrain loss: 590243.0299479166\n",
      "Epoch: 3250\tTrain loss: 576031.4622395834\n",
      "Epoch: 3300\tTrain loss: 562685.6477864584\n",
      "Epoch: 3350\tTrain loss: 549709.5426432291\n",
      "Epoch: 3400\tTrain loss: 532985.7682291666\n",
      "Epoch: 3450\tTrain loss: 519231.67203776044\n",
      "Epoch: 3500\tTrain loss: 506149.97639973956\n",
      "Epoch: 3550\tTrain loss: 493417.4749348958\n",
      "Epoch: 3600\tTrain loss: 481377.8795572917\n",
      "Epoch: 3650\tTrain loss: 469608.2807820638\n",
      "Epoch: 3700\tTrain loss: 458493.46681722003\n",
      "Epoch: 3750\tTrain loss: 447705.119140625\n",
      "Epoch: 3800\tTrain loss: 437250.0395914714\n",
      "Epoch: 3850\tTrain loss: 427488.3649088542\n",
      "Epoch: 3900\tTrain loss: 417831.1477864583\n",
      "Epoch: 3950\tTrain loss: 407442.60994466144\n",
      "Epoch: 4000\tTrain loss: 399684.4788411458\n",
      "Epoch: 4050\tTrain loss: 391026.8411458333\n",
      "Epoch: 4100\tTrain loss: 379096.12548828125\n",
      "Epoch: 4150\tTrain loss: 370323.0810546875\n",
      "Epoch: 4200\tTrain loss: 361399.7224121094\n",
      "Epoch: 4250\tTrain loss: 352842.8592936198\n",
      "Epoch: 4300\tTrain loss: 344526.46175130206\n",
      "Epoch: 4350\tTrain loss: 336411.4806722005\n",
      "Epoch: 4400\tTrain loss: 328692.62451171875\n",
      "Epoch: 4450\tTrain loss: 321151.61682128906\n",
      "Epoch: 4500\tTrain loss: 313784.84814453125\n",
      "Epoch: 4550\tTrain loss: 306644.58634440106\n",
      "Epoch: 4600\tTrain loss: 299562.07405598956\n",
      "Epoch: 4650\tTrain loss: 292330.4499511719\n",
      "Epoch: 4700\tTrain loss: 285409.5559895833\n",
      "Epoch: 4750\tTrain loss: 279184.61083984375\n",
      "Epoch: 4800\tTrain loss: 271707.4180501302\n",
      "Epoch: 4850\tTrain loss: 264899.330078125\n",
      "Epoch: 4900\tTrain loss: 258214.94986979166\n",
      "Epoch: 4950\tTrain loss: 251627.92759195963\n",
      "Epoch: 5000\tTrain loss: 245056.2763671875\n",
      "Epoch: 5050\tTrain loss: 238682.81760660806\n",
      "Epoch: 5100\tTrain loss: 232390.140625\n",
      "Epoch: 5150\tTrain loss: 226164.99798583984\n",
      "Epoch: 5200\tTrain loss: 220084.96256510416\n",
      "Epoch: 5250\tTrain loss: 213983.15834554037\n",
      "Epoch: 5300\tTrain loss: 207990.44970703125\n",
      "Epoch: 5350\tTrain loss: 202112.29402669272\n",
      "Epoch: 5400\tTrain loss: 196427.85095214844\n",
      "Epoch: 5450\tTrain loss: 190702.03900146484\n",
      "Epoch: 5500\tTrain loss: 185434.27311197916\n",
      "Epoch: 5550\tTrain loss: 179677.58439127603\n",
      "Epoch: 5600\tTrain loss: 174257.04516601562\n",
      "Epoch: 5650\tTrain loss: 168960.544535319\n",
      "Epoch: 5700\tTrain loss: 163773.7752278646\n",
      "Epoch: 5750\tTrain loss: 158735.43587239584\n",
      "Epoch: 5800\tTrain loss: 153621.23470052084\n",
      "Epoch: 5850\tTrain loss: 148714.05330403647\n",
      "Epoch: 5900\tTrain loss: 143921.96732584634\n",
      "Epoch: 5950\tTrain loss: 139114.14579264322\n",
      "Epoch: 6000\tTrain loss: 134485.33473714194\n",
      "Epoch: 6050\tTrain loss: 129912.32084147136\n",
      "Epoch: 6100\tTrain loss: 125492.56156412761\n",
      "Epoch: 6150\tTrain loss: 121164.69299316406\n",
      "Epoch: 6200\tTrain loss: 116807.64680989583\n",
      "Epoch: 6250\tTrain loss: 112598.26118977864\n",
      "Epoch: 6300\tTrain loss: 108525.9775390625\n",
      "Epoch: 6350\tTrain loss: 104502.73701985677\n",
      "Epoch: 6400\tTrain loss: 100607.8383992513\n",
      "Epoch: 6450\tTrain loss: 96768.0731608073\n",
      "Epoch: 6500\tTrain loss: 93006.8055826823\n",
      "Epoch: 6550\tTrain loss: 89391.75769042969\n",
      "Epoch: 6600\tTrain loss: 85760.4526163737\n",
      "Epoch: 6650\tTrain loss: 82321.69437662761\n",
      "Epoch: 6700\tTrain loss: 78975.64896647136\n",
      "Epoch: 6750\tTrain loss: 75610.97233072917\n",
      "Epoch: 6800\tTrain loss: 72396.49912516277\n",
      "Epoch: 6850\tTrain loss: 69259.23270670573\n",
      "Epoch: 6900\tTrain loss: 66353.84167480469\n",
      "Epoch: 6950\tTrain loss: 63274.28556315104\n",
      "Epoch: 7000\tTrain loss: 60401.975748697914\n",
      "Epoch: 7050\tTrain loss: 57601.179361979164\n",
      "Epoch: 7100\tTrain loss: 54915.802897135414\n",
      "Epoch: 7150\tTrain loss: 52322.39298502604\n",
      "Epoch: 7200\tTrain loss: 49719.81140136719\n",
      "Epoch: 7250\tTrain loss: 47406.99450683594\n",
      "Epoch: 7300\tTrain loss: 44952.806579589844\n",
      "Epoch: 7350\tTrain loss: 42616.42435709635\n",
      "Epoch: 7400\tTrain loss: 40378.591959635414\n",
      "Epoch: 7450\tTrain loss: 38240.15079752604\n",
      "Epoch: 7500\tTrain loss: 36186.69942220052\n",
      "Epoch: 7550\tTrain loss: 34200.38077799479\n",
      "Epoch: 7600\tTrain loss: 32261.181518554688\n",
      "Epoch: 7650\tTrain loss: 30423.08426920573\n",
      "Epoch: 7700\tTrain loss: 28669.639729817707\n",
      "Epoch: 7750\tTrain loss: 26967.324178059895\n",
      "Epoch: 7800\tTrain loss: 25364.10009765625\n",
      "Epoch: 7850\tTrain loss: 23792.320841471355\n",
      "Epoch: 7900\tTrain loss: 22345.162353515625\n",
      "Epoch: 7950\tTrain loss: 20901.272318522137\n",
      "Epoch: 8000\tTrain loss: 19546.36405436198\n",
      "Epoch: 8050\tTrain loss: 18255.07684326172\n",
      "Epoch: 8100\tTrain loss: 17044.078165690105\n",
      "Epoch: 8150\tTrain loss: 15905.942952473959\n",
      "Epoch: 8200\tTrain loss: 14803.783894856771\n",
      "Epoch: 8250\tTrain loss: 13771.212687174479\n",
      "Epoch: 8300\tTrain loss: 12803.524190266928\n",
      "Epoch: 8350\tTrain loss: 11883.040730794271\n",
      "Epoch: 8400\tTrain loss: 11025.427856445312\n",
      "Epoch: 8450\tTrain loss: 10219.615437825521\n",
      "Epoch: 8500\tTrain loss: 9457.537231445312\n",
      "Epoch: 8550\tTrain loss: 8739.522684733072\n",
      "Epoch: 8600\tTrain loss: 8082.1575113932295\n",
      "Epoch: 8650\tTrain loss: 7463.3586018880205\n",
      "Epoch: 8700\tTrain loss: 6875.7991943359375\n",
      "Epoch: 8750\tTrain loss: 6347.195149739583\n",
      "Epoch: 8800\tTrain loss: 5837.429880777995\n",
      "Epoch: 8850\tTrain loss: 5379.440043131511\n",
      "Epoch: 8900\tTrain loss: 5361.2367757161455\n",
      "Epoch: 8950\tTrain loss: 4563.082784016927\n",
      "Epoch: 9000\tTrain loss: 4210.573954264323\n",
      "Epoch: 9050\tTrain loss: 3878.092071533203\n",
      "Epoch: 9100\tTrain loss: 3583.3035278320312\n",
      "Epoch: 9150\tTrain loss: 3307.5941263834634\n",
      "Epoch: 9200\tTrain loss: 3058.5054321289062\n",
      "Epoch: 9250\tTrain loss: 2825.7783813476562\n",
      "Epoch: 9300\tTrain loss: 2614.9464721679688\n",
      "Epoch: 9350\tTrain loss: 2425.7269999186196\n",
      "Epoch: 9400\tTrain loss: 2249.9368286132812\n",
      "Epoch: 9450\tTrain loss: 2089.9046427408853\n",
      "Epoch: 9500\tTrain loss: 1945.1047871907551\n",
      "Epoch: 9550\tTrain loss: 1816.4937744140625\n",
      "Epoch: 9600\tTrain loss: 1694.1484375\n",
      "Epoch: 9650\tTrain loss: 1582.4094136555989\n",
      "Epoch: 9700\tTrain loss: 1480.5733439127605\n",
      "Epoch: 9750\tTrain loss: 1376.3464965820312\n",
      "Epoch: 9800\tTrain loss: 1300.6696370442708\n",
      "Epoch: 9850\tTrain loss: 1218.6334533691406\n",
      "Epoch: 9900\tTrain loss: 1129.3183898925781\n",
      "Epoch: 9950\tTrain loss: 1057.3772684733074\n",
      "Epoch: 10000\tTrain loss: 989.8492838541666\n",
      "Epoch: 10050\tTrain loss: 924.1992594401041\n",
      "Epoch: 10100\tTrain loss: 864.6840311686198\n",
      "Epoch: 10150\tTrain loss: 809.772705078125\n",
      "Epoch: 10200\tTrain loss: 758.4432067871094\n",
      "Epoch: 10250\tTrain loss: 711.5609079996744\n",
      "Epoch: 10300\tTrain loss: 668.437245686849\n",
      "Epoch: 10350\tTrain loss: 622.8992614746094\n",
      "Epoch: 10400\tTrain loss: 923.3975423177084\n",
      "Epoch: 10450\tTrain loss: 612.2612915039062\n",
      "Epoch: 10500\tTrain loss: 574.4245402018229\n",
      "Epoch: 10550\tTrain loss: 542.8006490071615\n",
      "Epoch: 10600\tTrain loss: 515.071523030599\n",
      "Epoch: 10650\tTrain loss: 489.5675760904948\n",
      "Epoch: 10700\tTrain loss: 465.49705505371094\n",
      "Epoch: 10750\tTrain loss: 444.1270751953125\n",
      "Epoch: 10800\tTrain loss: 420.26966349283856\n",
      "Epoch: 10850\tTrain loss: 824.319081624349\n",
      "Epoch: 10900\tTrain loss: 388.43675740559894\n",
      "Epoch: 10950\tTrain loss: 365.9129943847656\n",
      "Epoch: 11000\tTrain loss: 345.42112223307294\n",
      "Epoch: 11050\tTrain loss: 325.0989634195964\n",
      "Epoch: 11100\tTrain loss: 307.26259358723956\n",
      "Epoch: 11150\tTrain loss: 326.55499776204425\n",
      "Epoch: 11200\tTrain loss: 307.7999801635742\n",
      "Epoch: 11250\tTrain loss: 291.9926045735677\n",
      "Epoch: 11300\tTrain loss: 278.8660074869792\n",
      "Epoch: 11350\tTrain loss: 265.05765533447266\n",
      "Epoch: 11400\tTrain loss: 251.5300038655599\n",
      "Epoch: 11450\tTrain loss: 235.46211751302084\n",
      "Epoch: 11500\tTrain loss: 214.55731455485025\n",
      "Epoch: 11550\tTrain loss: 199.06927617390951\n",
      "Epoch: 11600\tTrain loss: 187.929079691569\n",
      "Epoch: 11650\tTrain loss: 174.96886698404947\n",
      "Epoch: 11700\tTrain loss: 163.72368367513022\n",
      "Epoch: 11750\tTrain loss: 152.76577631632486\n",
      "Epoch: 11800\tTrain loss: 142.37070846557617\n",
      "Epoch: 11850\tTrain loss: 132.20463180541992\n",
      "Epoch: 11900\tTrain loss: 154.03425979614258\n",
      "Epoch: 11950\tTrain loss: 119.73262023925781\n",
      "Epoch: 12000\tTrain loss: 110.6677843729655\n",
      "Epoch: 12050\tTrain loss: 101.63025856018066\n",
      "Epoch: 12100\tTrain loss: 89.31782150268555\n",
      "Epoch: 12150\tTrain loss: 73.79111735026042\n",
      "Epoch: 12200\tTrain loss: 64.16496149698894\n",
      "Epoch: 12250\tTrain loss: 55.4459867477417\n",
      "Epoch: 12300\tTrain loss: 48.096640268961586\n",
      "Epoch: 12350\tTrain loss: 41.58518727620443\n",
      "Epoch: 12400\tTrain loss: 35.73676554361979\n",
      "Epoch: 12450\tTrain loss: 30.878299713134766\n",
      "Epoch: 12500\tTrain loss: 26.17288366953532\n",
      "Epoch: 12550\tTrain loss: 22.184293746948242\n",
      "Epoch: 12600\tTrain loss: 18.711165984471638\n",
      "Epoch: 12650\tTrain loss: 15.703365961710611\n",
      "Epoch: 12700\tTrain loss: 1572.1572380065918\n",
      "Epoch: 12750\tTrain loss: 307.32215245564777\n",
      "Epoch: 12800\tTrain loss: 270.6907895406087\n",
      "Epoch: 12850\tTrain loss: 250.83834624290466\n",
      "Epoch: 12900\tTrain loss: 183.37911669413248\n",
      "Epoch: 12950\tTrain loss: 163.42876116434732\n",
      "Epoch: 13000\tTrain loss: 145.56711514790854\n",
      "Epoch: 13050\tTrain loss: 133.52628993988037\n",
      "Epoch: 13100\tTrain loss: 127.07739448547363\n",
      "Epoch: 13150\tTrain loss: 117.41057520111401\n",
      "Epoch: 13200\tTrain loss: 196.3291130065918\n",
      "Epoch: 13250\tTrain loss: 166.51350911458334\n",
      "Epoch: 13300\tTrain loss: 142.64625676472983\n",
      "Epoch: 13350\tTrain loss: 151.14597829182944\n",
      "Epoch: 13400\tTrain loss: 112.99152119954427\n",
      "Epoch: 13450\tTrain loss: 98.35424550374348\n",
      "Epoch: 13500\tTrain loss: 93.64275614420573\n",
      "Epoch: 13550\tTrain loss: 76.5226338704427\n",
      "Epoch: 13600\tTrain loss: 66.89952278137207\n",
      "Epoch: 13650\tTrain loss: 55.392704010009766\n",
      "Epoch: 13700\tTrain loss: 205.71924996376038\n",
      "Epoch: 13750\tTrain loss: 41.654214223225914\n",
      "Epoch: 13800\tTrain loss: 32.33174057801565\n",
      "Epoch: 13850\tTrain loss: 25.759125391642254\n",
      "Epoch: 13900\tTrain loss: 72.53933779398601\n",
      "Epoch: 13950\tTrain loss: 18.378655115763348\n",
      "Epoch: 14000\tTrain loss: 13.903151035308838\n",
      "Epoch: 14050\tTrain loss: 51.337116718292236\n",
      "Epoch: 14100\tTrain loss: 35.81333621342977\n",
      "Epoch: 14150\tTrain loss: 8030.667453765869\n",
      "Epoch: 14200\tTrain loss: 254.78451283772787\n",
      "Epoch: 14250\tTrain loss: 47.70343621571859\n",
      "Epoch: 14300\tTrain loss: 36.30541960398356\n",
      "Epoch: 14350\tTrain loss: 29.25596555074056\n",
      "Epoch: 14400\tTrain loss: 23.52150058746338\n",
      "Epoch: 14450\tTrain loss: 18.398717403411865\n",
      "Epoch: 14500\tTrain loss: 14.484786430994669\n",
      "Epoch: 14550\tTrain loss: 13.399960199991861\n",
      "Epoch: 14600\tTrain loss: 10.794636011123657\n",
      "Epoch: 14650\tTrain loss: 8.476715286572775\n",
      "Epoch: 14700\tTrain loss: 6.550353765487671\n",
      "Epoch: 14750\tTrain loss: 5.056970715522766\n",
      "Epoch: 14800\tTrain loss: 3.828569526473681\n",
      "Epoch: 14850\tTrain loss: 3.6680467625459037\n",
      "Epoch: 14900\tTrain loss: 2.5874327520529428\n",
      "Epoch: 14950\tTrain loss: 2.49981360634168\n",
      "Epoch: 15000\tTrain loss: 7.661377012729645\n",
      "Epoch: 15050\tTrain loss: 1.1444311539332073\n",
      "Epoch: 15100\tTrain loss: 0.7547208517789841\n",
      "Epoch: 15150\tTrain loss: 0.6863163659969965\n",
      "Epoch: 15200\tTrain loss: 0.39729519188404083\n",
      "Epoch: 15250\tTrain loss: 0.2412425329287847\n",
      "Epoch: 15300\tTrain loss: 0.14965810626745224\n",
      "Epoch: 15350\tTrain loss: 1.1106902106354635\n",
      "Epoch: 15400\tTrain loss: 0.2198695888121923\n",
      "Epoch: 15450\tTrain loss: 189.08995791276297\n",
      "Epoch: 15500\tTrain loss: 6.556961278120677\n",
      "Epoch: 15550\tTrain loss: 2.671645164489746\n",
      "Epoch: 15600\tTrain loss: 1.2802952925364177\n",
      "Epoch: 15650\tTrain loss: 0.6173921391988794\n",
      "Epoch: 15700\tTrain loss: 0.2960857419917981\n",
      "Epoch: 15750\tTrain loss: 0.1439792311284691\n",
      "Epoch: 15800\tTrain loss: 0.07271051499992609\n",
      "Epoch: 15850\tTrain loss: 0.03831227729097009\n",
      "Epoch: 15900\tTrain loss: 0.020936035240689915\n",
      "Epoch: 15950\tTrain loss: 0.011679692775942385\n",
      "Epoch: 16000\tTrain loss: 0.006561989415786229\n",
      "Epoch: 16050\tTrain loss: 0.003617864567786455\n",
      "Epoch: 16100\tTrain loss: 0.0019411555695114657\n",
      "Epoch: 16150\tTrain loss: 0.0010219950733395915\n",
      "Epoch: 16200\tTrain loss: 0.0005212115593167255\n",
      "Epoch: 16250\tTrain loss: 0.0002595715592785079\n",
      "Epoch: 16300\tTrain loss: 0.00014462533120725615\n",
      "Epoch: 16350\tTrain loss: 0.0017743906931476279\n",
      "Epoch: 16400\tTrain loss: 85.43862994511922\n",
      "Epoch: 16450\tTrain loss: 0.9405627995729446\n",
      "Epoch: 16500\tTrain loss: 0.27159835894902545\n",
      "Epoch: 16550\tTrain loss: 0.11369489381710689\n",
      "Epoch: 16600\tTrain loss: 0.05854207120137289\n",
      "Epoch: 16650\tTrain loss: 0.03665599366649985\n",
      "Epoch: 16700\tTrain loss: 12.793148977061113\n",
      "Epoch: 16750\tTrain loss: 0.24350815452635288\n",
      "Epoch: 16800\tTrain loss: 0.030123988011231024\n",
      "Epoch: 16850\tTrain loss: 0.004293906968086958\n",
      "Epoch: 16900\tTrain loss: 0.0009870426438283175\n",
      "Epoch: 16950\tTrain loss: 0.10779527487951175\n",
      "Epoch: 17000\tTrain loss: 0.4833705483470112\n",
      "Epoch: 17050\tTrain loss: 0.0007762893801555037\n",
      "Epoch: 17100\tTrain loss: 0.00011633117537712678\n",
      "Epoch: 17150\tTrain loss: 3.252977830925374e-05\n",
      "Epoch: 17200\tTrain loss: 1.1787175670482005e-05\n",
      "Epoch: 17250\tTrain loss: 7.276542457172279e-06\n",
      "Epoch: 17300\tTrain loss: 0.045738094175855316\n",
      "Epoch: 17350\tTrain loss: 0.08221767097711563\n",
      "Epoch: 17400\tTrain loss: 0.014371438863842437\n",
      "Epoch: 17450\tTrain loss: 21.66849351208657\n",
      "Epoch: 17500\tTrain loss: 0.1824808462212483\n",
      "Epoch: 17550\tTrain loss: 0.003260143722097079\n",
      "Epoch: 17600\tTrain loss: 0.0005630976456814096\n",
      "Epoch: 17650\tTrain loss: 0.00014507711603073403\n",
      "Epoch: 17700\tTrain loss: 3.838971161940208e-05\n",
      "Epoch: 17750\tTrain loss: 1.0401662175733387e-05\n",
      "Epoch: 17800\tTrain loss: 3.4972196563861266e-06\n",
      "Epoch: 17850\tTrain loss: 1.2144601081824173e-06\n",
      "Epoch: 17900\tTrain loss: 7.717456173376528e-07\n",
      "Epoch: 17950\tTrain loss: 1.2550295972838892e-06\n",
      "Epoch: 18000\tTrain loss: 0.1085000545620763\n",
      "Epoch: 18050\tTrain loss: 1.6397341688474019\n",
      "Epoch: 18100\tTrain loss: 0.036558760330080986\n",
      "Epoch: 18150\tTrain loss: 0.002963206611942345\n",
      "Epoch: 18200\tTrain loss: 6.899798851615439e-05\n",
      "Epoch: 18250\tTrain loss: 1.4099960187271185e-05\n",
      "Epoch: 18300\tTrain loss: 2.856022145654909e-05\n",
      "Epoch: 18350\tTrain loss: 0.01802085995829354\n",
      "Epoch: 18400\tTrain loss: 13.77945073445638\n",
      "Epoch: 18450\tTrain loss: 0.00886531826108694\n",
      "Epoch: 18500\tTrain loss: 0.001485933554855971\n",
      "Epoch: 18550\tTrain loss: 0.0004081956625062351\n",
      "Epoch: 18600\tTrain loss: 0.0001174162610671677\n",
      "Epoch: 18650\tTrain loss: 3.431042508357981e-05\n",
      "Epoch: 18700\tTrain loss: 1.0625138344266816e-05\n",
      "Epoch: 18750\tTrain loss: 3.832206135238418e-06\n",
      "Epoch: 18800\tTrain loss: 1.7375788653832085e-06\n",
      "Epoch: 18850\tTrain loss: 1.1980326727704476e-06\n",
      "Epoch: 18900\tTrain loss: 0.21935041241037348\n",
      "Epoch: 18950\tTrain loss: 2.919743336737156\n",
      "Epoch: 19000\tTrain loss: 0.020532602133850258\n",
      "Epoch: 19050\tTrain loss: 0.007620406259472172\n",
      "Epoch: 19100\tTrain loss: 6.2858545780181885\n",
      "Epoch: 19150\tTrain loss: 0.00656075911441197\n",
      "Epoch: 19200\tTrain loss: 0.00030899922421667725\n",
      "Epoch: 19250\tTrain loss: 2.9044503875752525e-05\n",
      "Epoch: 19300\tTrain loss: 4.745683590101635e-06\n",
      "Epoch: 19350\tTrain loss: 1.2691132837971963e-06\n",
      "Epoch: 19400\tTrain loss: 5.515188907049643e-07\n",
      "Epoch: 19450\tTrain loss: 1.142675876053545e-05\n",
      "Epoch: 19500\tTrain loss: 34.18739861249924\n",
      "Epoch: 19550\tTrain loss: 7594.909962972005\n",
      "Epoch: 19600\tTrain loss: 1292.7480494181316\n",
      "Epoch: 19650\tTrain loss: 1086.7363535563152\n",
      "Epoch: 19700\tTrain loss: 945.5291646321615\n",
      "Epoch: 19750\tTrain loss: 851.0098775227865\n",
      "Epoch: 19800\tTrain loss: 809.6079686482748\n",
      "Epoch: 19850\tTrain loss: 727.0409495035807\n",
      "Epoch: 19900\tTrain loss: 682.7482172648112\n",
      "Epoch: 19950\tTrain loss: 646.4088694254557\n",
      "Epoch: 20000\tTrain loss: 731.1045277913412\n",
      "Epoch: 20050\tTrain loss: 581.3804550170898\n",
      "Epoch: 20100\tTrain loss: 541.8039970397949\n",
      "Epoch: 20150\tTrain loss: 596.7026494344076\n",
      "Epoch: 20200\tTrain loss: 513.1073048909506\n",
      "Epoch: 20250\tTrain loss: 485.4839172363281\n",
      "Epoch: 20300\tTrain loss: 460.6560338338216\n",
      "Epoch: 20350\tTrain loss: 437.4340108235677\n",
      "Epoch: 20400\tTrain loss: 414.8941535949707\n",
      "Epoch: 20450\tTrain loss: 392.0412368774414\n",
      "Epoch: 20500\tTrain loss: 369.2230224609375\n",
      "Epoch: 20550\tTrain loss: 346.3744812011719\n",
      "Epoch: 20600\tTrain loss: 322.11680094401044\n",
      "Epoch: 20650\tTrain loss: 299.9484634399414\n",
      "Epoch: 20700\tTrain loss: 280.81463623046875\n",
      "Epoch: 20750\tTrain loss: 263.4583435058594\n",
      "Epoch: 20800\tTrain loss: 247.2315533955892\n",
      "Epoch: 20850\tTrain loss: 231.8006591796875\n",
      "Epoch: 20900\tTrain loss: 217.22888946533203\n",
      "Epoch: 20950\tTrain loss: 203.94830830891928\n",
      "Epoch: 21000\tTrain loss: 190.42149607340494\n",
      "Epoch: 21050\tTrain loss: 180.83050537109375\n",
      "Epoch: 21100\tTrain loss: 167.08675130208334\n",
      "Epoch: 21150\tTrain loss: 1242.5008850097656\n",
      "Epoch: 21200\tTrain loss: 389.18970489501953\n",
      "Epoch: 21250\tTrain loss: 338.96299743652344\n",
      "Epoch: 21300\tTrain loss: 319.74735895792645\n",
      "Epoch: 21350\tTrain loss: 305.78314717610675\n",
      "Epoch: 21400\tTrain loss: 293.53131612141925\n",
      "Epoch: 21450\tTrain loss: 282.07440185546875\n",
      "Epoch: 21500\tTrain loss: 271.24023310343426\n",
      "Epoch: 21550\tTrain loss: 260.68860880533856\n",
      "Epoch: 21600\tTrain loss: 250.64383443196616\n",
      "Epoch: 21650\tTrain loss: 240.49282582600912\n",
      "Epoch: 21700\tTrain loss: 230.82546742757162\n",
      "Epoch: 21750\tTrain loss: 221.14522552490234\n",
      "Epoch: 21800\tTrain loss: 211.58516693115234\n",
      "Epoch: 21850\tTrain loss: 201.82103983561197\n",
      "Epoch: 21900\tTrain loss: 191.91803614298502\n",
      "Epoch: 21950\tTrain loss: 181.50351333618164\n",
      "Epoch: 22000\tTrain loss: 169.5341771443685\n",
      "Epoch: 22050\tTrain loss: 153.183474222819\n",
      "Epoch: 22100\tTrain loss: 135.03881327311197\n",
      "Epoch: 22150\tTrain loss: 125.47784996032715\n",
      "Epoch: 22200\tTrain loss: 108.62370618184407\n",
      "Epoch: 22250\tTrain loss: 94.21161142985027\n",
      "Epoch: 22300\tTrain loss: 79.3453852335612\n",
      "Epoch: 22350\tTrain loss: 73.27300707499187\n",
      "Epoch: 22400\tTrain loss: 60.1095937093099\n",
      "Epoch: 22450\tTrain loss: 68.35873254140218\n",
      "Epoch: 22500\tTrain loss: 48.68585650126139\n",
      "Epoch: 22550\tTrain loss: 42.116302490234375\n",
      "Epoch: 22600\tTrain loss: 37.0389715830485\n",
      "Epoch: 22650\tTrain loss: 244.89727147420248\n",
      "Epoch: 22700\tTrain loss: 31.155384699503582\n",
      "Epoch: 22750\tTrain loss: 26.819780190785725\n",
      "Epoch: 22800\tTrain loss: 23.344569047292072\n",
      "Epoch: 22850\tTrain loss: 24.07731278737386\n",
      "Epoch: 22900\tTrain loss: 18.7396723429362\n",
      "Epoch: 22950\tTrain loss: 17.05795192718506\n",
      "Epoch: 23000\tTrain loss: 21.48759110768636\n",
      "Epoch: 23050\tTrain loss: 13.071884473164877\n",
      "Epoch: 23100\tTrain loss: 10.929207483927408\n",
      "Epoch: 23150\tTrain loss: 9.194565693537394\n",
      "Epoch: 23200\tTrain loss: 8.451872030893961\n",
      "Epoch: 23250\tTrain loss: 12.421082019805908\n",
      "Epoch: 23300\tTrain loss: 6.226183891296387\n",
      "Epoch: 23350\tTrain loss: 9.57386302947998\n",
      "Epoch: 23400\tTrain loss: 5.4726738929748535\n",
      "Epoch: 23450\tTrain loss: 5.548498551050822\n",
      "Epoch: 23500\tTrain loss: 3.7089230020840964\n",
      "Epoch: 23550\tTrain loss: 2.9963109691937766\n",
      "Epoch: 23600\tTrain loss: 2.4635011355082193\n",
      "Epoch: 23650\tTrain loss: 64.70059935251872\n",
      "Epoch: 23700\tTrain loss: 7.089656352996826\n",
      "Epoch: 23750\tTrain loss: 1.821798284848531\n",
      "Epoch: 23800\tTrain loss: 1.3179074227809906\n",
      "Epoch: 23850\tTrain loss: 1.054060697555542\n",
      "Epoch: 23900\tTrain loss: 0.8501985172430674\n",
      "Epoch: 23950\tTrain loss: 7.574116826057434\n",
      "Epoch: 24000\tTrain loss: 41.816439628601074\n",
      "Epoch: 24050\tTrain loss: 5.255499839782715\n",
      "Epoch: 24100\tTrain loss: 1.7683329184850056\n",
      "Epoch: 24150\tTrain loss: 0.7928204933802286\n",
      "Epoch: 24200\tTrain loss: 0.8809157609939575\n",
      "Epoch: 24250\tTrain loss: 4.606493592262268\n",
      "Epoch: 24300\tTrain loss: 0.30918160018821556\n",
      "Epoch: 24350\tTrain loss: 0.3036974370479584\n",
      "Epoch: 24400\tTrain loss: 0.19481638570626578\n",
      "Epoch: 24450\tTrain loss: 0.7027804851531982\n",
      "Epoch: 24500\tTrain loss: 0.18793139792978764\n",
      "Epoch: 24550\tTrain loss: 0.856666311621666\n",
      "Epoch: 24600\tTrain loss: 0.21541531383991241\n",
      "Epoch: 24650\tTrain loss: 1.5471398780743282\n",
      "Epoch: 24700\tTrain loss: 0.04040932469069958\n",
      "Epoch: 24750\tTrain loss: 1.9827290897568066\n",
      "Epoch: 24800\tTrain loss: 6.090999102840821\n",
      "Epoch: 24850\tTrain loss: 1.3066779027382533\n",
      "Epoch: 24900\tTrain loss: 0.12259077063451211\n",
      "Epoch: 24950\tTrain loss: 0.01263481704518199\n",
      "Epoch: 25000\tTrain loss: 0.10789446994507064\n",
      "Epoch: 25050\tTrain loss: 2.8272289037704468\n",
      "Epoch: 25100\tTrain loss: 8.526140913367271\n",
      "Epoch: 25150\tTrain loss: 117.58624267578125\n",
      "Epoch: 25200\tTrain loss: 1.024161593367656\n",
      "Epoch: 25250\tTrain loss: 0.41000314118961495\n",
      "Epoch: 25300\tTrain loss: 0.19943027819196382\n",
      "Epoch: 25350\tTrain loss: 0.10720346464465062\n",
      "Epoch: 25400\tTrain loss: 0.06080338746930162\n",
      "Epoch: 25450\tTrain loss: 0.03506630069265763\n",
      "Epoch: 25500\tTrain loss: 0.021500209579244256\n",
      "Epoch: 25550\tTrain loss: 0.011992022472744187\n",
      "Epoch: 25600\tTrain loss: 0.007014045026153326\n",
      "Epoch: 25650\tTrain loss: 0.01869400404393673\n",
      "Epoch: 25700\tTrain loss: 16.36360454559326\n",
      "Epoch: 25750\tTrain loss: 0.0315701924264431\n",
      "Epoch: 25800\tTrain loss: 1.2778567628314097\n",
      "Epoch: 25850\tTrain loss: 5.774110913276672\n",
      "Epoch: 25900\tTrain loss: 0.14410719772179922\n",
      "Epoch: 25950\tTrain loss: 0.0023631545870254436\n",
      "Epoch: 26000\tTrain loss: 0.0009099012240767479\n",
      "Epoch: 26050\tTrain loss: 0.00039169065227421623\n",
      "Epoch: 26100\tTrain loss: 0.0001793306194789087\n",
      "Epoch: 26150\tTrain loss: 8.800014620646834e-05\n",
      "Epoch: 26200\tTrain loss: 0.0002245273244625423\n",
      "Epoch: 26250\tTrain loss: 0.0012117546478596826\n",
      "Epoch: 26300\tTrain loss: 22.370651404062908\n",
      "Epoch: 26350\tTrain loss: 5.890317797660828\n",
      "Epoch: 26400\tTrain loss: 1.3862415328621864\n",
      "Epoch: 26450\tTrain loss: 0.4732953955729802\n",
      "Epoch: 26500\tTrain loss: 0.24380621065696081\n",
      "Epoch: 26550\tTrain loss: 0.14092409114042917\n",
      "Epoch: 26600\tTrain loss: 0.08382795751094818\n",
      "Epoch: 26650\tTrain loss: 0.2694007145861785\n",
      "Epoch: 26700\tTrain loss: 0.04078726563602686\n",
      "Epoch: 26750\tTrain loss: 0.022163333371281624\n",
      "Epoch: 26800\tTrain loss: 0.014202893866846958\n",
      "Epoch: 26850\tTrain loss: 0.00918701213474075\n",
      "Epoch: 26900\tTrain loss: 0.005953426162401835\n",
      "Epoch: 26950\tTrain loss: 0.0038245957888041935\n",
      "Epoch: 27000\tTrain loss: 0.05878494866192341\n",
      "Epoch: 27050\tTrain loss: 0.008560255480309328\n",
      "Epoch: 27100\tTrain loss: 0.001490239849469314\n",
      "Epoch: 27150\tTrain loss: 0.0008541751964609526\n",
      "Epoch: 27200\tTrain loss: 0.00047467658199214685\n",
      "Epoch: 27250\tTrain loss: 0.00026227762373309815\n",
      "Epoch: 27300\tTrain loss: 0.00014116364642783688\n",
      "Epoch: 27350\tTrain loss: 7.390248659551919e-05\n",
      "Epoch: 27400\tTrain loss: 3.90636366015921e-05\n",
      "Epoch: 27450\tTrain loss: 1.27826938778162\n",
      "Epoch: 27500\tTrain loss: 0.028212324861669913\n",
      "Epoch: 27550\tTrain loss: 0.0007367625536668735\n",
      "Epoch: 27600\tTrain loss: 0.0002729922562139109\n",
      "Epoch: 27650\tTrain loss: 0.003998318106217387\n",
      "Epoch: 27700\tTrain loss: 2.099507207671801\n",
      "Epoch: 27750\tTrain loss: 1.1417476162314415\n",
      "Epoch: 27800\tTrain loss: 0.012267302935166905\n",
      "Epoch: 27850\tTrain loss: 0.003445359024529656\n",
      "Epoch: 27900\tTrain loss: 0.0011861662108761568\n",
      "Epoch: 27950\tTrain loss: 0.0004057434562128037\n",
      "Epoch: 28000\tTrain loss: 0.0012723184190690517\n",
      "Epoch: 28050\tTrain loss: 0.1635253665347894\n",
      "Epoch: 28100\tTrain loss: 2.3049302274982133\n",
      "Epoch: 28150\tTrain loss: 0.017179436671237152\n",
      "Epoch: 28200\tTrain loss: 0.006098565393282722\n",
      "Epoch: 28250\tTrain loss: 0.0024088469023505845\n",
      "Epoch: 28300\tTrain loss: 0.0009902662908037503\n",
      "Epoch: 28350\tTrain loss: 0.00041339374244368326\n",
      "Epoch: 28400\tTrain loss: 0.00017275830881165652\n",
      "Epoch: 28450\tTrain loss: 7.141323537022497e-05\n",
      "Epoch: 28500\tTrain loss: 2.923711872426793e-05\n",
      "Epoch: 28550\tTrain loss: 1.2334932913897015e-05\n",
      "Epoch: 28600\tTrain loss: 0.004958639328833669\n",
      "Epoch: 28650\tTrain loss: 4.801073710123698\n",
      "Epoch: 28700\tTrain loss: 1.1255454470713933\n",
      "Epoch: 28750\tTrain loss: 0.10023183049634099\n",
      "Epoch: 28800\tTrain loss: 0.006617480423301458\n",
      "Epoch: 28850\tTrain loss: 0.002417025660785536\n",
      "Epoch: 28900\tTrain loss: 0.000934994847436125\n",
      "Epoch: 28950\tTrain loss: 0.00037601175912035006\n",
      "Epoch: 29000\tTrain loss: 0.00015115761076837467\n",
      "Epoch: 29050\tTrain loss: 6.090185464321015e-05\n",
      "Epoch: 29100\tTrain loss: 2.5350088738681126e-05\n",
      "Epoch: 29150\tTrain loss: 1.0583063309847299e-05\n",
      "Epoch: 29200\tTrain loss: 4.95112821378522e-06\n",
      "Epoch: 29250\tTrain loss: 0.005235439827629307\n",
      "Epoch: 29300\tTrain loss: 21.319896114679675\n",
      "Epoch: 29350\tTrain loss: 40.20081011454264\n",
      "Epoch: 29400\tTrain loss: 9.016838709513346\n",
      "Epoch: 29450\tTrain loss: 6.761391003926595\n",
      "Epoch: 29500\tTrain loss: 5.144679983456929\n",
      "Epoch: 29550\tTrain loss: 3.7849725087483725\n",
      "Epoch: 29600\tTrain loss: 2.671368400255839\n",
      "Epoch: 29650\tTrain loss: 1.9020812511444092\n",
      "Epoch: 29700\tTrain loss: 1.4164346655209858\n",
      "Epoch: 29750\tTrain loss: 1.0774968167146046\n",
      "Epoch: 29800\tTrain loss: 0.8297982613245646\n",
      "Epoch: 29850\tTrain loss: 0.6405053486426672\n",
      "Epoch: 29900\tTrain loss: 0.49537618706623715\n",
      "Epoch: 29950\tTrain loss: 0.382861436655124\n",
      "Training CFRNN\n",
      "Epoch: 0\tTrain loss: 2947234.4166666665\n",
      "Epoch: 50\tTrain loss: 2854278.5\n",
      "Epoch: 100\tTrain loss: 2769453.46875\n",
      "Epoch: 150\tTrain loss: 2687969.5416666665\n",
      "Epoch: 200\tTrain loss: 2609378.0416666665\n",
      "Epoch: 250\tTrain loss: 2533761.2708333335\n",
      "Epoch: 300\tTrain loss: 2460334.3333333335\n",
      "Epoch: 350\tTrain loss: 2389375.1041666665\n",
      "Epoch: 400\tTrain loss: 2320763.3125\n",
      "Epoch: 450\tTrain loss: 2254225.5208333335\n",
      "Epoch: 500\tTrain loss: 2189633.5833333335\n",
      "Epoch: 550\tTrain loss: 2127479.9322916665\n",
      "Epoch: 600\tTrain loss: 2067365.4479166667\n",
      "Epoch: 650\tTrain loss: 2008881.2708333333\n",
      "Epoch: 700\tTrain loss: 1952634.9791666667\n",
      "Epoch: 750\tTrain loss: 1897880.3125\n",
      "Epoch: 800\tTrain loss: 1845491.9739583333\n",
      "Epoch: 850\tTrain loss: 1794456.8098958333\n",
      "Epoch: 900\tTrain loss: 1745307.6770833333\n",
      "Epoch: 950\tTrain loss: 1698082.515625\n",
      "Epoch: 1000\tTrain loss: 1652438.6380208333\n",
      "Epoch: 1050\tTrain loss: 1608016.9791666667\n",
      "Epoch: 1100\tTrain loss: 1565565.1041666667\n",
      "Epoch: 1150\tTrain loss: 1524784.15625\n",
      "Epoch: 1200\tTrain loss: 1485211.4895833333\n",
      "Epoch: 1250\tTrain loss: 1447331.4166666667\n",
      "Epoch: 1300\tTrain loss: 1410910.6979166667\n",
      "Epoch: 1350\tTrain loss: 1375570.2395833333\n",
      "Epoch: 1400\tTrain loss: 1341802.986328125\n",
      "Epoch: 1450\tTrain loss: 1309741.7366536458\n",
      "Epoch: 1500\tTrain loss: 1278627.984375\n",
      "Epoch: 1550\tTrain loss: 1248914.1956380208\n",
      "Epoch: 1600\tTrain loss: 1220644.8580729167\n",
      "Epoch: 1650\tTrain loss: 1193337.5104166667\n",
      "Epoch: 1700\tTrain loss: 1167484.65234375\n",
      "Epoch: 1750\tTrain loss: 1142623.0234375\n",
      "Epoch: 1800\tTrain loss: 1119339.373046875\n",
      "Epoch: 1850\tTrain loss: 1096703.2864583333\n",
      "Epoch: 1900\tTrain loss: 1075411.7330729167\n",
      "Epoch: 1950\tTrain loss: 1055263.203125\n",
      "Epoch: 2000\tTrain loss: 1036232.1614583334\n",
      "Epoch: 2050\tTrain loss: 1018009.6432291666\n",
      "Epoch: 2100\tTrain loss: 1000812.7083333334\n",
      "Epoch: 2150\tTrain loss: 984636.6692708334\n",
      "Epoch: 2200\tTrain loss: 969294.2473958334\n",
      "Epoch: 2250\tTrain loss: 954947.9505208334\n",
      "Epoch: 2300\tTrain loss: 941486.5182291666\n",
      "Epoch: 2350\tTrain loss: 928814.703125\n",
      "Epoch: 2400\tTrain loss: 916985.3307291666\n",
      "Epoch: 2450\tTrain loss: 905790.3645833334\n",
      "Epoch: 2500\tTrain loss: 895455.4440104166\n",
      "Epoch: 2550\tTrain loss: 885897.0260416666\n",
      "Epoch: 2600\tTrain loss: 876838.7369791666\n",
      "Epoch: 2650\tTrain loss: 868391.0729166666\n",
      "Epoch: 2700\tTrain loss: 837124.7252604166\n",
      "Epoch: 2750\tTrain loss: 771580.7060546875\n",
      "Epoch: 2800\tTrain loss: 746426.8043619791\n",
      "Epoch: 2850\tTrain loss: 727340.927734375\n",
      "Epoch: 2900\tTrain loss: 709212.662109375\n",
      "Epoch: 2950\tTrain loss: 691816.1643880209\n",
      "Epoch: 3000\tTrain loss: 675088.0981445312\n",
      "Epoch: 3050\tTrain loss: 658903.4116210938\n",
      "Epoch: 3100\tTrain loss: 643404.6644287109\n",
      "Epoch: 3150\tTrain loss: 628246.7054036459\n",
      "Epoch: 3200\tTrain loss: 613822.8902994791\n",
      "Epoch: 3250\tTrain loss: 599493.6170654297\n",
      "Epoch: 3300\tTrain loss: 585158.4977213541\n",
      "Epoch: 3350\tTrain loss: 571132.1056315104\n",
      "Epoch: 3400\tTrain loss: 557304.0153808594\n",
      "Epoch: 3450\tTrain loss: 543777.9458821615\n",
      "Epoch: 3500\tTrain loss: 530742.6272786459\n",
      "Epoch: 3550\tTrain loss: 517782.8543701172\n",
      "Epoch: 3600\tTrain loss: 505163.2188313802\n",
      "Epoch: 3650\tTrain loss: 493010.53251139325\n",
      "Epoch: 3700\tTrain loss: 480841.02176920575\n",
      "Epoch: 3750\tTrain loss: 469275.9696451823\n",
      "Epoch: 3800\tTrain loss: 457682.01936848956\n",
      "Epoch: 3850\tTrain loss: 446635.48409016925\n",
      "Epoch: 3900\tTrain loss: 435862.26155598956\n",
      "Epoch: 3950\tTrain loss: 425213.50960286456\n",
      "Epoch: 4000\tTrain loss: 414943.23193359375\n",
      "Epoch: 4050\tTrain loss: 407247.49072265625\n",
      "Epoch: 4100\tTrain loss: 395183.7380777995\n",
      "Epoch: 4150\tTrain loss: 385694.2362467448\n",
      "Epoch: 4200\tTrain loss: 376495.27176920575\n",
      "Epoch: 4250\tTrain loss: 367501.6549479167\n",
      "Epoch: 4300\tTrain loss: 358866.46044921875\n",
      "Epoch: 4350\tTrain loss: 350538.40641276044\n",
      "Epoch: 4400\tTrain loss: 342180.93190511066\n",
      "Epoch: 4450\tTrain loss: 334356.0003255208\n",
      "Epoch: 4500\tTrain loss: 326641.13419596356\n",
      "Epoch: 4550\tTrain loss: 319108.6048177083\n",
      "Epoch: 4600\tTrain loss: 311977.82731119794\n",
      "Epoch: 4650\tTrain loss: 304947.37166341144\n",
      "Epoch: 4700\tTrain loss: 298183.6826985677\n",
      "Epoch: 4750\tTrain loss: 291613.2848307292\n",
      "Epoch: 4800\tTrain loss: 284453.0165201823\n",
      "Epoch: 4850\tTrain loss: 277437.9255777995\n",
      "Epoch: 4900\tTrain loss: 270616.8072916667\n",
      "Epoch: 4950\tTrain loss: 263804.798828125\n",
      "Epoch: 5000\tTrain loss: 257163.06144205728\n",
      "Epoch: 5050\tTrain loss: 250528.24003092447\n",
      "Epoch: 5100\tTrain loss: 244099.9457804362\n",
      "Epoch: 5150\tTrain loss: 237708.67631022134\n",
      "Epoch: 5200\tTrain loss: 231358.21492513022\n",
      "Epoch: 5250\tTrain loss: 225256.48962402344\n",
      "Epoch: 5300\tTrain loss: 219002.27880859375\n",
      "Epoch: 5350\tTrain loss: 212955.91341145834\n",
      "Epoch: 5400\tTrain loss: 207080.744140625\n",
      "Epoch: 5450\tTrain loss: 201730.29243977866\n",
      "Epoch: 5500\tTrain loss: 195500.765625\n",
      "Epoch: 5550\tTrain loss: 189883.0986735026\n",
      "Epoch: 5600\tTrain loss: 184283.55981445312\n",
      "Epoch: 5650\tTrain loss: 178824.18253580728\n",
      "Epoch: 5700\tTrain loss: 173476.458984375\n",
      "Epoch: 5750\tTrain loss: 168413.5128173828\n",
      "Epoch: 5800\tTrain loss: 162975.00122070312\n",
      "Epoch: 5850\tTrain loss: 157954.52840169272\n",
      "Epoch: 5900\tTrain loss: 152955.76782226562\n",
      "Epoch: 5950\tTrain loss: 148044.0399983724\n",
      "Epoch: 6000\tTrain loss: 143166.3321940104\n",
      "Epoch: 6050\tTrain loss: 138493.83349609375\n",
      "Epoch: 6100\tTrain loss: 133881.6222330729\n",
      "Epoch: 6150\tTrain loss: 129294.85803222656\n",
      "Epoch: 6200\tTrain loss: 124895.8798828125\n",
      "Epoch: 6250\tTrain loss: 120570.9565633138\n",
      "Epoch: 6300\tTrain loss: 116267.21543375652\n",
      "Epoch: 6350\tTrain loss: 112127.69083658855\n",
      "Epoch: 6400\tTrain loss: 108043.82352701823\n",
      "Epoch: 6450\tTrain loss: 104009.67932128906\n",
      "Epoch: 6500\tTrain loss: 100103.18701171875\n",
      "Epoch: 6550\tTrain loss: 96301.90201822917\n",
      "Epoch: 6600\tTrain loss: 92597.0907389323\n",
      "Epoch: 6650\tTrain loss: 88974.05826822917\n",
      "Epoch: 6700\tTrain loss: 85606.33215332031\n",
      "Epoch: 6750\tTrain loss: 82026.49572753906\n",
      "Epoch: 6800\tTrain loss: 78676.03910319011\n",
      "Epoch: 6850\tTrain loss: 75365.40199788411\n",
      "Epoch: 6900\tTrain loss: 72183.85032145183\n",
      "Epoch: 6950\tTrain loss: 69095.7001953125\n",
      "Epoch: 7000\tTrain loss: 66029.54113769531\n",
      "Epoch: 7050\tTrain loss: 63122.72627766927\n",
      "Epoch: 7100\tTrain loss: 60279.98069254557\n",
      "Epoch: 7150\tTrain loss: 57487.2939453125\n",
      "Epoch: 7200\tTrain loss: 54806.81317138672\n",
      "Epoch: 7250\tTrain loss: 52206.31506347656\n",
      "Epoch: 7300\tTrain loss: 49679.40909830729\n",
      "Epoch: 7350\tTrain loss: 47232.42036946615\n",
      "Epoch: 7400\tTrain loss: 44898.93273925781\n",
      "Epoch: 7450\tTrain loss: 42613.80546061198\n",
      "Epoch: 7500\tTrain loss: 40401.95125325521\n",
      "Epoch: 7550\tTrain loss: 38297.632486979164\n",
      "Epoch: 7600\tTrain loss: 36253.99043782552\n",
      "Epoch: 7650\tTrain loss: 34284.40767415365\n",
      "Epoch: 7700\tTrain loss: 32398.188232421875\n",
      "Epoch: 7750\tTrain loss: 30685.685587565105\n",
      "Epoch: 7800\tTrain loss: 28835.80985514323\n",
      "Epoch: 7850\tTrain loss: 27637.45829264323\n",
      "Epoch: 7900\tTrain loss: 25609.968831380207\n",
      "Epoch: 7950\tTrain loss: 24076.177327473957\n",
      "Epoch: 8000\tTrain loss: 22620.299397786457\n",
      "Epoch: 8050\tTrain loss: 21226.52372233073\n",
      "Epoch: 8100\tTrain loss: 19896.813517252605\n",
      "Epoch: 8150\tTrain loss: 18639.723754882812\n",
      "Epoch: 8200\tTrain loss: 17444.97412109375\n",
      "Epoch: 8250\tTrain loss: 16298.351643880209\n",
      "Epoch: 8300\tTrain loss: 15221.835693359375\n",
      "Epoch: 8350\tTrain loss: 14246.51806640625\n",
      "Epoch: 8400\tTrain loss: 13228.059366861979\n",
      "Epoch: 8450\tTrain loss: 12320.630086263021\n",
      "Epoch: 8500\tTrain loss: 11465.99033610026\n",
      "Epoch: 8550\tTrain loss: 10661.452209472656\n",
      "Epoch: 8600\tTrain loss: 9904.343973795572\n",
      "Epoch: 8650\tTrain loss: 15101.899495442709\n",
      "Epoch: 8700\tTrain loss: 8535.634460449219\n",
      "Epoch: 8750\tTrain loss: 7911.359130859375\n",
      "Epoch: 8800\tTrain loss: 7331.458902994792\n",
      "Epoch: 8850\tTrain loss: 6790.714436848958\n",
      "Epoch: 8900\tTrain loss: 6284.72802734375\n",
      "Epoch: 8950\tTrain loss: 5815.095682779948\n",
      "Epoch: 9000\tTrain loss: 5376.880269368489\n",
      "Epoch: 9050\tTrain loss: 4973.410461425781\n",
      "Epoch: 9100\tTrain loss: 4602.0816650390625\n",
      "Epoch: 9150\tTrain loss: 4252.911071777344\n",
      "Epoch: 9200\tTrain loss: 3937.3357747395835\n",
      "Epoch: 9250\tTrain loss: 3640.1075032552085\n",
      "Epoch: 9300\tTrain loss: 3367.7378743489585\n",
      "Epoch: 9350\tTrain loss: 3113.519083658854\n",
      "Epoch: 9400\tTrain loss: 2884.037150065104\n",
      "Epoch: 9450\tTrain loss: 2667.9496866861978\n",
      "Epoch: 9500\tTrain loss: 6946.4737548828125\n",
      "Epoch: 9550\tTrain loss: 2793.019246419271\n",
      "Epoch: 9600\tTrain loss: 2441.4342651367188\n",
      "Epoch: 9650\tTrain loss: 2230.408487955729\n",
      "Epoch: 9700\tTrain loss: 2061.9340209960938\n",
      "Epoch: 9750\tTrain loss: 1921.4605509440105\n",
      "Epoch: 9800\tTrain loss: 1796.165506998698\n",
      "Epoch: 9850\tTrain loss: 1683.5358072916667\n",
      "Epoch: 9900\tTrain loss: 1580.5596313476562\n",
      "Epoch: 9950\tTrain loss: 1485.4434000651042\n",
      "Epoch: 10000\tTrain loss: 1398.4895426432292\n",
      "Epoch: 10050\tTrain loss: 1319.5350341796875\n",
      "Epoch: 10100\tTrain loss: 1246.522237141927\n",
      "Epoch: 10150\tTrain loss: 1179.2822875976562\n",
      "Epoch: 10200\tTrain loss: 1117.8934122721355\n",
      "Epoch: 10250\tTrain loss: 1058.1119791666667\n",
      "Epoch: 10300\tTrain loss: 1002.3073323567709\n",
      "Epoch: 10350\tTrain loss: 951.5328572591146\n",
      "Epoch: 10400\tTrain loss: 902.033447265625\n",
      "Epoch: 10450\tTrain loss: 855.7568664550781\n",
      "Epoch: 10500\tTrain loss: 1076.6752522786458\n",
      "Epoch: 10550\tTrain loss: 779.9560953776041\n",
      "Epoch: 10600\tTrain loss: 743.0503845214844\n",
      "Epoch: 10650\tTrain loss: 709.677001953125\n",
      "Epoch: 10700\tTrain loss: 679.9589131673177\n",
      "Epoch: 10750\tTrain loss: 651.9711710611979\n",
      "Epoch: 10800\tTrain loss: 627.3677673339844\n",
      "Epoch: 10850\tTrain loss: 605.4859008789062\n",
      "Epoch: 10900\tTrain loss: 766.5223592122396\n",
      "Epoch: 10950\tTrain loss: 566.1008911132812\n",
      "Epoch: 11000\tTrain loss: 546.7045694986979\n",
      "Epoch: 11050\tTrain loss: 529.2509562174479\n",
      "Epoch: 11100\tTrain loss: 513.2502492268881\n",
      "Epoch: 11150\tTrain loss: 500.9147135416667\n",
      "Epoch: 11200\tTrain loss: 485.1020914713542\n",
      "Epoch: 11250\tTrain loss: 479.46302795410156\n",
      "Epoch: 11300\tTrain loss: 464.58056640625\n",
      "Epoch: 11350\tTrain loss: 452.403756459554\n",
      "Epoch: 11400\tTrain loss: 441.0986836751302\n",
      "Epoch: 11450\tTrain loss: 430.6746368408203\n",
      "Epoch: 11500\tTrain loss: 420.14418029785156\n",
      "Epoch: 11550\tTrain loss: 409.36898549397785\n",
      "Epoch: 11600\tTrain loss: 398.0507507324219\n",
      "Epoch: 11650\tTrain loss: 385.79637400309247\n",
      "Epoch: 11700\tTrain loss: 367.9210510253906\n",
      "Epoch: 11750\tTrain loss: 346.48141288757324\n",
      "Epoch: 11800\tTrain loss: 326.22709528605145\n",
      "Epoch: 11850\tTrain loss: 306.96515401204425\n",
      "Epoch: 11900\tTrain loss: 292.06572341918945\n",
      "Epoch: 11950\tTrain loss: 485.0633239746094\n",
      "Epoch: 12000\tTrain loss: 277.5621067682902\n",
      "Epoch: 12050\tTrain loss: 268.5097713470459\n",
      "Epoch: 12100\tTrain loss: 260.66930770874023\n",
      "Epoch: 12150\tTrain loss: 253.7478993733724\n",
      "Epoch: 12200\tTrain loss: 247.88829898834229\n",
      "Epoch: 12250\tTrain loss: 242.6051762898763\n",
      "Epoch: 12300\tTrain loss: 238.097536722819\n",
      "Epoch: 12350\tTrain loss: 233.99431037902832\n",
      "Epoch: 12400\tTrain loss: 656.4865951538086\n",
      "Epoch: 12450\tTrain loss: 4123.612386067708\n",
      "Epoch: 12500\tTrain loss: 3103.4905802408853\n",
      "Epoch: 12550\tTrain loss: 2438.690663655599\n",
      "Epoch: 12600\tTrain loss: 1941.7945353190105\n",
      "Epoch: 12650\tTrain loss: 1553.4600321451824\n",
      "Epoch: 12700\tTrain loss: 1238.2384490966797\n",
      "Epoch: 12750\tTrain loss: 993.5921223958334\n",
      "Epoch: 12800\tTrain loss: 814.6697540283203\n",
      "Epoch: 12850\tTrain loss: 681.1235860188802\n",
      "Epoch: 12900\tTrain loss: 579.6273803710938\n",
      "Epoch: 12950\tTrain loss: 537.0028686523438\n",
      "Epoch: 13000\tTrain loss: 411.12217712402344\n",
      "Epoch: 13050\tTrain loss: 367.7995147705078\n",
      "Epoch: 13100\tTrain loss: 336.9910430908203\n",
      "Epoch: 13150\tTrain loss: 315.50365447998047\n",
      "Epoch: 13200\tTrain loss: 299.33905537923175\n",
      "Epoch: 13250\tTrain loss: 287.6346791585286\n",
      "Epoch: 13300\tTrain loss: 277.6916046142578\n",
      "Epoch: 13350\tTrain loss: 269.60595703125\n",
      "Epoch: 13400\tTrain loss: 262.567133585612\n",
      "Epoch: 13450\tTrain loss: 256.46057383219403\n",
      "Epoch: 13500\tTrain loss: 252.48952865600586\n",
      "Epoch: 13550\tTrain loss: 247.27915954589844\n",
      "Epoch: 13600\tTrain loss: 242.74470011393228\n",
      "Epoch: 13650\tTrain loss: 239.49259821573892\n",
      "Epoch: 13700\tTrain loss: 237.44128354390463\n",
      "Epoch: 13750\tTrain loss: 231.807892481486\n",
      "Epoch: 13800\tTrain loss: 232.8697306315104\n",
      "Epoch: 13850\tTrain loss: 226.30870564778647\n",
      "Epoch: 13900\tTrain loss: 225.6600144704183\n",
      "Epoch: 13950\tTrain loss: 221.51750628153482\n",
      "Epoch: 14000\tTrain loss: 219.83853244781494\n",
      "Epoch: 14050\tTrain loss: 218.2490038871765\n",
      "Epoch: 14100\tTrain loss: 220.33810440699258\n",
      "Epoch: 14150\tTrain loss: 214.76338577270508\n",
      "Epoch: 14200\tTrain loss: 243.54323132832846\n",
      "Epoch: 14250\tTrain loss: 213.40755542119345\n",
      "Epoch: 14300\tTrain loss: 212.0381494363149\n",
      "Epoch: 14350\tTrain loss: 210.97209286689758\n",
      "Epoch: 14400\tTrain loss: 209.94541120529175\n",
      "Epoch: 14450\tTrain loss: 209.93508307139078\n",
      "Epoch: 14500\tTrain loss: 224.35419988632202\n",
      "Epoch: 14550\tTrain loss: 218.09265772501627\n",
      "Epoch: 14600\tTrain loss: 215.52059276898703\n",
      "Epoch: 14650\tTrain loss: 213.7274808883667\n",
      "Epoch: 14700\tTrain loss: 212.3160249789556\n",
      "Epoch: 14750\tTrain loss: 211.1538918018341\n",
      "Epoch: 14800\tTrain loss: 210.2917210261027\n",
      "Epoch: 14850\tTrain loss: 209.50693075855574\n",
      "Epoch: 14900\tTrain loss: 208.94784967104593\n",
      "Epoch: 14950\tTrain loss: 208.48131895065308\n",
      "Epoch: 15000\tTrain loss: 208.033198595047\n",
      "Epoch: 15050\tTrain loss: 207.61562871436277\n",
      "Epoch: 15100\tTrain loss: 219.99018573760986\n",
      "Epoch: 15150\tTrain loss: 217.09831232825914\n",
      "Epoch: 15200\tTrain loss: 207.9312783603867\n",
      "Epoch: 15250\tTrain loss: 206.4541111191114\n",
      "Epoch: 15300\tTrain loss: 482.70653279622394\n",
      "Epoch: 15350\tTrain loss: 207.96112093826136\n",
      "Epoch: 15400\tTrain loss: 206.98423918088278\n",
      "Epoch: 15450\tTrain loss: 206.4320774525404\n",
      "Epoch: 15500\tTrain loss: 206.07084413369498\n",
      "Epoch: 15550\tTrain loss: 205.68179610247412\n",
      "Epoch: 15600\tTrain loss: 205.23043994853893\n",
      "Epoch: 15650\tTrain loss: 204.837736406674\n",
      "Epoch: 15700\tTrain loss: 207.51420307159424\n",
      "Epoch: 15750\tTrain loss: 204.1845222314199\n",
      "Epoch: 15800\tTrain loss: 202.61733229955038\n",
      "Epoch: 15850\tTrain loss: 201.6166679660479\n",
      "Epoch: 15900\tTrain loss: 203.3031226793925\n",
      "Epoch: 15950\tTrain loss: 197.67391232649484\n",
      "Epoch: 16000\tTrain loss: 195.21534154812494\n",
      "Epoch: 16050\tTrain loss: 519.4735107421875\n",
      "Epoch: 16100\tTrain loss: 333.0365753173828\n",
      "Epoch: 16150\tTrain loss: 232.80581696828207\n",
      "Epoch: 16200\tTrain loss: 209.86585009098053\n",
      "Epoch: 16250\tTrain loss: 197.5066028436025\n",
      "Epoch: 16300\tTrain loss: 190.44881415367126\n",
      "Epoch: 16350\tTrain loss: 184.6567806402842\n",
      "Epoch: 16400\tTrain loss: 186.58877539634705\n",
      "Epoch: 16450\tTrain loss: 210.21246655782065\n",
      "Epoch: 16500\tTrain loss: 176.42207007606825\n",
      "Epoch: 16550\tTrain loss: 199.71595760186514\n",
      "Epoch: 16600\tTrain loss: 175.63493516047797\n",
      "Epoch: 16650\tTrain loss: 293.84314727783203\n",
      "Epoch: 16700\tTrain loss: 207.0479443470637\n",
      "Epoch: 16750\tTrain loss: 199.54746665557226\n",
      "Epoch: 16800\tTrain loss: 189.97297406196594\n",
      "Epoch: 16850\tTrain loss: 202.81710561116537\n",
      "Epoch: 16900\tTrain loss: 176.3137390886744\n",
      "Epoch: 16950\tTrain loss: 191.98403803507486\n",
      "Epoch: 17000\tTrain loss: 177.02344988783202\n",
      "Epoch: 17050\tTrain loss: 196.1740494519472\n",
      "Epoch: 17100\tTrain loss: 177.87871507306895\n",
      "Epoch: 17150\tTrain loss: 181.3738629023234\n",
      "Epoch: 17200\tTrain loss: 175.4153598000606\n",
      "Epoch: 17250\tTrain loss: 165.66304683685303\n",
      "Epoch: 17300\tTrain loss: 203.12533247470856\n",
      "Epoch: 17350\tTrain loss: 179.4826629559199\n",
      "Epoch: 17400\tTrain loss: 166.86062939961752\n",
      "Epoch: 17450\tTrain loss: 153.9116646448771\n",
      "Epoch: 17500\tTrain loss: 242.2802302042643\n",
      "Epoch: 17550\tTrain loss: 240.15502214431763\n",
      "Epoch: 17600\tTrain loss: 221.4965856075287\n",
      "Epoch: 17650\tTrain loss: 208.0649307568868\n",
      "Epoch: 17700\tTrain loss: 199.6351776123047\n",
      "Epoch: 17750\tTrain loss: 191.54992787043253\n",
      "Epoch: 17800\tTrain loss: 178.60888769229254\n",
      "Epoch: 17850\tTrain loss: 149.41557852427164\n",
      "Epoch: 17900\tTrain loss: 134.77355416615805\n",
      "Epoch: 17950\tTrain loss: 192.63449668884277\n",
      "Epoch: 18000\tTrain loss: 182.9739201863607\n",
      "Epoch: 18050\tTrain loss: 100.8202756245931\n",
      "Epoch: 18100\tTrain loss: 92.87139574686687\n",
      "Epoch: 18150\tTrain loss: 84.7115756670634\n",
      "Epoch: 18200\tTrain loss: 291.34085241953534\n",
      "Epoch: 18250\tTrain loss: 245.87116940816244\n",
      "Epoch: 18300\tTrain loss: 123.75445000330608\n",
      "Epoch: 18350\tTrain loss: 87.09571552276611\n",
      "Epoch: 18400\tTrain loss: 74.74944130579631\n",
      "Epoch: 18450\tTrain loss: 84.67536048094432\n",
      "Epoch: 18500\tTrain loss: 58.81059487660726\n",
      "Epoch: 18550\tTrain loss: 56.905093351999916\n",
      "Epoch: 18600\tTrain loss: 76.56218496958415\n",
      "Epoch: 18650\tTrain loss: 48.86029179890951\n",
      "Epoch: 18700\tTrain loss: 39.75599813461304\n",
      "Epoch: 18750\tTrain loss: 33.89557361602783\n",
      "Epoch: 18800\tTrain loss: 643.0593439737955\n",
      "Epoch: 18850\tTrain loss: 52.745878299077354\n",
      "Epoch: 18900\tTrain loss: 49.24682482083639\n",
      "Epoch: 18950\tTrain loss: 45.098861376444496\n",
      "Epoch: 19000\tTrain loss: 40.70892747243246\n",
      "Epoch: 19050\tTrain loss: 41.81437762578329\n",
      "Epoch: 19100\tTrain loss: 34.653563817342125\n",
      "Epoch: 19150\tTrain loss: 32.35315656661987\n",
      "Epoch: 19200\tTrain loss: 29.72737471262614\n",
      "Epoch: 19250\tTrain loss: 33.576626459757485\n",
      "Epoch: 19300\tTrain loss: 26.602669338385265\n",
      "Epoch: 19350\tTrain loss: 23.33045752843221\n",
      "Epoch: 19400\tTrain loss: 20.389878431955974\n",
      "Epoch: 19450\tTrain loss: 17.781837463378906\n",
      "Epoch: 19500\tTrain loss: 15.491432110468546\n",
      "Epoch: 19550\tTrain loss: 13.423952102661133\n",
      "Epoch: 19600\tTrain loss: 11.591834217309952\n",
      "Epoch: 19650\tTrain loss: 10.065274397532145\n",
      "Epoch: 19700\tTrain loss: 8.567930738131205\n",
      "Epoch: 19750\tTrain loss: 7.273919741312663\n",
      "Epoch: 19800\tTrain loss: 6.136804938316345\n",
      "Epoch: 19850\tTrain loss: 5.097129940986633\n",
      "Epoch: 19900\tTrain loss: 10.056385040283203\n",
      "Epoch: 19950\tTrain loss: 4.1501089334487915\n",
      "Epoch: 20000\tTrain loss: 2.9289807081222534\n",
      "Epoch: 20050\tTrain loss: 2.381565590699514\n",
      "Epoch: 20100\tTrain loss: 1.916867730518182\n",
      "Epoch: 20150\tTrain loss: 1.5239376177390416\n",
      "Epoch: 20200\tTrain loss: 1.1980879542728264\n",
      "Epoch: 20250\tTrain loss: 0.9237132693330447\n",
      "Epoch: 20300\tTrain loss: 62.35374895731608\n",
      "Epoch: 20350\tTrain loss: 27.49332618713379\n",
      "Epoch: 20400\tTrain loss: 0.7105317028860251\n",
      "Epoch: 20450\tTrain loss: 0.49033108726143837\n",
      "Epoch: 20500\tTrain loss: 0.37327811121940613\n",
      "Epoch: 20550\tTrain loss: 0.2838613937298457\n",
      "Epoch: 20600\tTrain loss: 0.21444842964410782\n",
      "Epoch: 20650\tTrain loss: 0.16220066075523695\n",
      "Epoch: 20700\tTrain loss: 0.6819171557823817\n",
      "Epoch: 20750\tTrain loss: 0.1475753535827001\n",
      "Epoch: 20800\tTrain loss: 9.678183555603027\n",
      "Epoch: 20850\tTrain loss: 0.35364370544751483\n",
      "Epoch: 20900\tTrain loss: 0.037054575669268765\n",
      "Epoch: 20950\tTrain loss: 3.819958051045736\n",
      "Epoch: 21000\tTrain loss: 1.1886741717656453\n",
      "Epoch: 21050\tTrain loss: 1.4883835775156815\n",
      "Epoch: 21100\tTrain loss: 0.09220520717402299\n",
      "Epoch: 21150\tTrain loss: 6.199830909570058\n",
      "Epoch: 21200\tTrain loss: 2.216199199358622\n",
      "Epoch: 21250\tTrain loss: 0.025015706506868202\n",
      "Epoch: 21300\tTrain loss: 0.011320474247137705\n",
      "Epoch: 21350\tTrain loss: 0.005872995126992464\n",
      "Epoch: 21400\tTrain loss: 0.002834408893249929\n",
      "Epoch: 21450\tTrain loss: 0.0015047138440422714\n",
      "Epoch: 21500\tTrain loss: 0.0009042393454971412\n",
      "Epoch: 21550\tTrain loss: 4.434677556157112\n",
      "Epoch: 21600\tTrain loss: 1.961400330066681\n",
      "Epoch: 21650\tTrain loss: 696.737922668457\n",
      "Epoch: 21700\tTrain loss: 514.8555526733398\n",
      "Epoch: 21750\tTrain loss: 408.3196856180827\n",
      "Epoch: 21800\tTrain loss: 262.5099016825358\n",
      "Epoch: 21850\tTrain loss: 9797.287228902182\n",
      "Epoch: 21900\tTrain loss: 12458.180212656656\n",
      "Epoch: 21950\tTrain loss: 10073.922257741293\n",
      "Epoch: 22000\tTrain loss: 8313.751501719156\n",
      "Epoch: 22050\tTrain loss: 6952.270131429036\n",
      "Epoch: 22100\tTrain loss: 5828.75026957194\n",
      "Epoch: 22150\tTrain loss: 4906.381330490112\n",
      "Epoch: 22200\tTrain loss: 4318.131408691406\n",
      "Epoch: 22250\tTrain loss: 3471.528461456299\n",
      "Epoch: 22300\tTrain loss: 285.2262013753255\n",
      "Epoch: 22350\tTrain loss: 328.1677500406901\n",
      "Epoch: 22400\tTrain loss: 295.1148427327474\n",
      "Epoch: 22450\tTrain loss: 260.891357421875\n",
      "Epoch: 22500\tTrain loss: 200.18167622884116\n",
      "Epoch: 22550\tTrain loss: 177.91044743855795\n",
      "Epoch: 22600\tTrain loss: 165.3876520792643\n",
      "Epoch: 22650\tTrain loss: 120.20521926879883\n",
      "Epoch: 22700\tTrain loss: 108.65559768676758\n",
      "Epoch: 22750\tTrain loss: 90.32354545593262\n",
      "Epoch: 22800\tTrain loss: 72.26493072509766\n",
      "Epoch: 22850\tTrain loss: 59.29096603393555\n",
      "Epoch: 22900\tTrain loss: 52.773301442464195\n",
      "Epoch: 22950\tTrain loss: 48.01959673563639\n",
      "Epoch: 23000\tTrain loss: 41.22021293640137\n",
      "Epoch: 23050\tTrain loss: 30.389598846435547\n",
      "Epoch: 23100\tTrain loss: 15691.000597635904\n",
      "Epoch: 23150\tTrain loss: 3453.2247111002603\n",
      "Epoch: 23200\tTrain loss: 1082.4657351175945\n",
      "Epoch: 23250\tTrain loss: 967.9039510091146\n",
      "Epoch: 23300\tTrain loss: 879.5814921061198\n",
      "Epoch: 23350\tTrain loss: 805.5194956461588\n",
      "Epoch: 23400\tTrain loss: 741.038818359375\n",
      "Epoch: 23450\tTrain loss: 685.3041280110677\n",
      "Epoch: 23500\tTrain loss: 636.9028209050497\n",
      "Epoch: 23550\tTrain loss: 595.0412276585897\n",
      "Epoch: 23600\tTrain loss: 558.9133097330729\n",
      "Epoch: 23650\tTrain loss: 527.76416015625\n",
      "Epoch: 23700\tTrain loss: 500.54046630859375\n",
      "Epoch: 23750\tTrain loss: 476.9150899251302\n",
      "Epoch: 23800\tTrain loss: 456.2444661458333\n",
      "Epoch: 23850\tTrain loss: 438.0005340576172\n",
      "Epoch: 23900\tTrain loss: 421.4690806070964\n",
      "Epoch: 23950\tTrain loss: 406.7042973836263\n",
      "Epoch: 24000\tTrain loss: 393.41546122233075\n",
      "Epoch: 24050\tTrain loss: 380.96828206380206\n",
      "Epoch: 24100\tTrain loss: 2559.2293599446616\n",
      "Epoch: 24150\tTrain loss: 745.2314453125\n",
      "Epoch: 24200\tTrain loss: 480.6637268066406\n",
      "Epoch: 24250\tTrain loss: 452.97833251953125\n",
      "Epoch: 24300\tTrain loss: 437.3051613171895\n",
      "Epoch: 24350\tTrain loss: 424.2709147135417\n",
      "Epoch: 24400\tTrain loss: 412.8612060546875\n",
      "Epoch: 24450\tTrain loss: 402.76202392578125\n",
      "Epoch: 24500\tTrain loss: 393.37237548828125\n",
      "Epoch: 24550\tTrain loss: 384.3517138163249\n",
      "Epoch: 24600\tTrain loss: 375.5248107910156\n",
      "Epoch: 24650\tTrain loss: 366.79651641845703\n",
      "Epoch: 24700\tTrain loss: 358.21678670247394\n",
      "Epoch: 24750\tTrain loss: 349.755126953125\n",
      "Epoch: 24800\tTrain loss: 341.44712829589844\n",
      "Epoch: 24850\tTrain loss: 333.5159047444661\n",
      "Epoch: 24900\tTrain loss: 326.1814096371333\n",
      "Epoch: 24950\tTrain loss: 319.11938438812894\n",
      "Epoch: 25000\tTrain loss: 312.42894236246747\n",
      "Epoch: 25050\tTrain loss: 306.03863525390625\n",
      "Epoch: 25100\tTrain loss: 300.01475471258163\n",
      "Epoch: 25150\tTrain loss: 294.245854695638\n",
      "Epoch: 25200\tTrain loss: 289.0101547241211\n",
      "Epoch: 25250\tTrain loss: 284.10905838012695\n",
      "Epoch: 25300\tTrain loss: 279.7271143595378\n",
      "Epoch: 25350\tTrain loss: 275.562957127889\n",
      "Epoch: 25400\tTrain loss: 271.6139914194743\n",
      "Epoch: 25450\tTrain loss: 267.80942952632904\n",
      "Epoch: 25500\tTrain loss: 264.2342535654704\n",
      "Epoch: 25550\tTrain loss: 260.8555038968722\n",
      "Epoch: 25600\tTrain loss: 257.74530665079754\n",
      "Epoch: 25650\tTrain loss: 254.5887419382731\n",
      "Epoch: 25700\tTrain loss: 251.70122655232748\n",
      "Epoch: 25750\tTrain loss: 248.92090479532877\n",
      "Epoch: 25800\tTrain loss: 246.4368985493978\n",
      "Epoch: 25850\tTrain loss: 244.05630683898926\n",
      "Epoch: 25900\tTrain loss: 241.6411313811938\n",
      "Epoch: 25950\tTrain loss: 239.5406223932902\n",
      "Epoch: 26000\tTrain loss: 237.36590194702148\n",
      "Epoch: 26050\tTrain loss: 235.25351254145303\n",
      "Epoch: 26100\tTrain loss: 233.80055753638348\n",
      "Epoch: 26150\tTrain loss: 231.65252812703451\n",
      "Epoch: 26200\tTrain loss: 229.9026781717936\n",
      "Epoch: 26250\tTrain loss: 228.4243075052897\n",
      "Epoch: 26300\tTrain loss: 226.79496002197266\n",
      "Epoch: 26350\tTrain loss: 225.6656405131022\n",
      "Epoch: 26400\tTrain loss: 224.23195401827493\n",
      "Epoch: 26450\tTrain loss: 222.65315373738608\n",
      "Epoch: 26500\tTrain loss: 221.4627193150421\n",
      "Epoch: 26550\tTrain loss: 1660.224365234375\n",
      "Epoch: 26600\tTrain loss: 300.4464747111003\n",
      "Epoch: 26650\tTrain loss: 270.9864273071289\n",
      "Epoch: 26700\tTrain loss: 258.123680750529\n",
      "Epoch: 26750\tTrain loss: 237.1334941883882\n",
      "Epoch: 26800\tTrain loss: 231.14652736236653\n",
      "Epoch: 26850\tTrain loss: 225.94257275263467\n",
      "Epoch: 26900\tTrain loss: 224.4556744893392\n",
      "Epoch: 26950\tTrain loss: 238.2231114593645\n",
      "Epoch: 27000\tTrain loss: 229.97661844889322\n",
      "Epoch: 27050\tTrain loss: 222.16480127970377\n",
      "Epoch: 27100\tTrain loss: 213.90219140052795\n",
      "Epoch: 27150\tTrain loss: 201.1339905539838\n",
      "Epoch: 27200\tTrain loss: 182.99639765421549\n",
      "Epoch: 27250\tTrain loss: 169.11024324099222\n",
      "Epoch: 27300\tTrain loss: 158.9262669881185\n",
      "Epoch: 27350\tTrain loss: 145.26183485984802\n",
      "Epoch: 27400\tTrain loss: 151.5755345026652\n",
      "Epoch: 27450\tTrain loss: 126.28011274337769\n",
      "Epoch: 27500\tTrain loss: 114.82793792088826\n",
      "Epoch: 27550\tTrain loss: 110.60424931844075\n",
      "Epoch: 27600\tTrain loss: 95.30102237065633\n",
      "Epoch: 27650\tTrain loss: 91.75420570373535\n",
      "Epoch: 27700\tTrain loss: 77.43694750467937\n",
      "Epoch: 27750\tTrain loss: 72.48515288035075\n",
      "Epoch: 27800\tTrain loss: 68.64779084424178\n",
      "Epoch: 27850\tTrain loss: 56.14195537567139\n",
      "Epoch: 27900\tTrain loss: 49.90571562449137\n",
      "Epoch: 27950\tTrain loss: 45.417575200398765\n",
      "Epoch: 28000\tTrain loss: 41.6235656340917\n",
      "Epoch: 28050\tTrain loss: 35.99912277857462\n",
      "Epoch: 28100\tTrain loss: 44.948885917663574\n",
      "Epoch: 28150\tTrain loss: 31.8668425877889\n",
      "Epoch: 28200\tTrain loss: 26.242417454719543\n",
      "Epoch: 28250\tTrain loss: 23.284523804982502\n",
      "Epoch: 28300\tTrain loss: 28.889612197875977\n",
      "Epoch: 28350\tTrain loss: 19.487664381663006\n",
      "Epoch: 28400\tTrain loss: 18.26129310950637\n",
      "Epoch: 28450\tTrain loss: 15.388894774019718\n",
      "Epoch: 28500\tTrain loss: 13.833508888880411\n",
      "Epoch: 28550\tTrain loss: 11.928831696510315\n",
      "Epoch: 28600\tTrain loss: 16.052207310994465\n",
      "Epoch: 28650\tTrain loss: 10.404600063959757\n",
      "Epoch: 28700\tTrain loss: 9.974315087000528\n",
      "Epoch: 28750\tTrain loss: 7.945644526742399\n",
      "Epoch: 28800\tTrain loss: 61.24852466583252\n",
      "Epoch: 28850\tTrain loss: 8.833956718444824\n",
      "Epoch: 28900\tTrain loss: 6.870980262756348\n",
      "Epoch: 28950\tTrain loss: 5.849240104357402\n",
      "Epoch: 29000\tTrain loss: 5.029234802660842\n",
      "Epoch: 29050\tTrain loss: 4.336957315603892\n",
      "Epoch: 29100\tTrain loss: 4.028859933217366\n",
      "Epoch: 29150\tTrain loss: 5.313287181779742\n",
      "Epoch: 29200\tTrain loss: 3.5377608935038247\n",
      "Epoch: 29250\tTrain loss: 2.505463272333145\n",
      "Epoch: 29300\tTrain loss: 2.9353174765904746\n",
      "Epoch: 29350\tTrain loss: 7.53531289100647\n",
      "Epoch: 29400\tTrain loss: 1.7279644285639126\n",
      "Epoch: 29450\tTrain loss: 1.4100109736124675\n",
      "Epoch: 29500\tTrain loss: 413.69124417503673\n",
      "Epoch: 29550\tTrain loss: 2.487116018931071\n",
      "Epoch: 29600\tTrain loss: 3.501684625943502\n",
      "Epoch: 29650\tTrain loss: 0.8354898989200592\n",
      "Epoch: 29700\tTrain loss: 0.9573141584793726\n",
      "Epoch: 29750\tTrain loss: 0.5940308428059021\n",
      "Epoch: 29800\tTrain loss: 0.6149444381395975\n",
      "Epoch: 29850\tTrain loss: 0.5319554569820563\n",
      "Epoch: 29900\tTrain loss: 50.23020283381144\n",
      "Epoch: 29950\tTrain loss: 0.3848862051963806\n",
      "Training CFRNN\n",
      "Epoch: 0\tTrain loss: 9919806.708333334\n",
      "Epoch: 50\tTrain loss: 9807781.854166666\n",
      "Epoch: 100\tTrain loss: 9703823.5625\n",
      "Epoch: 150\tTrain loss: 9602504.166666666\n",
      "Epoch: 200\tTrain loss: 9505099.28125\n",
      "Epoch: 250\tTrain loss: 9409613.729166666\n",
      "Epoch: 300\tTrain loss: 9315729.145833334\n",
      "Epoch: 350\tTrain loss: 9224475.479166666\n",
      "Epoch: 400\tTrain loss: 9134988.572916666\n",
      "Epoch: 450\tTrain loss: 9048108.770833334\n",
      "Epoch: 500\tTrain loss: 8961801.614583334\n",
      "Epoch: 550\tTrain loss: 8878015.9375\n",
      "Epoch: 600\tTrain loss: 8795675.260416666\n",
      "Epoch: 650\tTrain loss: 8715428.885416666\n",
      "Epoch: 700\tTrain loss: 8636770.729166666\n",
      "Epoch: 750\tTrain loss: 8559525.729166666\n",
      "Epoch: 800\tTrain loss: 8483828.0625\n",
      "Epoch: 850\tTrain loss: 8409217.583333334\n",
      "Epoch: 900\tTrain loss: 8336937.510416667\n",
      "Epoch: 950\tTrain loss: 8265678.432291667\n",
      "Epoch: 1000\tTrain loss: 8196774.9375\n",
      "Epoch: 1050\tTrain loss: 8128194.979166667\n",
      "Epoch: 1100\tTrain loss: 8061873.450520833\n",
      "Epoch: 1150\tTrain loss: 7996599.822916667\n",
      "Epoch: 1200\tTrain loss: 7933325.96875\n",
      "Epoch: 1250\tTrain loss: 7870723.390625\n",
      "Epoch: 1300\tTrain loss: 7809455.302083333\n",
      "Epoch: 1350\tTrain loss: 7750258.4375\n",
      "Epoch: 1400\tTrain loss: 7691271.689453125\n",
      "Epoch: 1450\tTrain loss: 7634611.277994792\n",
      "Epoch: 1500\tTrain loss: 7578760.796875\n",
      "Epoch: 1550\tTrain loss: 7523835.807291667\n",
      "Epoch: 1600\tTrain loss: 7470215.447916667\n",
      "Epoch: 1650\tTrain loss: 7418262.685546875\n",
      "Epoch: 1700\tTrain loss: 7367282.794270833\n",
      "Epoch: 1750\tTrain loss: 7317870.666666667\n",
      "Epoch: 1800\tTrain loss: 7268945.098958333\n",
      "Epoch: 1850\tTrain loss: 7221367.950520833\n",
      "Epoch: 1900\tTrain loss: 7175066.777994792\n",
      "Epoch: 1950\tTrain loss: 7130234.130208333\n",
      "Epoch: 2000\tTrain loss: 7086378.153645833\n",
      "Epoch: 2050\tTrain loss: 7043215.6988932295\n",
      "Epoch: 2100\tTrain loss: 7001344.063802083\n",
      "Epoch: 2150\tTrain loss: 6960518.7578125\n",
      "Epoch: 2200\tTrain loss: 6920789.571614583\n",
      "Epoch: 2250\tTrain loss: 6882266.924479167\n",
      "Epoch: 2300\tTrain loss: 6844656.8515625\n",
      "Epoch: 2350\tTrain loss: 6807796.729166667\n",
      "Epoch: 2400\tTrain loss: 6772200.1953125\n",
      "Epoch: 2450\tTrain loss: 6737439.010416667\n",
      "Epoch: 2500\tTrain loss: 6703977.092447917\n",
      "Epoch: 2550\tTrain loss: 6670973.725260417\n",
      "Epoch: 2600\tTrain loss: 6639068.42578125\n",
      "Epoch: 2650\tTrain loss: 6607986.841145833\n",
      "Epoch: 2700\tTrain loss: 6577472.673177083\n",
      "Epoch: 2750\tTrain loss: 6548037.40234375\n",
      "Epoch: 2800\tTrain loss: 6519413.540364583\n",
      "Epoch: 2850\tTrain loss: 6479769.845052083\n",
      "Epoch: 2900\tTrain loss: 6437415.024739583\n",
      "Epoch: 2950\tTrain loss: 6362030.541015625\n",
      "Epoch: 3000\tTrain loss: 6355472.088541667\n",
      "Epoch: 3050\tTrain loss: 6281832.423665364\n",
      "Epoch: 3100\tTrain loss: 6244780.3232421875\n",
      "Epoch: 3150\tTrain loss: 6237646.31640625\n",
      "Epoch: 3200\tTrain loss: 6170926.561197917\n",
      "Epoch: 3250\tTrain loss: 6181635.141927083\n",
      "Epoch: 3300\tTrain loss: 6139775.515625\n",
      "Epoch: 3350\tTrain loss: 6075431.94140625\n",
      "Epoch: 3400\tTrain loss: 6024385.141276042\n",
      "Epoch: 3450\tTrain loss: 5986593.529459636\n",
      "Epoch: 3500\tTrain loss: 5949725.128092448\n",
      "Epoch: 3550\tTrain loss: 5914786.2802734375\n",
      "Epoch: 3600\tTrain loss: 5879542.730997722\n",
      "Epoch: 3650\tTrain loss: 5844542.319661458\n",
      "Epoch: 3700\tTrain loss: 5809753.865071614\n",
      "Epoch: 3750\tTrain loss: 5774531.785970052\n",
      "Epoch: 3800\tTrain loss: 5739758.438639323\n",
      "Epoch: 3850\tTrain loss: 5705236.964192708\n",
      "Epoch: 3900\tTrain loss: 5671152.528361003\n",
      "Epoch: 3950\tTrain loss: 5636801.158365886\n",
      "Epoch: 4000\tTrain loss: 5602507.3714192705\n",
      "Epoch: 4050\tTrain loss: 5568478.95707194\n",
      "Epoch: 4100\tTrain loss: 5534736.0244140625\n",
      "Epoch: 4150\tTrain loss: 5501142.233072917\n",
      "Epoch: 4200\tTrain loss: 5466909.571329753\n",
      "Epoch: 4250\tTrain loss: 5433547.767089844\n",
      "Epoch: 4300\tTrain loss: 5399767.0432942705\n",
      "Epoch: 4350\tTrain loss: 5366590.147949219\n",
      "Epoch: 4400\tTrain loss: 5333155.532552083\n",
      "Epoch: 4450\tTrain loss: 5300554.3649088545\n",
      "Epoch: 4500\tTrain loss: 5267132.479736328\n",
      "Epoch: 4550\tTrain loss: 5233981.647623698\n",
      "Epoch: 4600\tTrain loss: 5200980.096272786\n",
      "Epoch: 4650\tTrain loss: 5168264.470458984\n",
      "Epoch: 4700\tTrain loss: 5135863.392333984\n",
      "Epoch: 4750\tTrain loss: 5103341.1663411455\n",
      "Epoch: 4800\tTrain loss: 5071643.4508463545\n",
      "Epoch: 4850\tTrain loss: 5038342.213378906\n",
      "Epoch: 4900\tTrain loss: 5006051.321695964\n",
      "Epoch: 4950\tTrain loss: 4974230.917805989\n",
      "Epoch: 5000\tTrain loss: 4942406.776814778\n",
      "Epoch: 5050\tTrain loss: 4910360.221761067\n",
      "Epoch: 5100\tTrain loss: 4878353.051595052\n",
      "Epoch: 5150\tTrain loss: 4846766.920735677\n",
      "Epoch: 5200\tTrain loss: 4814964.503011067\n",
      "Epoch: 5250\tTrain loss: 4783850.088541667\n",
      "Epoch: 5300\tTrain loss: 4752247.03507487\n",
      "Epoch: 5350\tTrain loss: 4720833.674479167\n",
      "Epoch: 5400\tTrain loss: 4689947.051269531\n",
      "Epoch: 5450\tTrain loss: 4658948.534993489\n",
      "Epoch: 5500\tTrain loss: 4627916.064656575\n",
      "Epoch: 5550\tTrain loss: 4597200.1044921875\n",
      "Epoch: 5600\tTrain loss: 4566235.459431966\n",
      "Epoch: 5650\tTrain loss: 4535768.994954427\n",
      "Epoch: 5700\tTrain loss: 4504914.789550781\n",
      "Epoch: 5750\tTrain loss: 4475198.004069011\n",
      "Epoch: 5800\tTrain loss: 4444229.555501302\n",
      "Epoch: 5850\tTrain loss: 4414196.217854817\n",
      "Epoch: 5900\tTrain loss: 4384206.113444011\n",
      "Epoch: 5950\tTrain loss: 4353908.203694661\n",
      "Epoch: 6000\tTrain loss: 4324260.5745442705\n",
      "Epoch: 6050\tTrain loss: 4294620.351074219\n",
      "Epoch: 6100\tTrain loss: 4265032.39070638\n",
      "Epoch: 6150\tTrain loss: 4235411.209879558\n",
      "Epoch: 6200\tTrain loss: 4206008.290120442\n",
      "Epoch: 6250\tTrain loss: 4176402.3837890625\n",
      "Epoch: 6300\tTrain loss: 4147406.3106282554\n",
      "Epoch: 6350\tTrain loss: 4118053.6687825522\n",
      "Epoch: 6400\tTrain loss: 4088911.7801106772\n",
      "Epoch: 6450\tTrain loss: 4059978.9158528647\n",
      "Epoch: 6500\tTrain loss: 4031233.5226236978\n",
      "Epoch: 6550\tTrain loss: 4002516.5077311196\n",
      "Epoch: 6600\tTrain loss: 3973951.302775065\n",
      "Epoch: 6650\tTrain loss: 3945358.0144856772\n",
      "Epoch: 6700\tTrain loss: 3917084.1204427085\n",
      "Epoch: 6750\tTrain loss: 3888767.6907552085\n",
      "Epoch: 6800\tTrain loss: 3860622.881184896\n",
      "Epoch: 6850\tTrain loss: 3833335.5283203125\n",
      "Epoch: 6900\tTrain loss: 3804726.7631835938\n",
      "Epoch: 6950\tTrain loss: 3777010.0704752603\n",
      "Epoch: 7000\tTrain loss: 3748955.4767252603\n",
      "Epoch: 7050\tTrain loss: 3721267.9947916665\n",
      "Epoch: 7100\tTrain loss: 3693774.2534179688\n",
      "Epoch: 7150\tTrain loss: 3666409.0694986978\n",
      "Epoch: 7200\tTrain loss: 3638995.4752604165\n",
      "Epoch: 7250\tTrain loss: 3612836.7934570312\n",
      "Epoch: 7300\tTrain loss: 3585045.1267903647\n",
      "Epoch: 7350\tTrain loss: 3557862.0259602866\n",
      "Epoch: 7400\tTrain loss: 3531177.104573568\n",
      "Epoch: 7450\tTrain loss: 3505064.8894856772\n",
      "Epoch: 7500\tTrain loss: 3477463.9939778647\n",
      "Epoch: 7550\tTrain loss: 3450841.476155599\n",
      "Epoch: 7600\tTrain loss: 3424761.4054361978\n",
      "Epoch: 7650\tTrain loss: 3398201.9276123047\n",
      "Epoch: 7700\tTrain loss: 3371963.514851888\n",
      "Epoch: 7750\tTrain loss: 3345678.5775553384\n",
      "Epoch: 7800\tTrain loss: 3319543.855794271\n",
      "Epoch: 7850\tTrain loss: 3293764.0313720703\n",
      "Epoch: 7900\tTrain loss: 3268197.746419271\n",
      "Epoch: 7950\tTrain loss: 3242116.8286132812\n",
      "Epoch: 8000\tTrain loss: 3216488.9223225913\n",
      "Epoch: 8050\tTrain loss: 3191140.3533528647\n",
      "Epoch: 8100\tTrain loss: 3165904.8334147134\n",
      "Epoch: 8150\tTrain loss: 3140504.7227376304\n",
      "Epoch: 8200\tTrain loss: 3115246.1333007812\n",
      "Epoch: 8250\tTrain loss: 3090956.639933268\n",
      "Epoch: 8300\tTrain loss: 3065659.2869059243\n",
      "Epoch: 8350\tTrain loss: 3040586.153564453\n",
      "Epoch: 8400\tTrain loss: 3015423.3670247397\n",
      "Epoch: 8450\tTrain loss: 2991107.3411458335\n",
      "Epoch: 8500\tTrain loss: 2966267.196411133\n",
      "Epoch: 8550\tTrain loss: 2941947.517252604\n",
      "Epoch: 8600\tTrain loss: 2917683.326904297\n",
      "Epoch: 8650\tTrain loss: 2893599.511393229\n",
      "Epoch: 8700\tTrain loss: 2869479.0234375\n",
      "Epoch: 8750\tTrain loss: 2845113.7165934243\n",
      "Epoch: 8800\tTrain loss: 2821472.4244384766\n",
      "Epoch: 8850\tTrain loss: 2797696.0841471353\n",
      "Epoch: 8900\tTrain loss: 2773875.782470703\n",
      "Epoch: 8950\tTrain loss: 2750375.233968099\n",
      "Epoch: 9000\tTrain loss: 2726854.0555013022\n",
      "Epoch: 9050\tTrain loss: 2703553.1794026694\n",
      "Epoch: 9100\tTrain loss: 2679967.8872070312\n",
      "Epoch: 9150\tTrain loss: 2656989.90625\n",
      "Epoch: 9200\tTrain loss: 2633941.0493164062\n",
      "Epoch: 9250\tTrain loss: 2611232.197591146\n",
      "Epoch: 9300\tTrain loss: 2588107.7400716147\n",
      "Epoch: 9350\tTrain loss: 2565562.668741862\n",
      "Epoch: 9400\tTrain loss: 2542798.056966146\n",
      "Epoch: 9450\tTrain loss: 2522862.8719075522\n",
      "Epoch: 9500\tTrain loss: 2497852.2628580728\n",
      "Epoch: 9550\tTrain loss: 2475558.8865559897\n",
      "Epoch: 9600\tTrain loss: 2453192.4707845054\n",
      "Epoch: 9650\tTrain loss: 2431358.056315104\n",
      "Epoch: 9700\tTrain loss: 2409378.720703125\n",
      "Epoch: 9750\tTrain loss: 2387295.4412841797\n",
      "Epoch: 9800\tTrain loss: 2365594.937825521\n",
      "Epoch: 9850\tTrain loss: 2343770.307413737\n",
      "Epoch: 9900\tTrain loss: 2322562.3483072915\n",
      "Epoch: 9950\tTrain loss: 2301039.648803711\n",
      "Epoch: 10000\tTrain loss: 2279533.6588541665\n",
      "Epoch: 10050\tTrain loss: 2258275.0373942056\n",
      "Epoch: 10100\tTrain loss: 2237086.7882486978\n",
      "Epoch: 10150\tTrain loss: 2216237.4741210938\n",
      "Epoch: 10200\tTrain loss: 2195301.752360026\n",
      "Epoch: 10250\tTrain loss: 2174261.163736979\n",
      "Epoch: 10300\tTrain loss: 2153517.4405924478\n",
      "Epoch: 10350\tTrain loss: 2133125.2807617188\n",
      "Epoch: 10400\tTrain loss: 2112513.6384277344\n",
      "Epoch: 10450\tTrain loss: 2091953.7096354167\n",
      "Epoch: 10500\tTrain loss: 2071737.524943034\n",
      "Epoch: 10550\tTrain loss: 2051719.9552408855\n",
      "Epoch: 10600\tTrain loss: 2031505.8791503906\n",
      "Epoch: 10650\tTrain loss: 2011483.0239257812\n",
      "Epoch: 10700\tTrain loss: 1991792.194173177\n",
      "Epoch: 10750\tTrain loss: 1971738.6069335938\n",
      "Epoch: 10800\tTrain loss: 1952117.2841796875\n",
      "Epoch: 10850\tTrain loss: 1932511.8204752605\n",
      "Epoch: 10900\tTrain loss: 1913252.682454427\n",
      "Epoch: 10950\tTrain loss: 1894015.4343261719\n",
      "Epoch: 11000\tTrain loss: 1874608.7528076172\n",
      "Epoch: 11050\tTrain loss: 1855577.316040039\n",
      "Epoch: 11100\tTrain loss: 1836458.3081054688\n",
      "Epoch: 11150\tTrain loss: 1817326.0947265625\n",
      "Epoch: 11200\tTrain loss: 1798641.7005208333\n",
      "Epoch: 11250\tTrain loss: 1779905.0318196614\n",
      "Epoch: 11300\tTrain loss: 1761259.6995442708\n",
      "Epoch: 11350\tTrain loss: 1742877.7176920574\n",
      "Epoch: 11400\tTrain loss: 1724380.7454427083\n",
      "Epoch: 11450\tTrain loss: 1706136.9502766926\n",
      "Epoch: 11500\tTrain loss: 1687875.3772786458\n",
      "Epoch: 11550\tTrain loss: 1669983.1995442708\n",
      "Epoch: 11600\tTrain loss: 1651918.8830566406\n",
      "Epoch: 11650\tTrain loss: 1633901.8372395833\n",
      "Epoch: 11700\tTrain loss: 1616908.1823730469\n",
      "Epoch: 11750\tTrain loss: 1598572.3697102864\n",
      "Epoch: 11800\tTrain loss: 1580824.3919677734\n",
      "Epoch: 11850\tTrain loss: 1563459.3993733723\n",
      "Epoch: 11900\tTrain loss: 1546328.4908040364\n",
      "Epoch: 11950\tTrain loss: 1528991.6559244792\n",
      "Epoch: 12000\tTrain loss: 1511724.6360677083\n",
      "Epoch: 12050\tTrain loss: 1494775.7135416667\n",
      "Epoch: 12100\tTrain loss: 1477895.4392496746\n",
      "Epoch: 12150\tTrain loss: 1461125.4407552083\n",
      "Epoch: 12200\tTrain loss: 1444336.9935709636\n",
      "Epoch: 12250\tTrain loss: 1427736.9892985027\n",
      "Epoch: 12300\tTrain loss: 1411078.407063802\n",
      "Epoch: 12350\tTrain loss: 1394851.3508300781\n",
      "Epoch: 12400\tTrain loss: 1378408.510579427\n",
      "Epoch: 12450\tTrain loss: 1362141.7985432942\n",
      "Epoch: 12500\tTrain loss: 1346235.3063151042\n",
      "Epoch: 12550\tTrain loss: 1330209.1397298176\n",
      "Epoch: 12600\tTrain loss: 1314189.1212158203\n",
      "Epoch: 12650\tTrain loss: 1298559.2604573567\n",
      "Epoch: 12700\tTrain loss: 1282832.3851725261\n",
      "Epoch: 12750\tTrain loss: 1267396.2097167969\n",
      "Epoch: 12800\tTrain loss: 1251846.0978190105\n",
      "Epoch: 12850\tTrain loss: 1236479.9123942058\n",
      "Epoch: 12900\tTrain loss: 1221294.6685384114\n",
      "Epoch: 12950\tTrain loss: 1205889.0076090496\n",
      "Epoch: 13000\tTrain loss: 1190873.421061198\n",
      "Epoch: 13050\tTrain loss: 1175941.4855957031\n",
      "Epoch: 13100\tTrain loss: 1161110.2384440105\n",
      "Epoch: 13150\tTrain loss: 1146435.1139322917\n",
      "Epoch: 13200\tTrain loss: 1131743.5596923828\n",
      "Epoch: 13250\tTrain loss: 1117343.7871907551\n",
      "Epoch: 13300\tTrain loss: 1102840.885904948\n",
      "Epoch: 13350\tTrain loss: 1088604.5883789062\n",
      "Epoch: 13400\tTrain loss: 1074196.7856445312\n",
      "Epoch: 13450\tTrain loss: 1060235.149251302\n",
      "Epoch: 13500\tTrain loss: 1046239.6060791016\n",
      "Epoch: 13550\tTrain loss: 1032362.4185384115\n",
      "Epoch: 13600\tTrain loss: 1018586.7820638021\n",
      "Epoch: 13650\tTrain loss: 1004870.6876627604\n",
      "Epoch: 13700\tTrain loss: 991164.0463867188\n",
      "Epoch: 13750\tTrain loss: 977711.5373535156\n",
      "Epoch: 13800\tTrain loss: 964411.5390625\n",
      "Epoch: 13850\tTrain loss: 950976.9241129557\n",
      "Epoch: 13900\tTrain loss: 937831.3160807291\n",
      "Epoch: 13950\tTrain loss: 924804.9446614584\n",
      "Epoch: 14000\tTrain loss: 911785.8307291666\n",
      "Epoch: 14050\tTrain loss: 898980.6476236979\n",
      "Epoch: 14100\tTrain loss: 886147.4589436849\n",
      "Epoch: 14150\tTrain loss: 873812.6869303385\n",
      "Epoch: 14200\tTrain loss: 860967.0143229166\n",
      "Epoch: 14250\tTrain loss: 848504.1585286459\n",
      "Epoch: 14300\tTrain loss: 836381.0270996094\n",
      "Epoch: 14350\tTrain loss: 823826.9501953125\n",
      "Epoch: 14400\tTrain loss: 811731.5756835938\n",
      "Epoch: 14450\tTrain loss: 799535.4750976562\n",
      "Epoch: 14500\tTrain loss: 787573.6141764323\n",
      "Epoch: 14550\tTrain loss: 775773.3620605469\n",
      "Epoch: 14600\tTrain loss: 764109.3700358073\n",
      "Epoch: 14650\tTrain loss: 752352.0484619141\n",
      "Epoch: 14700\tTrain loss: 740893.3595377604\n",
      "Epoch: 14750\tTrain loss: 729477.5858561198\n",
      "Epoch: 14800\tTrain loss: 718227.0517985026\n",
      "Epoch: 14850\tTrain loss: 706920.8116861979\n",
      "Epoch: 14900\tTrain loss: 695666.5493164062\n",
      "Epoch: 14950\tTrain loss: 684683.8117268881\n",
      "Epoch: 15000\tTrain loss: 673766.0128580729\n",
      "Epoch: 15050\tTrain loss: 662967.614827474\n",
      "Epoch: 15100\tTrain loss: 652206.9264322916\n",
      "Epoch: 15150\tTrain loss: 641580.2428792318\n",
      "Epoch: 15200\tTrain loss: 631050.8787434896\n",
      "Epoch: 15250\tTrain loss: 620612.6965942383\n",
      "Epoch: 15300\tTrain loss: 610424.851969401\n",
      "Epoch: 15350\tTrain loss: 600243.5126139323\n",
      "Epoch: 15400\tTrain loss: 589998.139078776\n",
      "Epoch: 15450\tTrain loss: 580012.8168945312\n",
      "Epoch: 15500\tTrain loss: 570093.898844401\n",
      "Epoch: 15550\tTrain loss: 560241.1708984375\n",
      "Epoch: 15600\tTrain loss: 550534.3816731771\n",
      "Epoch: 15650\tTrain loss: 540879.8365885416\n",
      "Epoch: 15700\tTrain loss: 531440.1342773438\n",
      "Epoch: 15750\tTrain loss: 521991.0397135417\n",
      "Epoch: 15800\tTrain loss: 521910.1090494792\n",
      "Epoch: 15850\tTrain loss: 508838.1376953125\n",
      "Epoch: 15900\tTrain loss: 497045.38427734375\n",
      "Epoch: 15950\tTrain loss: 487817.99788411456\n",
      "Epoch: 16000\tTrain loss: 478681.4417317708\n",
      "Epoch: 16050\tTrain loss: 469820.8583984375\n",
      "Epoch: 16100\tTrain loss: 461006.54866536456\n",
      "Epoch: 16150\tTrain loss: 452356.5412597656\n",
      "Epoch: 16200\tTrain loss: 443780.4296875\n",
      "Epoch: 16250\tTrain loss: 435365.3401692708\n",
      "Epoch: 16300\tTrain loss: 426945.6787109375\n",
      "Epoch: 16350\tTrain loss: 418668.6140136719\n",
      "Epoch: 16400\tTrain loss: 410497.5572916667\n",
      "Epoch: 16450\tTrain loss: 402458.1267089844\n",
      "Epoch: 16500\tTrain loss: 394526.55525716144\n",
      "Epoch: 16550\tTrain loss: 386569.91422526044\n",
      "Epoch: 16600\tTrain loss: 378916.4533691406\n",
      "Epoch: 16650\tTrain loss: 371215.2350260417\n",
      "Epoch: 16700\tTrain loss: 363606.93896484375\n",
      "Epoch: 16750\tTrain loss: 356078.12150065106\n",
      "Epoch: 16800\tTrain loss: 348696.74300130206\n",
      "Epoch: 16850\tTrain loss: 341155.31787109375\n",
      "Epoch: 16900\tTrain loss: 333826.89046223956\n",
      "Epoch: 16950\tTrain loss: 326695.8180338542\n",
      "Epoch: 17000\tTrain loss: 319698.6591796875\n",
      "Epoch: 17050\tTrain loss: 312860.94278971356\n",
      "Epoch: 17100\tTrain loss: 306031.9244791667\n",
      "Epoch: 17150\tTrain loss: 299366.5752766927\n",
      "Epoch: 17200\tTrain loss: 292766.0358886719\n",
      "Epoch: 17250\tTrain loss: 286205.8948567708\n",
      "Epoch: 17300\tTrain loss: 279849.5479329427\n",
      "Epoch: 17350\tTrain loss: 273522.0283203125\n",
      "Epoch: 17400\tTrain loss: 267314.5415445964\n",
      "Epoch: 17450\tTrain loss: 261196.69873046875\n",
      "Epoch: 17500\tTrain loss: 255134.72998046875\n",
      "Epoch: 17550\tTrain loss: 249199.07153320312\n",
      "Epoch: 17600\tTrain loss: 243308.52954101562\n",
      "Epoch: 17650\tTrain loss: 237593.16219075522\n",
      "Epoch: 17700\tTrain loss: 231880.57552083334\n",
      "Epoch: 17750\tTrain loss: 226310.794921875\n",
      "Epoch: 17800\tTrain loss: 220837.28446451822\n",
      "Epoch: 17850\tTrain loss: 215472.74076334634\n",
      "Epoch: 17900\tTrain loss: 210098.90844726562\n",
      "Epoch: 17950\tTrain loss: 204924.40576171875\n",
      "Epoch: 18000\tTrain loss: 199774.3592122396\n",
      "Epoch: 18050\tTrain loss: 194695.49348958334\n",
      "Epoch: 18100\tTrain loss: 189779.3748372396\n",
      "Epoch: 18150\tTrain loss: 184903.79565429688\n",
      "Epoch: 18200\tTrain loss: 180088.65657552084\n",
      "Epoch: 18250\tTrain loss: 175436.32466634116\n",
      "Epoch: 18300\tTrain loss: 170830.78080240884\n",
      "Epoch: 18350\tTrain loss: 166256.40030924478\n",
      "Epoch: 18400\tTrain loss: 161833.92504882812\n",
      "Epoch: 18450\tTrain loss: 157446.8767903646\n",
      "Epoch: 18500\tTrain loss: 153211.04329427084\n",
      "Epoch: 18550\tTrain loss: 149011.79305013022\n",
      "Epoch: 18600\tTrain loss: 144912.42443847656\n",
      "Epoch: 18650\tTrain loss: 140841.94010416666\n",
      "Epoch: 18700\tTrain loss: 136894.9305013021\n",
      "Epoch: 18750\tTrain loss: 133054.6708984375\n",
      "Epoch: 18800\tTrain loss: 129324.3447265625\n",
      "Epoch: 18850\tTrain loss: 125531.68115234375\n",
      "Epoch: 18900\tTrain loss: 121895.15348307292\n",
      "Epoch: 18950\tTrain loss: 118347.3661295573\n",
      "Epoch: 19000\tTrain loss: 114850.71036783855\n",
      "Epoch: 19050\tTrain loss: 111480.49080403645\n",
      "Epoch: 19100\tTrain loss: 108120.33968098958\n",
      "Epoch: 19150\tTrain loss: 104867.80200195312\n",
      "Epoch: 19200\tTrain loss: 101676.9072265625\n",
      "Epoch: 19250\tTrain loss: 98529.41918945312\n",
      "Epoch: 19300\tTrain loss: 95494.59615071614\n",
      "Epoch: 19350\tTrain loss: 92501.37119547527\n",
      "Epoch: 19400\tTrain loss: 89795.16658528645\n",
      "Epoch: 19450\tTrain loss: 86768.87268066406\n",
      "Epoch: 19500\tTrain loss: 84007.95003255208\n",
      "Epoch: 19550\tTrain loss: 81291.51236979167\n",
      "Epoch: 19600\tTrain loss: 78627.58048502605\n",
      "Epoch: 19650\tTrain loss: 76070.64038085938\n",
      "Epoch: 19700\tTrain loss: 73526.2860514323\n",
      "Epoch: 19750\tTrain loss: 71116.0854695638\n",
      "Epoch: 19800\tTrain loss: 68679.68404134114\n",
      "Epoch: 19850\tTrain loss: 66348.84016927083\n",
      "Epoch: 19900\tTrain loss: 64084.14347330729\n",
      "Epoch: 19950\tTrain loss: 61872.79337565104\n",
      "Epoch: 20000\tTrain loss: 59682.91650390625\n",
      "Epoch: 20050\tTrain loss: 57558.7753499349\n",
      "Epoch: 20100\tTrain loss: 55531.435546875\n",
      "Epoch: 20150\tTrain loss: 53659.74861653646\n",
      "Epoch: 20200\tTrain loss: 51668.285400390625\n",
      "Epoch: 20250\tTrain loss: 49775.299479166664\n",
      "Epoch: 20300\tTrain loss: 47951.77583821615\n",
      "Epoch: 20350\tTrain loss: 46172.50826009115\n",
      "Epoch: 20400\tTrain loss: 44393.83378092448\n",
      "Epoch: 20450\tTrain loss: 42710.1191813151\n",
      "Epoch: 20500\tTrain loss: 41063.651204427086\n",
      "Epoch: 20550\tTrain loss: 39542.18225097656\n",
      "Epoch: 20600\tTrain loss: 38084.181966145836\n",
      "Epoch: 20650\tTrain loss: 36869.40108235677\n",
      "Epoch: 20700\tTrain loss: 35182.582356770836\n",
      "Epoch: 20750\tTrain loss: 33697.87284342448\n",
      "Epoch: 20800\tTrain loss: 32350.378499348957\n",
      "Epoch: 20850\tTrain loss: 31039.219645182293\n",
      "Epoch: 20900\tTrain loss: 29751.864908854168\n",
      "Epoch: 20950\tTrain loss: 28521.345540364582\n",
      "Epoch: 21000\tTrain loss: 27322.003580729168\n",
      "Epoch: 21050\tTrain loss: 26161.385498046875\n",
      "Epoch: 21100\tTrain loss: 25025.466918945312\n",
      "Epoch: 21150\tTrain loss: 23955.38877360026\n",
      "Epoch: 21200\tTrain loss: 22884.261637369793\n",
      "Epoch: 21250\tTrain loss: 21921.90185546875\n",
      "Epoch: 21300\tTrain loss: 20906.701131184895\n",
      "Epoch: 21350\tTrain loss: 19984.7934366862\n",
      "Epoch: 21400\tTrain loss: 19076.6782430013\n",
      "Epoch: 21450\tTrain loss: 18196.599365234375\n",
      "Epoch: 21500\tTrain loss: 17451.37274169922\n",
      "Epoch: 21550\tTrain loss: 16572.28253173828\n",
      "Epoch: 21600\tTrain loss: 16258.968627929688\n",
      "Epoch: 21650\tTrain loss: 15329.426981608072\n",
      "Epoch: 21700\tTrain loss: 14469.511901855469\n",
      "Epoch: 21750\tTrain loss: 13779.463175455729\n",
      "Epoch: 21800\tTrain loss: 13061.715372721354\n",
      "Epoch: 21850\tTrain loss: 12438.548095703125\n",
      "Epoch: 21900\tTrain loss: 11831.188761393229\n",
      "Epoch: 21950\tTrain loss: 11410.278381347656\n",
      "Epoch: 22000\tTrain loss: 10733.86484781901\n",
      "Epoch: 22050\tTrain loss: 10202.896545410156\n",
      "Epoch: 22100\tTrain loss: 9733.209208170572\n",
      "Epoch: 22150\tTrain loss: 9200.631876627604\n",
      "Epoch: 22200\tTrain loss: 8782.516316731771\n",
      "Epoch: 22250\tTrain loss: 8312.563415527344\n",
      "Epoch: 22300\tTrain loss: 7889.379740397136\n",
      "Epoch: 22350\tTrain loss: 7487.3329671223955\n",
      "Epoch: 22400\tTrain loss: 7122.502990722656\n",
      "Epoch: 22450\tTrain loss: 6944.732645670573\n",
      "Epoch: 22500\tTrain loss: 6536.942728678386\n",
      "Epoch: 22550\tTrain loss: 6152.004862467448\n",
      "Epoch: 22600\tTrain loss: 5809.852783203125\n",
      "Epoch: 22650\tTrain loss: 5486.980529785156\n",
      "Epoch: 22700\tTrain loss: 5216.5870361328125\n",
      "Epoch: 22750\tTrain loss: 4963.026306152344\n",
      "Epoch: 22800\tTrain loss: 4685.176188151042\n",
      "Epoch: 22850\tTrain loss: 4450.400085449219\n",
      "Epoch: 22900\tTrain loss: 4208.6120198567705\n",
      "Epoch: 22950\tTrain loss: 4199.409016927083\n",
      "Epoch: 23000\tTrain loss: 3818.7548828125\n",
      "Epoch: 23050\tTrain loss: 3602.9830322265625\n",
      "Epoch: 23100\tTrain loss: 3413.883585611979\n",
      "Epoch: 23150\tTrain loss: 3233.6720581054688\n",
      "Epoch: 23200\tTrain loss: 3064.8997802734375\n",
      "Epoch: 23250\tTrain loss: 2915.4353841145835\n",
      "Epoch: 23300\tTrain loss: 2752.4255981445312\n",
      "Epoch: 23350\tTrain loss: 2622.438273111979\n",
      "Epoch: 23400\tTrain loss: 2487.476765950521\n",
      "Epoch: 23450\tTrain loss: 2357.4949747721353\n",
      "Epoch: 23500\tTrain loss: 2315.6714884440103\n",
      "Epoch: 23550\tTrain loss: 2156.4152018229165\n",
      "Epoch: 23600\tTrain loss: 2033.6057535807292\n",
      "Epoch: 23650\tTrain loss: 1953.1792399088542\n",
      "Epoch: 23700\tTrain loss: 1839.7037150065105\n",
      "Epoch: 23750\tTrain loss: 1760.4876302083333\n",
      "Epoch: 23800\tTrain loss: 1687.171854654948\n",
      "Epoch: 23850\tTrain loss: 1599.5986735026042\n",
      "Epoch: 23900\tTrain loss: 1559.3656209309895\n",
      "Epoch: 23950\tTrain loss: 1454.5998128255208\n",
      "Epoch: 24000\tTrain loss: 1967.6585693359375\n",
      "Epoch: 24050\tTrain loss: 1575.7030843098958\n",
      "Epoch: 24100\tTrain loss: 1378.3993123372395\n",
      "Epoch: 24150\tTrain loss: 1471.591532389323\n",
      "Epoch: 24200\tTrain loss: 1339.0074666341145\n",
      "Epoch: 24250\tTrain loss: 1237.2070922851562\n",
      "Epoch: 24300\tTrain loss: 1193.5169270833333\n",
      "Epoch: 24350\tTrain loss: 1162.710673014323\n",
      "Epoch: 24400\tTrain loss: 1192.486796061198\n",
      "Epoch: 24450\tTrain loss: 1082.9575805664062\n",
      "Epoch: 24500\tTrain loss: 1046.180928548177\n",
      "Epoch: 24550\tTrain loss: 1116.0269775390625\n",
      "Epoch: 24600\tTrain loss: 993.2347819010416\n",
      "Epoch: 24650\tTrain loss: 963.0393269856771\n",
      "Epoch: 24700\tTrain loss: 960.2263793945312\n",
      "Epoch: 24750\tTrain loss: 927.1387939453125\n",
      "Epoch: 24800\tTrain loss: 899.968994140625\n",
      "Epoch: 24850\tTrain loss: 1045.7676188151042\n",
      "Epoch: 24900\tTrain loss: 858.4437662760416\n",
      "Epoch: 24950\tTrain loss: 835.7737630208334\n",
      "Epoch: 25000\tTrain loss: 845.1167399088541\n",
      "Epoch: 25050\tTrain loss: 815.1347249348959\n",
      "Epoch: 25100\tTrain loss: 792.9012552897135\n",
      "Epoch: 25150\tTrain loss: 786.3100891113281\n",
      "Epoch: 25200\tTrain loss: 1094.3590087890625\n",
      "Epoch: 25250\tTrain loss: 742.9734598795573\n",
      "Epoch: 25300\tTrain loss: 721.269297281901\n",
      "Epoch: 25350\tTrain loss: 3191.8878173828125\n",
      "Epoch: 25400\tTrain loss: 718.2294108072916\n",
      "Epoch: 25450\tTrain loss: 699.9030507405599\n",
      "Epoch: 25500\tTrain loss: 684.9531758626302\n",
      "Epoch: 25550\tTrain loss: 670.2770690917969\n",
      "Epoch: 25600\tTrain loss: 649.4668477376302\n",
      "Epoch: 25650\tTrain loss: 2614.0630798339844\n",
      "Epoch: 25700\tTrain loss: 939.0283711751302\n",
      "Epoch: 25750\tTrain loss: 906.9450276692709\n",
      "Epoch: 25800\tTrain loss: 887.721425374349\n",
      "Epoch: 25850\tTrain loss: 880.1222534179688\n",
      "Epoch: 25900\tTrain loss: 850.6898193359375\n",
      "Epoch: 25950\tTrain loss: 819.8343811035156\n",
      "Epoch: 26000\tTrain loss: 827.4325358072916\n",
      "Epoch: 26050\tTrain loss: 696.1927185058594\n",
      "Epoch: 26100\tTrain loss: 678.3763732910156\n",
      "Epoch: 26150\tTrain loss: 663.3846130371094\n",
      "Epoch: 26200\tTrain loss: 655.557009379069\n",
      "Epoch: 26250\tTrain loss: 668.6874186197916\n",
      "Epoch: 26300\tTrain loss: 721.009775797526\n",
      "Epoch: 26350\tTrain loss: 636.2429606119791\n",
      "Epoch: 26400\tTrain loss: 624.543690999349\n",
      "Epoch: 26450\tTrain loss: 617.0534464518229\n",
      "Epoch: 26500\tTrain loss: 615.6880187988281\n",
      "Epoch: 26550\tTrain loss: 611.7546691894531\n",
      "Epoch: 26600\tTrain loss: 604.5596110026041\n",
      "Epoch: 26650\tTrain loss: 619.0430603027344\n",
      "Epoch: 26700\tTrain loss: 588.0948333740234\n",
      "Epoch: 26750\tTrain loss: 588.214365641276\n",
      "Epoch: 26800\tTrain loss: 585.7486267089844\n",
      "Epoch: 26850\tTrain loss: 581.0946807861328\n",
      "Epoch: 26900\tTrain loss: 605.2429402669271\n",
      "Epoch: 26950\tTrain loss: 562.9978434244791\n",
      "Epoch: 27000\tTrain loss: 564.8171590169271\n",
      "Epoch: 27050\tTrain loss: 563.7363077799479\n",
      "Epoch: 27100\tTrain loss: 14122.726806640625\n",
      "Epoch: 27150\tTrain loss: 1302.3901774088542\n",
      "Epoch: 27200\tTrain loss: 1241.5558471679688\n",
      "Epoch: 27250\tTrain loss: 1151.56298828125\n",
      "Epoch: 27300\tTrain loss: 962.2090911865234\n",
      "Epoch: 27350\tTrain loss: 842.2726847330729\n",
      "Epoch: 27400\tTrain loss: 709.4379170735677\n",
      "Epoch: 27450\tTrain loss: 700.0390930175781\n",
      "Epoch: 27500\tTrain loss: 753.5448811848959\n",
      "Epoch: 27550\tTrain loss: 1513.0808003743489\n",
      "Epoch: 27600\tTrain loss: 671.884531656901\n",
      "Epoch: 27650\tTrain loss: 673.0682169596354\n",
      "Epoch: 27700\tTrain loss: 622.2646967569987\n",
      "Epoch: 27750\tTrain loss: 728.0060119628906\n",
      "Epoch: 27800\tTrain loss: 723.508066813151\n",
      "Epoch: 27850\tTrain loss: 709.3096110026041\n",
      "Epoch: 27900\tTrain loss: 591.1622009277344\n",
      "Epoch: 27950\tTrain loss: 601.9200744628906\n",
      "Epoch: 28000\tTrain loss: 580.3228861490885\n",
      "Epoch: 28050\tTrain loss: 556.6622873942057\n",
      "Epoch: 28100\tTrain loss: 547.4191182454427\n",
      "Epoch: 28150\tTrain loss: 542.1527201334635\n",
      "Epoch: 28200\tTrain loss: 535.0853118896484\n",
      "Epoch: 28250\tTrain loss: 532.142832438151\n",
      "Epoch: 28300\tTrain loss: 538.70703125\n",
      "Epoch: 28350\tTrain loss: 521.3849792480469\n",
      "Epoch: 28400\tTrain loss: 559.479933420817\n",
      "Epoch: 28450\tTrain loss: 513.427256266276\n",
      "Epoch: 28500\tTrain loss: 536.6811930338541\n",
      "Epoch: 28550\tTrain loss: 504.4889831542969\n",
      "Epoch: 28600\tTrain loss: 528.4733683268229\n",
      "Epoch: 28650\tTrain loss: 499.2969563802083\n",
      "Epoch: 28700\tTrain loss: 509.3773701985677\n",
      "Epoch: 28750\tTrain loss: 486.5626163482666\n",
      "Epoch: 28800\tTrain loss: 484.5431722005208\n",
      "Epoch: 28850\tTrain loss: 475.5783386230469\n",
      "Epoch: 28900\tTrain loss: 473.6938961346944\n",
      "Epoch: 28950\tTrain loss: 507.5839487711589\n",
      "Epoch: 29000\tTrain loss: 530.8181559244791\n",
      "Epoch: 29050\tTrain loss: 473.48374430338544\n",
      "Epoch: 29100\tTrain loss: 466.88258361816406\n",
      "Epoch: 29150\tTrain loss: 893.1883951822916\n",
      "Epoch: 29200\tTrain loss: 461.9930165608724\n",
      "Epoch: 29250\tTrain loss: 453.51842244466144\n",
      "Epoch: 29300\tTrain loss: 448.1978454589844\n",
      "Epoch: 29350\tTrain loss: 1168.5882364908855\n",
      "Epoch: 29400\tTrain loss: 761.3853352864584\n",
      "Epoch: 29450\tTrain loss: 464.17291259765625\n",
      "Epoch: 29500\tTrain loss: 454.98439025878906\n",
      "Epoch: 29550\tTrain loss: 452.8524576822917\n",
      "Epoch: 29600\tTrain loss: 446.44806416829425\n",
      "Epoch: 29650\tTrain loss: 438.81090037027997\n",
      "Epoch: 29700\tTrain loss: 431.62319533030194\n",
      "Epoch: 29750\tTrain loss: 430.7309322357178\n",
      "Epoch: 29800\tTrain loss: 461.4515787760417\n",
      "Epoch: 29850\tTrain loss: 421.511957804362\n",
      "Epoch: 29900\tTrain loss: 416.8822814623515\n",
      "Epoch: 29950\tTrain loss: 413.40369669596356\n"
     ]
    }
   ],
   "source": [
    "for baseline in CONFORMAL_BASELINES:\n",
    "    for seed in range(5):\n",
    "        run_medical_experiments(dataset='electricity', \n",
    "                                baseline=baseline,\n",
    "                                save_model=True, \n",
    "                                save_results=True,\n",
    "                                seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a45a98a5-3175-40ce-9c70-5c0b236300b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CFRNN\n",
      "40.0 \\(\\pm\\) 49.0\\%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for baseline in CONFORMAL_BASELINES:\n",
    "    print(baseline)\n",
    "    coverages_mean, coverages_std = get_joint_medical_coverages(baseline, 'electricity', seeds=range(5))\n",
    "    \n",
    "    print('{:.1f} \\\\(\\\\pm\\\\) {:.1f}\\\\%'.format(coverages_mean, coverages_std))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4074c07c-9cca-49c8-83b3-156d108530a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CFRNN\n",
      "4102.337948404948\n",
      "5236.533135474207\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for baseline in CONFORMAL_BASELINES:\n",
    "    print(baseline)\n",
    "    widths_mean, widths_std = get_medical_interval_widths(baseline, 'electricity', seeds=range(5))\n",
    "    \n",
    "    print(widths_mean)\n",
    "    print(widths_std)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b6d7af-a7c6-46eb-aead-23336fa9a665",
   "metadata": {},
   "source": [
    "## Ablation: Uncorrected calibration scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c58171e-7675-4983-851c-5a963d25aeec",
   "metadata": {},
   "source": [
    "#### Electricity Consumption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "752fa9ad-f6cc-43d3-9fb5-5bfdb071888e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40.0 \\(\\pm\\) 49.0\\%\n"
     ]
    }
   ],
   "source": [
    "coverages_mean, coverages_std = get_joint_medical_coverages('CFRNN', 'electricity', seeds=range(5), correct_conformal=True)\n",
    "    \n",
    "print('{:.1f} \\\\(\\\\pm\\\\) {:.1f}\\\\%'.format(coverages_mean, coverages_std))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "86d905e4-dc31-47ae-9b47-bda58c4c59aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40.0 \\(\\pm\\) 49.0\\%\n"
     ]
    }
   ],
   "source": [
    "coverages_mean, coverages_std = get_joint_medical_coverages('CFRNN', 'electricity', seeds=range(5), correct_conformal=False)\n",
    "    \n",
    "print('{:.1f} \\\\(\\\\pm\\\\) {:.1f}\\\\%'.format(coverages_mean, coverages_std))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "66c5f5a2-4423-4aff-b2e4-91cc7a3ce3be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100.0\\%, 100.0\\%]\n",
      "[0.0\\%, 0.0\\%]\n",
      "[0.0\\%, 50.0\\%]\n",
      "[0.0\\%, 50.0\\%]\n",
      "[100.0\\%, 100.0\\%]\n"
     ]
    }
   ],
   "source": [
    "for seed in range(5):\n",
    "    results = load_medical_results(dataset='electricity', baseline='CFRNN', seed=seed)\n",
    "    independent_coverages = results['Mean independent coverage']\n",
    "    print('[{:.1f}\\\\%, {:.1f}\\\\%]'.format(independent_coverages.min() * 100, independent_coverages.max() * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ab9b269d-13d6-4f21-8848-d6eaeb6bc8cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100.0\\%, 100.0\\%]\n",
      "[0.0\\%, 0.0\\%]\n",
      "[0.0\\%, 50.0\\%]\n",
      "[0.0\\%, 50.0\\%]\n",
      "[100.0\\%, 100.0\\%]\n"
     ]
    }
   ],
   "source": [
    "for seed in range(5):\n",
    "    uncorrected_mimic_results = get_uncorrected_medical_results(dataset='electricity', seed=seed)\n",
    "    independent_coverages = uncorrected_mimic_results['Mean independent coverage']\n",
    "    print('[{:.1f}\\\\%, {:.1f}\\\\%]'.format(independent_coverages.min() * 100, independent_coverages.max() * 100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('lab')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "55cd0a945cf9041a238b2dc7f6f29da44cde9fabaada4bd04363a8e3e2e8be13"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
