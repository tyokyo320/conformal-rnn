{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bfaf9e81-c095-4d66-812e-3459dbc93c65",
   "metadata": {},
   "source": [
    "# Experiments on real-world data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e8602d8b-cb7c-488b-9787-1e624204f8f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from utils.train_medical import run_medical_experiments\n",
    "from utils.results import (\n",
    "    get_joint_medical_coverages, \n",
    "    get_medical_interval_widths, \n",
    "    load_medical_results, \n",
    "    get_uncorrected_medical_results\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9455748f-ff7b-4fa6-8c02-45dfd5509810",
   "metadata": {},
   "source": [
    "To obtain the results as presented in the paper, run the following three sections."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20370c7c-eaca-49aa-8e45-7f91c01c4faa",
   "metadata": {},
   "source": [
    "## COVID-19 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "84cb28a9-05b8-4be6-9656-44cc61953beb",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training CFRNN\n",
      "Epoch: 0\tTrain loss: 1896881.875\n",
      "Epoch: 50\tTrain loss: 1867673.125\n",
      "Epoch: 100\tTrain loss: 1839294.125\n",
      "Epoch: 150\tTrain loss: 1812218.5\n",
      "Epoch: 200\tTrain loss: 1785856.0\n",
      "Epoch: 250\tTrain loss: 1760019.25\n",
      "Epoch: 300\tTrain loss: 1734625.125\n",
      "Epoch: 350\tTrain loss: 1709628.625\n",
      "Epoch: 400\tTrain loss: 1685001.0\n",
      "Epoch: 450\tTrain loss: 1660723.25\n",
      "Epoch: 500\tTrain loss: 1636781.25\n",
      "Epoch: 550\tTrain loss: 1613164.0\n",
      "Epoch: 600\tTrain loss: 1589862.25\n",
      "Epoch: 650\tTrain loss: 1566868.875\n",
      "Epoch: 700\tTrain loss: 1544177.0\n",
      "Epoch: 750\tTrain loss: 1521781.0\n",
      "Epoch: 800\tTrain loss: 1499676.0\n",
      "Epoch: 850\tTrain loss: 1477856.375\n",
      "Epoch: 900\tTrain loss: 1456318.375\n",
      "Epoch: 950\tTrain loss: 1435057.875\n",
      "Epoch: 1000\tTrain loss: 1414070.75\n",
      "Epoch: 1050\tTrain loss: 1393353.375\n",
      "Epoch: 1100\tTrain loss: 1372902.375\n",
      "Epoch: 1150\tTrain loss: 1352714.375\n",
      "Epoch: 1200\tTrain loss: 1332786.125\n",
      "Epoch: 1250\tTrain loss: 1313114.625\n",
      "Epoch: 1300\tTrain loss: 1293696.75\n",
      "Epoch: 1350\tTrain loss: 1274529.875\n",
      "Epoch: 1400\tTrain loss: 1255610.875\n",
      "Epoch: 1450\tTrain loss: 1236937.125\n",
      "Epoch: 1500\tTrain loss: 1218506.5\n",
      "Epoch: 1550\tTrain loss: 1200316.0\n",
      "Epoch: 1600\tTrain loss: 1182363.0\n",
      "Epoch: 1650\tTrain loss: 1164645.5\n",
      "Epoch: 1700\tTrain loss: 1147161.125\n",
      "Epoch: 1750\tTrain loss: 1129907.25\n",
      "Epoch: 1800\tTrain loss: 1112881.875\n",
      "Epoch: 1850\tTrain loss: 1096082.875\n",
      "Epoch: 1900\tTrain loss: 1079508.0\n",
      "Epoch: 1950\tTrain loss: 1063155.5\n",
      "Epoch: 2000\tTrain loss: 1047022.75\n",
      "Epoch: 2050\tTrain loss: 1031108.25\n",
      "Epoch: 2100\tTrain loss: 1015409.9375\n",
      "Epoch: 2150\tTrain loss: 999925.75\n",
      "Epoch: 2200\tTrain loss: 984654.0625\n",
      "Epoch: 2250\tTrain loss: 969592.9375\n",
      "Epoch: 2300\tTrain loss: 954740.6875\n",
      "Epoch: 2350\tTrain loss: 940095.375\n",
      "Epoch: 2400\tTrain loss: 925655.375\n",
      "Epoch: 2450\tTrain loss: 911419.0625\n",
      "Epoch: 2500\tTrain loss: 897384.625\n",
      "Epoch: 2550\tTrain loss: 883550.8125\n",
      "Epoch: 2600\tTrain loss: 869915.6875\n",
      "Epoch: 2650\tTrain loss: 856477.8125\n",
      "Epoch: 2700\tTrain loss: 843235.4375\n",
      "Epoch: 2750\tTrain loss: 830187.375\n",
      "Epoch: 2800\tTrain loss: 817331.875\n",
      "Epoch: 2850\tTrain loss: 804667.625\n",
      "Epoch: 2900\tTrain loss: 792193.1875\n",
      "Epoch: 2950\tTrain loss: 779907.0625\n",
      "Epoch: 3000\tTrain loss: 767807.8125\n",
      "Epoch: 3050\tTrain loss: 755894.1875\n",
      "Epoch: 3100\tTrain loss: 744164.6875\n",
      "Epoch: 3150\tTrain loss: 732618.0\n",
      "Epoch: 3200\tTrain loss: 721252.8125\n",
      "Epoch: 3250\tTrain loss: 710067.8125\n",
      "Epoch: 3300\tTrain loss: 699061.6875\n",
      "Epoch: 3350\tTrain loss: 688233.25\n",
      "Epoch: 3400\tTrain loss: 677581.0\n",
      "Epoch: 3450\tTrain loss: 667103.8125\n",
      "Epoch: 3500\tTrain loss: 656800.4375\n",
      "Epoch: 3550\tTrain loss: 646669.5625\n",
      "Epoch: 3600\tTrain loss: 636710.0\n",
      "Epoch: 3650\tTrain loss: 626920.5625\n",
      "Epoch: 3700\tTrain loss: 617299.875\n",
      "Epoch: 3750\tTrain loss: 607846.875\n",
      "Epoch: 3800\tTrain loss: 598560.375\n",
      "Epoch: 3850\tTrain loss: 589439.0625\n",
      "Epoch: 3900\tTrain loss: 580481.6875\n",
      "Epoch: 3950\tTrain loss: 571687.125\n",
      "Epoch: 4000\tTrain loss: 563054.375\n",
      "Epoch: 4050\tTrain loss: 554581.9375\n",
      "Epoch: 4100\tTrain loss: 546268.8125\n",
      "Epoch: 4150\tTrain loss: 538113.625\n",
      "Epoch: 4200\tTrain loss: 530115.375\n",
      "Epoch: 4250\tTrain loss: 522272.84375\n",
      "Epoch: 4300\tTrain loss: 514584.625\n",
      "Epoch: 4350\tTrain loss: 507049.875\n",
      "Epoch: 4400\tTrain loss: 499667.09375\n",
      "Epoch: 4450\tTrain loss: 492435.1875\n",
      "Epoch: 4500\tTrain loss: 485352.96875\n",
      "Epoch: 4550\tTrain loss: 478419.25\n",
      "Epoch: 4600\tTrain loss: 471632.6875\n",
      "Epoch: 4650\tTrain loss: 464992.09375\n",
      "Epoch: 4700\tTrain loss: 458496.3125\n",
      "Epoch: 4750\tTrain loss: 452144.0\n",
      "Epoch: 4800\tTrain loss: 445933.96875\n",
      "Epoch: 4850\tTrain loss: 439864.96875\n",
      "Epoch: 4900\tTrain loss: 433935.65625\n",
      "Epoch: 4950\tTrain loss: 428144.8125\n",
      "Epoch: 5000\tTrain loss: 422491.1875\n",
      "Epoch: 5050\tTrain loss: 416973.25\n",
      "Epoch: 5100\tTrain loss: 411589.90625\n",
      "Epoch: 5150\tTrain loss: 406339.71875\n",
      "Epoch: 5200\tTrain loss: 401221.4375\n",
      "Epoch: 5250\tTrain loss: 396233.5625\n",
      "Epoch: 5300\tTrain loss: 391374.875\n",
      "Epoch: 5350\tTrain loss: 386643.875\n",
      "Epoch: 5400\tTrain loss: 382039.15625\n",
      "Epoch: 5450\tTrain loss: 377559.375\n",
      "Epoch: 5500\tTrain loss: 373202.9375\n",
      "Epoch: 5550\tTrain loss: 368968.53125\n",
      "Epoch: 5600\tTrain loss: 364854.625\n",
      "Epoch: 5650\tTrain loss: 360859.75\n",
      "Epoch: 5700\tTrain loss: 356982.40625\n",
      "Epoch: 5750\tTrain loss: 353221.0\n",
      "Epoch: 5800\tTrain loss: 349574.03125\n",
      "Epoch: 5850\tTrain loss: 346039.90625\n",
      "Epoch: 5900\tTrain loss: 342617.0\n",
      "Epoch: 5950\tTrain loss: 339303.8125\n",
      "Epoch: 6000\tTrain loss: 336098.625\n",
      "Epoch: 6050\tTrain loss: 332999.78125\n",
      "Epoch: 6100\tTrain loss: 330005.59375\n",
      "Epoch: 6150\tTrain loss: 327114.4375\n",
      "Epoch: 6200\tTrain loss: 324324.5625\n",
      "Epoch: 6250\tTrain loss: 321634.28125\n",
      "Epoch: 6300\tTrain loss: 319041.1875\n",
      "Epoch: 6350\tTrain loss: 306438.65625\n",
      "Epoch: 6400\tTrain loss: 287161.25\n",
      "Epoch: 6450\tTrain loss: 266740.6875\n",
      "Epoch: 6500\tTrain loss: 260758.71875\n",
      "Epoch: 6550\tTrain loss: 255100.34375\n",
      "Epoch: 6600\tTrain loss: 249572.75\n",
      "Epoch: 6650\tTrain loss: 244171.5\n",
      "Epoch: 6700\tTrain loss: 238893.390625\n",
      "Epoch: 6750\tTrain loss: 233734.984375\n",
      "Epoch: 6800\tTrain loss: 228692.796875\n",
      "Epoch: 6850\tTrain loss: 223763.703125\n",
      "Epoch: 6900\tTrain loss: 218944.765625\n",
      "Epoch: 6950\tTrain loss: 214233.25\n",
      "Epoch: 7000\tTrain loss: 209626.671875\n",
      "Epoch: 7050\tTrain loss: 205122.6875\n",
      "Epoch: 7100\tTrain loss: 200728.734375\n",
      "Epoch: 7150\tTrain loss: 196413.09375\n",
      "Epoch: 7200\tTrain loss: 192203.421875\n",
      "Epoch: 7250\tTrain loss: 188088.890625\n",
      "Epoch: 7300\tTrain loss: 184067.25\n",
      "Epoch: 7350\tTrain loss: 180136.40625\n",
      "Epoch: 7400\tTrain loss: 176294.1875\n",
      "Epoch: 7450\tTrain loss: 172537.765625\n",
      "Epoch: 7500\tTrain loss: 168860.984375\n",
      "Epoch: 7550\tTrain loss: 165195.5\n",
      "Epoch: 7600\tTrain loss: 161397.078125\n",
      "Epoch: 7650\tTrain loss: 157744.265625\n",
      "Epoch: 7700\tTrain loss: 154138.453125\n",
      "Epoch: 7750\tTrain loss: 150580.5625\n",
      "Epoch: 7800\tTrain loss: 147070.1875\n",
      "Epoch: 7850\tTrain loss: 143608.4375\n",
      "Epoch: 7900\tTrain loss: 141832.859375\n",
      "Epoch: 7950\tTrain loss: 136844.40625\n",
      "Epoch: 8000\tTrain loss: 133523.34375\n",
      "Epoch: 8050\tTrain loss: 130255.3828125\n",
      "Epoch: 8100\tTrain loss: 127036.3359375\n",
      "Epoch: 8150\tTrain loss: 123865.9453125\n",
      "Epoch: 8200\tTrain loss: 120744.1171875\n",
      "Epoch: 8250\tTrain loss: 117670.84375\n",
      "Epoch: 8300\tTrain loss: 114647.90625\n",
      "Epoch: 8350\tTrain loss: 111670.359375\n",
      "Epoch: 8400\tTrain loss: 108743.1171875\n",
      "Epoch: 8450\tTrain loss: 105864.8203125\n",
      "Epoch: 8500\tTrain loss: 103034.6796875\n",
      "Epoch: 8550\tTrain loss: 100268.3828125\n",
      "Epoch: 8600\tTrain loss: 97517.015625\n",
      "Epoch: 8650\tTrain loss: 94828.8046875\n",
      "Epoch: 8700\tTrain loss: 92187.5546875\n",
      "Epoch: 8750\tTrain loss: 89594.1171875\n",
      "Epoch: 8800\tTrain loss: 87045.7734375\n",
      "Epoch: 8850\tTrain loss: 84557.421875\n",
      "Epoch: 8900\tTrain loss: 82087.8359375\n",
      "Epoch: 8950\tTrain loss: 79677.6640625\n",
      "Epoch: 9000\tTrain loss: 77313.6015625\n",
      "Epoch: 9050\tTrain loss: 74995.453125\n",
      "Epoch: 9100\tTrain loss: 72722.9609375\n",
      "Epoch: 9150\tTrain loss: 70495.90625\n",
      "Epoch: 9200\tTrain loss: 68314.015625\n",
      "Epoch: 9250\tTrain loss: 66177.0234375\n",
      "Epoch: 9300\tTrain loss: 64084.66015625\n",
      "Epoch: 9350\tTrain loss: 62036.6015625\n",
      "Epoch: 9400\tTrain loss: 60032.56640625\n",
      "Epoch: 9450\tTrain loss: 58072.19921875\n",
      "Epoch: 9500\tTrain loss: 56155.21484375\n",
      "Epoch: 9550\tTrain loss: 54281.3125\n",
      "Epoch: 9600\tTrain loss: 52450.1015625\n",
      "Epoch: 9650\tTrain loss: 50661.32421875\n",
      "Epoch: 9700\tTrain loss: 48914.78515625\n",
      "Epoch: 9750\tTrain loss: 47209.59375\n",
      "Epoch: 9800\tTrain loss: 45546.01171875\n",
      "Epoch: 9850\tTrain loss: 43923.9921875\n",
      "Epoch: 9900\tTrain loss: 42341.34375\n",
      "Epoch: 9950\tTrain loss: 40800.6484375\n",
      "Epoch: 10000\tTrain loss: 39298.14453125\n",
      "Epoch: 10050\tTrain loss: 37835.5546875\n",
      "Epoch: 10100\tTrain loss: 36412.16015625\n",
      "Epoch: 10150\tTrain loss: 35027.1171875\n",
      "Epoch: 10200\tTrain loss: 33680.24609375\n",
      "Epoch: 10250\tTrain loss: 32371.109375\n",
      "Epoch: 10300\tTrain loss: 31100.189453125\n",
      "Epoch: 10350\tTrain loss: 29864.216796875\n",
      "Epoch: 10400\tTrain loss: 28665.7734375\n",
      "Epoch: 10450\tTrain loss: 27502.9609375\n",
      "Epoch: 10500\tTrain loss: 26375.640625\n",
      "Epoch: 10550\tTrain loss: 25283.408203125\n",
      "Epoch: 10600\tTrain loss: 24225.708984375\n",
      "Epoch: 10650\tTrain loss: 23201.744140625\n",
      "Epoch: 10700\tTrain loss: 22212.552734375\n",
      "Epoch: 10750\tTrain loss: 21254.02734375\n",
      "Epoch: 10800\tTrain loss: 20329.0390625\n",
      "Epoch: 10850\tTrain loss: 19436.037109375\n",
      "Epoch: 10900\tTrain loss: 18574.482421875\n",
      "Epoch: 10950\tTrain loss: 17743.4765625\n",
      "Epoch: 11000\tTrain loss: 16942.890625\n",
      "Epoch: 11050\tTrain loss: 16171.96484375\n",
      "Epoch: 11100\tTrain loss: 15432.7353515625\n",
      "Epoch: 11150\tTrain loss: 14717.0419921875\n",
      "Epoch: 11200\tTrain loss: 14031.76953125\n",
      "Epoch: 11250\tTrain loss: 13524.2490234375\n",
      "Epoch: 11300\tTrain loss: 12830.9853515625\n",
      "Epoch: 11350\tTrain loss: 12191.9501953125\n",
      "Epoch: 11400\tTrain loss: 11595.89453125\n",
      "Epoch: 11450\tTrain loss: 11031.4755859375\n",
      "Epoch: 11500\tTrain loss: 10494.8603515625\n",
      "Epoch: 11550\tTrain loss: 9984.0595703125\n",
      "Epoch: 11600\tTrain loss: 9497.7451171875\n",
      "Epoch: 11650\tTrain loss: 9034.89453125\n",
      "Epoch: 11700\tTrain loss: 8594.6689453125\n",
      "Epoch: 11750\tTrain loss: 8176.20654296875\n",
      "Epoch: 11800\tTrain loss: 7778.8173828125\n",
      "Epoch: 11850\tTrain loss: 7401.78076171875\n",
      "Epoch: 11900\tTrain loss: 7044.4091796875\n",
      "Epoch: 11950\tTrain loss: 6706.021484375\n",
      "Epoch: 12000\tTrain loss: 6386.0458984375\n",
      "Epoch: 12050\tTrain loss: 6083.646484375\n",
      "Epoch: 12100\tTrain loss: 5798.337890625\n",
      "Epoch: 12150\tTrain loss: 5529.44384765625\n",
      "Epoch: 12200\tTrain loss: 5276.3388671875\n",
      "Epoch: 12250\tTrain loss: 5038.40087890625\n",
      "Epoch: 12300\tTrain loss: 4815.0126953125\n",
      "Epoch: 12350\tTrain loss: 4605.56982421875\n",
      "Epoch: 12400\tTrain loss: 4409.47119140625\n",
      "Epoch: 12450\tTrain loss: 4226.126953125\n",
      "Epoch: 12500\tTrain loss: 4054.953857421875\n",
      "Epoch: 12550\tTrain loss: 3895.375732421875\n",
      "Epoch: 12600\tTrain loss: 3746.830322265625\n",
      "Epoch: 12650\tTrain loss: 3608.762451171875\n",
      "Epoch: 12700\tTrain loss: 3480.630859375\n",
      "Epoch: 12750\tTrain loss: 3361.900390625\n",
      "Epoch: 12800\tTrain loss: 3252.05322265625\n",
      "Epoch: 12850\tTrain loss: 3150.58056640625\n",
      "Epoch: 12900\tTrain loss: 3057.20361328125\n",
      "Epoch: 12950\tTrain loss: 2970.784912109375\n",
      "Epoch: 13000\tTrain loss: 2891.51318359375\n",
      "Epoch: 13050\tTrain loss: 2818.71826171875\n",
      "Epoch: 13100\tTrain loss: 2751.938232421875\n",
      "Epoch: 13150\tTrain loss: 2690.7421875\n",
      "Epoch: 13200\tTrain loss: 2634.705810546875\n",
      "Epoch: 13250\tTrain loss: 2583.399658203125\n",
      "Epoch: 13300\tTrain loss: 2536.393310546875\n",
      "Epoch: 13350\tTrain loss: 2493.22509765625\n",
      "Epoch: 13400\tTrain loss: 2453.373779296875\n",
      "Epoch: 13450\tTrain loss: 2416.151611328125\n",
      "Epoch: 13500\tTrain loss: 2380.44482421875\n",
      "Epoch: 13550\tTrain loss: 2343.738525390625\n",
      "Epoch: 13600\tTrain loss: 2296.785888671875\n",
      "Epoch: 13650\tTrain loss: 2142.92041015625\n",
      "Epoch: 13700\tTrain loss: 1817.566650390625\n",
      "Epoch: 13750\tTrain loss: 1713.156494140625\n",
      "Epoch: 13800\tTrain loss: 1613.76416015625\n",
      "Epoch: 13850\tTrain loss: 1518.4998779296875\n",
      "Epoch: 13900\tTrain loss: 1427.794189453125\n",
      "Epoch: 13950\tTrain loss: 1344.8746337890625\n",
      "Epoch: 14000\tTrain loss: 1268.8304443359375\n",
      "Epoch: 14050\tTrain loss: 1201.9691162109375\n",
      "Epoch: 14100\tTrain loss: 1142.8416748046875\n",
      "Epoch: 14150\tTrain loss: 1079.50390625\n",
      "Epoch: 14200\tTrain loss: 1021.7169799804688\n",
      "Epoch: 14250\tTrain loss: 969.0651245117188\n",
      "Epoch: 14300\tTrain loss: 920.811279296875\n",
      "Epoch: 14350\tTrain loss: 876.4600830078125\n",
      "Epoch: 14400\tTrain loss: 835.6777954101562\n",
      "Epoch: 14450\tTrain loss: 798.2093505859375\n",
      "Epoch: 14500\tTrain loss: 763.8278198242188\n",
      "Epoch: 14550\tTrain loss: 732.3228759765625\n",
      "Epoch: 14600\tTrain loss: 703.89794921875\n",
      "Epoch: 14650\tTrain loss: 677.4046630859375\n",
      "Epoch: 14700\tTrain loss: 653.2173461914062\n",
      "Epoch: 14750\tTrain loss: 631.1693115234375\n",
      "Epoch: 14800\tTrain loss: 611.0690307617188\n",
      "Epoch: 14850\tTrain loss: 592.7400512695312\n",
      "Epoch: 14900\tTrain loss: 576.0177001953125\n",
      "Epoch: 14950\tTrain loss: 560.7423706054688\n",
      "Epoch: 15000\tTrain loss: 546.7592163085938\n",
      "Epoch: 15050\tTrain loss: 533.9258422851562\n",
      "Epoch: 15100\tTrain loss: 522.1266479492188\n",
      "Epoch: 15150\tTrain loss: 511.2926025390625\n",
      "Epoch: 15200\tTrain loss: 501.33551025390625\n",
      "Epoch: 15250\tTrain loss: 492.15594482421875\n",
      "Epoch: 15300\tTrain loss: 483.666259765625\n",
      "Epoch: 15350\tTrain loss: 475.78973388671875\n",
      "Epoch: 15400\tTrain loss: 468.4550476074219\n",
      "Epoch: 15450\tTrain loss: 461.5971984863281\n",
      "Epoch: 15500\tTrain loss: 455.1581726074219\n",
      "Epoch: 15550\tTrain loss: 449.0854187011719\n",
      "Epoch: 15600\tTrain loss: 443.32952880859375\n",
      "Epoch: 15650\tTrain loss: 437.8497009277344\n",
      "Epoch: 15700\tTrain loss: 432.6081237792969\n",
      "Epoch: 15750\tTrain loss: 427.5679931640625\n",
      "Epoch: 15800\tTrain loss: 422.7007751464844\n",
      "Epoch: 15850\tTrain loss: 485.72235107421875\n",
      "Epoch: 15900\tTrain loss: 957.6669921875\n",
      "Epoch: 15950\tTrain loss: 673.7373657226562\n",
      "Epoch: 16000\tTrain loss: 640.3135375976562\n",
      "Epoch: 16050\tTrain loss: 599.9365844726562\n",
      "Epoch: 16100\tTrain loss: 408.03460693359375\n",
      "Epoch: 16150\tTrain loss: 402.1059875488281\n",
      "Epoch: 16200\tTrain loss: 396.6780090332031\n",
      "Epoch: 16250\tTrain loss: 391.50360107421875\n",
      "Epoch: 16300\tTrain loss: 386.47735595703125\n",
      "Epoch: 16350\tTrain loss: 381.5550537109375\n",
      "Epoch: 16400\tTrain loss: 376.7159423828125\n",
      "Epoch: 16450\tTrain loss: 371.9439697265625\n",
      "Epoch: 16500\tTrain loss: 367.22381591796875\n",
      "Epoch: 16550\tTrain loss: 362.54119873046875\n",
      "Epoch: 16600\tTrain loss: 357.8833923339844\n",
      "Epoch: 16650\tTrain loss: 353.2419738769531\n",
      "Epoch: 16700\tTrain loss: 348.6082763671875\n",
      "Epoch: 16750\tTrain loss: 343.9767761230469\n",
      "Epoch: 16800\tTrain loss: 339.33984375\n",
      "Epoch: 16850\tTrain loss: 334.69219970703125\n",
      "Epoch: 16900\tTrain loss: 330.0279235839844\n",
      "Epoch: 16950\tTrain loss: 325.3393249511719\n",
      "Epoch: 17000\tTrain loss: 320.6204833984375\n",
      "Epoch: 17050\tTrain loss: 315.8621826171875\n",
      "Epoch: 17100\tTrain loss: 311.0533447265625\n",
      "Epoch: 17150\tTrain loss: 306.17852783203125\n",
      "Epoch: 17200\tTrain loss: 301.219482421875\n",
      "Epoch: 17250\tTrain loss: 296.1382141113281\n",
      "Epoch: 17300\tTrain loss: 290.8667297363281\n",
      "Epoch: 17350\tTrain loss: 285.248046875\n",
      "Epoch: 17400\tTrain loss: 279.0567321777344\n",
      "Epoch: 17450\tTrain loss: 272.58294677734375\n",
      "Epoch: 17500\tTrain loss: 266.1698913574219\n",
      "Epoch: 17550\tTrain loss: 260.3153076171875\n",
      "Epoch: 17600\tTrain loss: 256.7865905761719\n",
      "Epoch: 17650\tTrain loss: 247.6554412841797\n",
      "Epoch: 17700\tTrain loss: 241.61703491210938\n",
      "Epoch: 17750\tTrain loss: 235.82432556152344\n",
      "Epoch: 17800\tTrain loss: 231.65982055664062\n",
      "Epoch: 17850\tTrain loss: 224.21893310546875\n",
      "Epoch: 17900\tTrain loss: 218.67770385742188\n",
      "Epoch: 17950\tTrain loss: 213.58206176757812\n",
      "Epoch: 18000\tTrain loss: 207.8938446044922\n",
      "Epoch: 18050\tTrain loss: 202.2248992919922\n",
      "Epoch: 18100\tTrain loss: 197.01806640625\n",
      "Epoch: 18150\tTrain loss: 191.4844207763672\n",
      "Epoch: 18200\tTrain loss: 186.49612426757812\n",
      "Epoch: 18250\tTrain loss: 181.34024047851562\n",
      "Epoch: 18300\tTrain loss: 176.88807678222656\n",
      "Epoch: 18350\tTrain loss: 171.4540252685547\n",
      "Epoch: 18400\tTrain loss: 166.6365203857422\n",
      "Epoch: 18450\tTrain loss: 162.78208923339844\n",
      "Epoch: 18500\tTrain loss: 157.70071411132812\n",
      "Epoch: 18550\tTrain loss: 153.1621551513672\n",
      "Epoch: 18600\tTrain loss: 148.94244384765625\n",
      "Epoch: 18650\tTrain loss: 144.99392700195312\n",
      "Epoch: 18700\tTrain loss: 141.20501708984375\n",
      "Epoch: 18750\tTrain loss: 137.06979370117188\n",
      "Epoch: 18800\tTrain loss: 133.57461547851562\n",
      "Epoch: 18850\tTrain loss: 132.35028076171875\n",
      "Epoch: 18900\tTrain loss: 126.97932434082031\n",
      "Epoch: 18950\tTrain loss: 122.7944107055664\n",
      "Epoch: 19000\tTrain loss: 119.14639282226562\n",
      "Epoch: 19050\tTrain loss: 115.6954574584961\n",
      "Epoch: 19100\tTrain loss: 112.37010192871094\n",
      "Epoch: 19150\tTrain loss: 109.1449203491211\n",
      "Epoch: 19200\tTrain loss: 106.00718688964844\n",
      "Epoch: 19250\tTrain loss: 103.26252746582031\n",
      "Epoch: 19300\tTrain loss: 99.97746276855469\n",
      "Epoch: 19350\tTrain loss: 97.07024383544922\n",
      "Epoch: 19400\tTrain loss: 94.2385025024414\n",
      "Epoch: 19450\tTrain loss: 101.97041320800781\n",
      "Epoch: 19500\tTrain loss: 90.70336151123047\n",
      "Epoch: 19550\tTrain loss: 87.3308334350586\n",
      "Epoch: 19600\tTrain loss: 84.43521881103516\n",
      "Epoch: 19650\tTrain loss: 81.69915008544922\n",
      "Epoch: 19700\tTrain loss: 78.96310424804688\n",
      "Epoch: 19750\tTrain loss: 83.00138854980469\n",
      "Epoch: 19800\tTrain loss: 74.96633911132812\n",
      "Epoch: 19850\tTrain loss: 72.45885467529297\n",
      "Epoch: 19900\tTrain loss: 70.1016845703125\n",
      "Epoch: 19950\tTrain loss: 67.83818054199219\n",
      "Epoch: 20000\tTrain loss: 65.65263366699219\n",
      "Epoch: 20050\tTrain loss: 63.53472137451172\n",
      "Epoch: 20100\tTrain loss: 61.49057388305664\n",
      "Epoch: 20150\tTrain loss: 59.49282455444336\n",
      "Epoch: 20200\tTrain loss: 57.55384063720703\n",
      "Epoch: 20250\tTrain loss: 55.66769027709961\n",
      "Epoch: 20300\tTrain loss: 53.83253860473633\n",
      "Epoch: 20350\tTrain loss: 52.04609680175781\n",
      "Epoch: 20400\tTrain loss: 50.307220458984375\n",
      "Epoch: 20450\tTrain loss: 48.61383819580078\n",
      "Epoch: 20500\tTrain loss: 46.965274810791016\n",
      "Epoch: 20550\tTrain loss: 45.35996627807617\n",
      "Epoch: 20600\tTrain loss: 43.7972412109375\n",
      "Epoch: 20650\tTrain loss: 42.27583694458008\n",
      "Epoch: 20700\tTrain loss: 40.79481887817383\n",
      "Epoch: 20750\tTrain loss: 39.353553771972656\n",
      "Epoch: 20800\tTrain loss: 37.95109558105469\n",
      "Epoch: 20850\tTrain loss: 36.58658218383789\n",
      "Epoch: 20900\tTrain loss: 35.259315490722656\n",
      "Epoch: 20950\tTrain loss: 33.968650817871094\n",
      "Epoch: 21000\tTrain loss: 32.71390914916992\n",
      "Epoch: 21050\tTrain loss: 31.494537353515625\n",
      "Epoch: 21100\tTrain loss: 30.309791564941406\n",
      "Epoch: 21150\tTrain loss: 29.158599853515625\n",
      "Epoch: 21200\tTrain loss: 28.041126251220703\n",
      "Epoch: 21250\tTrain loss: 26.956249237060547\n",
      "Epoch: 21300\tTrain loss: 25.903587341308594\n",
      "Epoch: 21350\tTrain loss: 24.882551193237305\n",
      "Epoch: 21400\tTrain loss: 23.892431259155273\n",
      "Epoch: 21450\tTrain loss: 22.932588577270508\n",
      "Epoch: 21500\tTrain loss: 22.002857208251953\n",
      "Epoch: 21550\tTrain loss: 21.102012634277344\n",
      "Epoch: 21600\tTrain loss: 20.230134963989258\n",
      "Epoch: 21650\tTrain loss: 19.386594772338867\n",
      "Epoch: 21700\tTrain loss: 18.570478439331055\n",
      "Epoch: 21750\tTrain loss: 17.781333923339844\n",
      "Epoch: 21800\tTrain loss: 17.018938064575195\n",
      "Epoch: 21850\tTrain loss: 16.282390594482422\n",
      "Epoch: 21900\tTrain loss: 15.57097339630127\n",
      "Epoch: 21950\tTrain loss: 14.884627342224121\n",
      "Epoch: 22000\tTrain loss: 14.395627975463867\n",
      "Epoch: 22050\tTrain loss: 13.5836763381958\n",
      "Epoch: 22100\tTrain loss: 12.96834659576416\n",
      "Epoch: 22150\tTrain loss: 12.375542640686035\n",
      "Epoch: 22200\tTrain loss: 12.112091064453125\n",
      "Epoch: 22250\tTrain loss: 11.25624942779541\n",
      "Epoch: 22300\tTrain loss: 10.7269287109375\n",
      "Epoch: 22350\tTrain loss: 10.21880054473877\n",
      "Epoch: 22400\tTrain loss: 9.730582237243652\n",
      "Epoch: 22450\tTrain loss: 9.278812408447266\n",
      "Epoch: 22500\tTrain loss: 8.811483383178711\n",
      "Epoch: 22550\tTrain loss: 8.379467964172363\n",
      "Epoch: 22600\tTrain loss: 7.965081214904785\n",
      "Epoch: 22650\tTrain loss: 7.604235649108887\n",
      "Epoch: 22700\tTrain loss: 7.187330722808838\n",
      "Epoch: 22750\tTrain loss: 6.822935581207275\n",
      "Epoch: 22800\tTrain loss: 6.47420597076416\n",
      "Epoch: 22850\tTrain loss: 6.160613536834717\n",
      "Epoch: 22900\tTrain loss: 5.82135009765625\n",
      "Epoch: 22950\tTrain loss: 5.516329288482666\n",
      "Epoch: 23000\tTrain loss: 5.650578022003174\n",
      "Epoch: 23050\tTrain loss: 4.947261810302734\n",
      "Epoch: 23100\tTrain loss: 4.681347370147705\n",
      "Epoch: 23150\tTrain loss: 4.4279890060424805\n",
      "Epoch: 23200\tTrain loss: 4.196564197540283\n",
      "Epoch: 23250\tTrain loss: 3.9639246463775635\n",
      "Epoch: 23300\tTrain loss: 3.73720383644104\n",
      "Epoch: 23350\tTrain loss: 3.5284221172332764\n",
      "Epoch: 23400\tTrain loss: 3.3651018142700195\n",
      "Epoch: 23450\tTrain loss: 3.1424503326416016\n",
      "Epoch: 23500\tTrain loss: 2.9610273838043213\n",
      "Epoch: 23550\tTrain loss: 2.790285348892212\n",
      "Epoch: 23600\tTrain loss: 2.6640360355377197\n",
      "Epoch: 23650\tTrain loss: 2.4737958908081055\n",
      "Epoch: 23700\tTrain loss: 2.3275346755981445\n",
      "Epoch: 23750\tTrain loss: 2.1909754276275635\n",
      "Epoch: 23800\tTrain loss: 2.0631532669067383\n",
      "Epoch: 23850\tTrain loss: 1.9322688579559326\n",
      "Epoch: 23900\tTrain loss: 1.8139078617095947\n",
      "Epoch: 23950\tTrain loss: 1.7750177383422852\n",
      "Epoch: 24000\tTrain loss: 1.596204400062561\n",
      "Epoch: 24050\tTrain loss: 1.4953322410583496\n",
      "Epoch: 24100\tTrain loss: 1.4003376960754395\n",
      "Epoch: 24150\tTrain loss: 1.33470618724823\n",
      "Epoch: 24200\tTrain loss: 1.2255632877349854\n",
      "Epoch: 24250\tTrain loss: 1.1452598571777344\n",
      "Epoch: 24300\tTrain loss: 1.4165019989013672\n",
      "Epoch: 24350\tTrain loss: 0.9984208345413208\n",
      "Epoch: 24400\tTrain loss: 0.9304503202438354\n",
      "Epoch: 24450\tTrain loss: 0.866790235042572\n",
      "Epoch: 24500\tTrain loss: 0.9405264258384705\n",
      "Epoch: 24550\tTrain loss: 0.7508450150489807\n",
      "Epoch: 24600\tTrain loss: 0.6971534490585327\n",
      "Epoch: 24650\tTrain loss: 0.6471126079559326\n",
      "Epoch: 24700\tTrain loss: 0.6206821799278259\n",
      "Epoch: 24750\tTrain loss: 0.5559457540512085\n",
      "Epoch: 24800\tTrain loss: 0.5144363045692444\n",
      "Epoch: 24850\tTrain loss: 0.4755452871322632\n",
      "Epoch: 24900\tTrain loss: 0.4394621253013611\n",
      "Epoch: 24950\tTrain loss: 0.4126691520214081\n",
      "Training CFRNN\n",
      "Epoch: 0\tTrain loss: 1542153.875\n",
      "Epoch: 50\tTrain loss: 1517436.625\n",
      "Epoch: 100\tTrain loss: 1490262.25\n",
      "Epoch: 150\tTrain loss: 1465065.125\n",
      "Epoch: 200\tTrain loss: 1440792.375\n",
      "Epoch: 250\tTrain loss: 1417141.0\n",
      "Epoch: 300\tTrain loss: 1393987.0\n",
      "Epoch: 350\tTrain loss: 1371265.125\n",
      "Epoch: 400\tTrain loss: 1348936.75\n",
      "Epoch: 450\tTrain loss: 1326975.125\n",
      "Epoch: 500\tTrain loss: 1305362.125\n",
      "Epoch: 550\tTrain loss: 1284083.375\n",
      "Epoch: 600\tTrain loss: 1263127.375\n",
      "Epoch: 650\tTrain loss: 1242484.875\n",
      "Epoch: 700\tTrain loss: 1222148.0\n",
      "Epoch: 750\tTrain loss: 1202109.875\n",
      "Epoch: 800\tTrain loss: 1182364.375\n",
      "Epoch: 850\tTrain loss: 1162905.625\n",
      "Epoch: 900\tTrain loss: 1143729.0\n",
      "Epoch: 950\tTrain loss: 1124829.5\n",
      "Epoch: 1000\tTrain loss: 1106202.875\n",
      "Epoch: 1050\tTrain loss: 1087845.0\n",
      "Epoch: 1100\tTrain loss: 1069752.125\n",
      "Epoch: 1150\tTrain loss: 1051920.125\n",
      "Epoch: 1200\tTrain loss: 1034345.9375\n",
      "Epoch: 1250\tTrain loss: 1017025.9375\n",
      "Epoch: 1300\tTrain loss: 999957.0\n",
      "Epoch: 1350\tTrain loss: 983135.875\n",
      "Epoch: 1400\tTrain loss: 966559.6875\n",
      "Epoch: 1450\tTrain loss: 950225.5\n",
      "Epoch: 1500\tTrain loss: 934130.375\n",
      "Epoch: 1550\tTrain loss: 918271.5625\n",
      "Epoch: 1600\tTrain loss: 902646.5\n",
      "Epoch: 1650\tTrain loss: 887252.5625\n",
      "Epoch: 1700\tTrain loss: 872087.3125\n",
      "Epoch: 1750\tTrain loss: 857148.1875\n",
      "Epoch: 1800\tTrain loss: 842432.875\n",
      "Epoch: 1850\tTrain loss: 827938.875\n",
      "Epoch: 1900\tTrain loss: 813664.0625\n",
      "Epoch: 1950\tTrain loss: 799606.1875\n",
      "Epoch: 2000\tTrain loss: 785763.1875\n",
      "Epoch: 2050\tTrain loss: 772132.6875\n",
      "Epoch: 2100\tTrain loss: 758712.75\n",
      "Epoch: 2150\tTrain loss: 745501.25\n",
      "Epoch: 2200\tTrain loss: 732496.375\n",
      "Epoch: 2250\tTrain loss: 719696.0\n",
      "Epoch: 2300\tTrain loss: 707098.1875\n",
      "Epoch: 2350\tTrain loss: 694701.0625\n",
      "Epoch: 2400\tTrain loss: 682502.9375\n",
      "Epoch: 2450\tTrain loss: 670501.875\n",
      "Epoch: 2500\tTrain loss: 658696.0\n",
      "Epoch: 2550\tTrain loss: 647083.75\n",
      "Epoch: 2600\tTrain loss: 635663.125\n",
      "Epoch: 2650\tTrain loss: 624432.5625\n",
      "Epoch: 2700\tTrain loss: 613390.625\n",
      "Epoch: 2750\tTrain loss: 602535.3125\n",
      "Epoch: 2800\tTrain loss: 591865.0\n",
      "Epoch: 2850\tTrain loss: 581378.375\n",
      "Epoch: 2900\tTrain loss: 571073.5625\n",
      "Epoch: 2950\tTrain loss: 560949.125\n",
      "Epoch: 3000\tTrain loss: 551003.5\n",
      "Epoch: 3050\tTrain loss: 541235.0625\n",
      "Epoch: 3100\tTrain loss: 531642.5\n",
      "Epoch: 3150\tTrain loss: 522224.0625\n",
      "Epoch: 3200\tTrain loss: 512978.40625\n",
      "Epoch: 3250\tTrain loss: 503904.09375\n",
      "Epoch: 3300\tTrain loss: 494999.5625\n",
      "Epoch: 3350\tTrain loss: 486263.46875\n",
      "Epoch: 3400\tTrain loss: 477694.1875\n",
      "Epoch: 3450\tTrain loss: 469290.40625\n",
      "Epoch: 3500\tTrain loss: 461050.71875\n",
      "Epoch: 3550\tTrain loss: 452973.65625\n",
      "Epoch: 3600\tTrain loss: 445057.78125\n",
      "Epoch: 3650\tTrain loss: 437301.75\n",
      "Epoch: 3700\tTrain loss: 429704.1875\n",
      "Epoch: 3750\tTrain loss: 422263.5625\n",
      "Epoch: 3800\tTrain loss: 414978.625\n",
      "Epoch: 3850\tTrain loss: 407847.78125\n",
      "Epoch: 3900\tTrain loss: 400869.8125\n",
      "Epoch: 3950\tTrain loss: 394043.21875\n",
      "Epoch: 4000\tTrain loss: 387366.6875\n",
      "Epoch: 4050\tTrain loss: 380838.71875\n",
      "Epoch: 4100\tTrain loss: 374457.96875\n",
      "Epoch: 4150\tTrain loss: 368223.03125\n",
      "Epoch: 4200\tTrain loss: 362132.46875\n",
      "Epoch: 4250\tTrain loss: 356184.8125\n",
      "Epoch: 4300\tTrain loss: 350378.78125\n",
      "Epoch: 4350\tTrain loss: 344712.8125\n",
      "Epoch: 4400\tTrain loss: 339185.5\n",
      "Epoch: 4450\tTrain loss: 333795.40625\n",
      "Epoch: 4500\tTrain loss: 328541.0625\n",
      "Epoch: 4550\tTrain loss: 323421.0\n",
      "Epoch: 4600\tTrain loss: 318433.75\n",
      "Epoch: 4650\tTrain loss: 313577.875\n",
      "Epoch: 4700\tTrain loss: 308851.78125\n",
      "Epoch: 4750\tTrain loss: 304254.0\n",
      "Epoch: 4800\tTrain loss: 299782.96875\n",
      "Epoch: 4850\tTrain loss: 295437.21875\n",
      "Epoch: 4900\tTrain loss: 291215.15625\n",
      "Epoch: 4950\tTrain loss: 287115.1875\n",
      "Epoch: 5000\tTrain loss: 283135.6875\n",
      "Epoch: 5050\tTrain loss: 279275.1875\n",
      "Epoch: 5100\tTrain loss: 275532.0\n",
      "Epoch: 5150\tTrain loss: 271904.4375\n",
      "Epoch: 5200\tTrain loss: 268390.90625\n",
      "Epoch: 5250\tTrain loss: 264989.71875\n",
      "Epoch: 5300\tTrain loss: 261699.234375\n",
      "Epoch: 5350\tTrain loss: 258517.6875\n",
      "Epoch: 5400\tTrain loss: 255443.328125\n",
      "Epoch: 5450\tTrain loss: 252474.5\n",
      "Epoch: 5500\tTrain loss: 249609.390625\n",
      "Epoch: 5550\tTrain loss: 246846.265625\n",
      "Epoch: 5600\tTrain loss: 244183.25\n",
      "Epoch: 5650\tTrain loss: 241618.59375\n",
      "Epoch: 5700\tTrain loss: 239150.421875\n",
      "Epoch: 5750\tTrain loss: 236776.90625\n",
      "Epoch: 5800\tTrain loss: 234496.15625\n",
      "Epoch: 5850\tTrain loss: 232306.265625\n",
      "Epoch: 5900\tTrain loss: 230205.390625\n",
      "Epoch: 5950\tTrain loss: 228191.546875\n",
      "Epoch: 6000\tTrain loss: 226262.796875\n",
      "Epoch: 6050\tTrain loss: 224417.25\n",
      "Epoch: 6100\tTrain loss: 222652.859375\n",
      "Epoch: 6150\tTrain loss: 220967.6875\n",
      "Epoch: 6200\tTrain loss: 219359.71875\n",
      "Epoch: 6250\tTrain loss: 217826.9375\n",
      "Epoch: 6300\tTrain loss: 216367.375\n",
      "Epoch: 6350\tTrain loss: 214978.953125\n",
      "Epoch: 6400\tTrain loss: 213659.65625\n",
      "Epoch: 6450\tTrain loss: 212407.4375\n",
      "Epoch: 6500\tTrain loss: 211220.234375\n",
      "Epoch: 6550\tTrain loss: 202803.5625\n",
      "Epoch: 6600\tTrain loss: 199603.34375\n",
      "Epoch: 6650\tTrain loss: 197859.921875\n",
      "Epoch: 6700\tTrain loss: 172215.890625\n",
      "Epoch: 6750\tTrain loss: 164723.375\n",
      "Epoch: 6800\tTrain loss: 161010.234375\n",
      "Epoch: 6850\tTrain loss: 157475.84375\n",
      "Epoch: 6900\tTrain loss: 154061.28125\n",
      "Epoch: 6950\tTrain loss: 150761.671875\n",
      "Epoch: 7000\tTrain loss: 147572.953125\n",
      "Epoch: 7050\tTrain loss: 144490.140625\n",
      "Epoch: 7100\tTrain loss: 141508.75\n",
      "Epoch: 7150\tTrain loss: 138626.96875\n",
      "Epoch: 7200\tTrain loss: 135842.6875\n",
      "Epoch: 7250\tTrain loss: 133153.03125\n",
      "Epoch: 7300\tTrain loss: 130555.15625\n",
      "Epoch: 7350\tTrain loss: 128046.1171875\n",
      "Epoch: 7400\tTrain loss: 125623.2265625\n",
      "Epoch: 7450\tTrain loss: 123283.84375\n",
      "Epoch: 7500\tTrain loss: 121025.1484375\n",
      "Epoch: 7550\tTrain loss: 118845.1875\n",
      "Epoch: 7600\tTrain loss: 116742.15625\n",
      "Epoch: 7650\tTrain loss: 114722.5859375\n",
      "Epoch: 7700\tTrain loss: 112756.71875\n",
      "Epoch: 7750\tTrain loss: 110868.421875\n",
      "Epoch: 7800\tTrain loss: 109045.2421875\n",
      "Epoch: 7850\tTrain loss: 107167.3828125\n",
      "Epoch: 7900\tTrain loss: 105266.5\n",
      "Epoch: 7950\tTrain loss: 103407.421875\n",
      "Epoch: 8000\tTrain loss: 101557.75\n",
      "Epoch: 8050\tTrain loss: 99716.2890625\n",
      "Epoch: 8100\tTrain loss: 97883.59375\n",
      "Epoch: 8150\tTrain loss: 96061.1171875\n",
      "Epoch: 8200\tTrain loss: 94250.109375\n",
      "Epoch: 8250\tTrain loss: 92451.46875\n",
      "Epoch: 8300\tTrain loss: 90775.0078125\n",
      "Epoch: 8350\tTrain loss: 89839.1484375\n",
      "Epoch: 8400\tTrain loss: 87188.078125\n",
      "Epoch: 8450\tTrain loss: 85432.5078125\n",
      "Epoch: 8500\tTrain loss: 83696.6484375\n",
      "Epoch: 8550\tTrain loss: 81976.9609375\n",
      "Epoch: 8600\tTrain loss: 80272.71875\n",
      "Epoch: 8650\tTrain loss: 78583.1875\n",
      "Epoch: 8700\tTrain loss: 76908.1171875\n",
      "Epoch: 8750\tTrain loss: 75248.8671875\n",
      "Epoch: 8800\tTrain loss: 73605.953125\n",
      "Epoch: 8850\tTrain loss: 71979.7421875\n",
      "Epoch: 8900\tTrain loss: 70370.5234375\n",
      "Epoch: 8950\tTrain loss: 68778.6875\n",
      "Epoch: 9000\tTrain loss: 67204.703125\n",
      "Epoch: 9050\tTrain loss: 65649.3515625\n",
      "Epoch: 9100\tTrain loss: 64113.38671875\n",
      "Epoch: 9150\tTrain loss: 62595.7265625\n",
      "Epoch: 9200\tTrain loss: 61091.33984375\n",
      "Epoch: 9250\tTrain loss: 93047.5703125\n",
      "Epoch: 9300\tTrain loss: 58036.33203125\n",
      "Epoch: 9350\tTrain loss: 56582.765625\n",
      "Epoch: 9400\tTrain loss: 55166.67578125\n",
      "Epoch: 9450\tTrain loss: 53771.02734375\n",
      "Epoch: 9500\tTrain loss: 52394.515625\n",
      "Epoch: 9550\tTrain loss: 51037.875\n",
      "Epoch: 9600\tTrain loss: 49704.92578125\n",
      "Epoch: 9650\tTrain loss: 48397.01953125\n",
      "Epoch: 9700\tTrain loss: 47108.78515625\n",
      "Epoch: 9750\tTrain loss: 45827.1015625\n",
      "Epoch: 9800\tTrain loss: 44569.39453125\n",
      "Epoch: 9850\tTrain loss: 43330.71875\n",
      "Epoch: 9900\tTrain loss: 42113.51171875\n",
      "Epoch: 9950\tTrain loss: 40916.734375\n",
      "Epoch: 10000\tTrain loss: 39740.91796875\n",
      "Epoch: 10050\tTrain loss: 38583.55859375\n",
      "Epoch: 10100\tTrain loss: 37446.46484375\n",
      "Epoch: 10150\tTrain loss: 36329.265625\n",
      "Epoch: 10200\tTrain loss: 35246.82421875\n",
      "Epoch: 10250\tTrain loss: 34156.15234375\n",
      "Epoch: 10300\tTrain loss: 33099.4609375\n",
      "Epoch: 10350\tTrain loss: 32063.0390625\n",
      "Epoch: 10400\tTrain loss: 31046.9296875\n",
      "Epoch: 10450\tTrain loss: 30051.22265625\n",
      "Epoch: 10500\tTrain loss: 29076.046875\n",
      "Epoch: 10550\tTrain loss: 28121.623046875\n",
      "Epoch: 10600\tTrain loss: 27188.119140625\n",
      "Epoch: 10650\tTrain loss: 26277.712890625\n",
      "Epoch: 10700\tTrain loss: 25382.455078125\n",
      "Epoch: 10750\tTrain loss: 24507.142578125\n",
      "Epoch: 10800\tTrain loss: 23691.92578125\n",
      "Epoch: 10850\tTrain loss: 22819.9375\n",
      "Epoch: 10900\tTrain loss: 22000.4609375\n",
      "Epoch: 10950\tTrain loss: 21203.306640625\n",
      "Epoch: 11000\tTrain loss: 20423.642578125\n",
      "Epoch: 11050\tTrain loss: 19662.306640625\n",
      "Epoch: 11100\tTrain loss: 18919.84765625\n",
      "Epoch: 11150\tTrain loss: 18196.298828125\n",
      "Epoch: 11200\tTrain loss: 17491.560546875\n",
      "Epoch: 11250\tTrain loss: 16805.47265625\n",
      "Epoch: 11300\tTrain loss: 16137.88671875\n",
      "Epoch: 11350\tTrain loss: 15489.0986328125\n",
      "Epoch: 11400\tTrain loss: 14857.73828125\n",
      "Epoch: 11450\tTrain loss: 14245.1533203125\n",
      "Epoch: 11500\tTrain loss: 13650.3740234375\n",
      "Epoch: 11550\tTrain loss: 13072.90625\n",
      "Epoch: 11600\tTrain loss: 12512.927734375\n",
      "Epoch: 11650\tTrain loss: 11969.455078125\n",
      "Epoch: 11700\tTrain loss: 11442.1513671875\n",
      "Epoch: 11750\tTrain loss: 10931.091796875\n",
      "Epoch: 11800\tTrain loss: 10436.259765625\n",
      "Epoch: 11850\tTrain loss: 9957.6259765625\n",
      "Epoch: 11900\tTrain loss: 9494.765625\n",
      "Epoch: 11950\tTrain loss: 9047.6904296875\n",
      "Epoch: 12000\tTrain loss: 8616.1015625\n",
      "Epoch: 12050\tTrain loss: 8199.7529296875\n",
      "Epoch: 12100\tTrain loss: 7798.85595703125\n",
      "Epoch: 12150\tTrain loss: 7411.830078125\n",
      "Epoch: 12200\tTrain loss: 7039.6748046875\n",
      "Epoch: 12250\tTrain loss: 6681.7294921875\n",
      "Epoch: 12300\tTrain loss: 6337.72314453125\n",
      "Epoch: 12350\tTrain loss: 6007.38037109375\n",
      "Epoch: 12400\tTrain loss: 5690.4375\n",
      "Epoch: 12450\tTrain loss: 5386.6142578125\n",
      "Epoch: 12500\tTrain loss: 5095.64208984375\n",
      "Epoch: 12550\tTrain loss: 4817.54736328125\n",
      "Epoch: 12600\tTrain loss: 4551.12353515625\n",
      "Epoch: 12650\tTrain loss: 4296.91064453125\n",
      "Epoch: 12700\tTrain loss: 4054.3203125\n",
      "Epoch: 12750\tTrain loss: 3823.024169921875\n",
      "Epoch: 12800\tTrain loss: 3602.700927734375\n",
      "Epoch: 12850\tTrain loss: 3393.05517578125\n",
      "Epoch: 12900\tTrain loss: 3193.8134765625\n",
      "Epoch: 12950\tTrain loss: 3004.732666015625\n",
      "Epoch: 13000\tTrain loss: 2825.355712890625\n",
      "Epoch: 13050\tTrain loss: 2655.4716796875\n",
      "Epoch: 13100\tTrain loss: 2494.75830078125\n",
      "Epoch: 13150\tTrain loss: 2342.9111328125\n",
      "Epoch: 13200\tTrain loss: 2199.62158203125\n",
      "Epoch: 13250\tTrain loss: 2064.58203125\n",
      "Epoch: 13300\tTrain loss: 1937.4832763671875\n",
      "Epoch: 13350\tTrain loss: 1818.0152587890625\n",
      "Epoch: 13400\tTrain loss: 1705.8741455078125\n",
      "Epoch: 13450\tTrain loss: 1600.824951171875\n",
      "Epoch: 13500\tTrain loss: 1502.36572265625\n",
      "Epoch: 13550\tTrain loss: 1410.393798828125\n",
      "Epoch: 13600\tTrain loss: 1324.5467529296875\n",
      "Epoch: 13650\tTrain loss: 1244.5352783203125\n",
      "Epoch: 13700\tTrain loss: 1170.0682373046875\n",
      "Epoch: 13750\tTrain loss: 1100.8646240234375\n",
      "Epoch: 13800\tTrain loss: 1036.646484375\n",
      "Epoch: 13850\tTrain loss: 977.1397705078125\n",
      "Epoch: 13900\tTrain loss: 922.0814819335938\n",
      "Epoch: 13950\tTrain loss: 871.2122802734375\n",
      "Epoch: 14000\tTrain loss: 824.2801513671875\n",
      "Epoch: 14050\tTrain loss: 781.0409545898438\n",
      "Epoch: 14100\tTrain loss: 741.2607421875\n",
      "Epoch: 14150\tTrain loss: 704.7091064453125\n",
      "Epoch: 14200\tTrain loss: 671.1679077148438\n",
      "Epoch: 14250\tTrain loss: 640.423583984375\n",
      "Epoch: 14300\tTrain loss: 612.271728515625\n",
      "Epoch: 14350\tTrain loss: 586.51708984375\n",
      "Epoch: 14400\tTrain loss: 562.9685668945312\n",
      "Epoch: 14450\tTrain loss: 541.4429931640625\n",
      "Epoch: 14500\tTrain loss: 521.7603149414062\n",
      "Epoch: 14550\tTrain loss: 503.74853515625\n",
      "Epoch: 14600\tTrain loss: 487.3255615234375\n",
      "Epoch: 14650\tTrain loss: 472.04803466796875\n",
      "Epoch: 14700\tTrain loss: 458.0849609375\n",
      "Epoch: 14750\tTrain loss: 445.0879211425781\n",
      "Epoch: 14800\tTrain loss: 433.2395935058594\n",
      "Epoch: 14850\tTrain loss: 422.32208251953125\n",
      "Epoch: 14900\tTrain loss: 412.2154235839844\n",
      "Epoch: 14950\tTrain loss: 402.9316101074219\n",
      "Epoch: 15000\tTrain loss: 394.43438720703125\n",
      "Epoch: 15050\tTrain loss: 386.4659729003906\n",
      "Epoch: 15100\tTrain loss: 379.1033935546875\n",
      "Epoch: 15150\tTrain loss: 372.2179260253906\n",
      "Epoch: 15200\tTrain loss: 365.7271728515625\n",
      "Epoch: 15250\tTrain loss: 359.5771179199219\n",
      "Epoch: 15300\tTrain loss: 353.735107421875\n",
      "Epoch: 15350\tTrain loss: 348.10675048828125\n",
      "Epoch: 15400\tTrain loss: 342.78863525390625\n",
      "Epoch: 15450\tTrain loss: 337.73284912109375\n",
      "Epoch: 15500\tTrain loss: 332.90277099609375\n",
      "Epoch: 15550\tTrain loss: 328.27764892578125\n",
      "Epoch: 15600\tTrain loss: 323.8323974609375\n",
      "Epoch: 15650\tTrain loss: 319.5398254394531\n",
      "Epoch: 15700\tTrain loss: 315.3689880371094\n",
      "Epoch: 15750\tTrain loss: 311.2714538574219\n",
      "Epoch: 15800\tTrain loss: 307.1724853515625\n",
      "Epoch: 15850\tTrain loss: 302.5679626464844\n",
      "Epoch: 15900\tTrain loss: 296.66290283203125\n",
      "Epoch: 15950\tTrain loss: 290.866943359375\n",
      "Epoch: 16000\tTrain loss: 285.80877685546875\n",
      "Epoch: 16050\tTrain loss: 281.2364196777344\n",
      "Epoch: 16100\tTrain loss: 277.06390380859375\n",
      "Epoch: 16150\tTrain loss: 273.45867919921875\n",
      "Epoch: 16200\tTrain loss: 272.1557312011719\n",
      "Epoch: 16250\tTrain loss: 265.44757080078125\n",
      "Epoch: 16300\tTrain loss: 4851.3642578125\n",
      "Epoch: 16350\tTrain loss: 434.5566101074219\n",
      "Epoch: 16400\tTrain loss: 388.759521484375\n",
      "Epoch: 16450\tTrain loss: 383.272705078125\n",
      "Epoch: 16500\tTrain loss: 378.9539489746094\n",
      "Epoch: 16550\tTrain loss: 374.99053955078125\n",
      "Epoch: 16600\tTrain loss: 371.22705078125\n",
      "Epoch: 16650\tTrain loss: 367.6051940917969\n",
      "Epoch: 16700\tTrain loss: 364.0979309082031\n",
      "Epoch: 16750\tTrain loss: 360.69085693359375\n",
      "Epoch: 16800\tTrain loss: 357.37567138671875\n",
      "Epoch: 16850\tTrain loss: 354.1510009765625\n",
      "Epoch: 16900\tTrain loss: 351.037841796875\n",
      "Epoch: 16950\tTrain loss: 348.0658264160156\n",
      "Epoch: 17000\tTrain loss: 345.1469421386719\n",
      "Epoch: 17050\tTrain loss: 342.2731628417969\n",
      "Epoch: 17100\tTrain loss: 339.4402160644531\n",
      "Epoch: 17150\tTrain loss: 336.64251708984375\n",
      "Epoch: 17200\tTrain loss: 333.8761901855469\n",
      "Epoch: 17250\tTrain loss: 331.1344909667969\n",
      "Epoch: 17300\tTrain loss: 328.4077453613281\n",
      "Epoch: 17350\tTrain loss: 325.6797790527344\n",
      "Epoch: 17400\tTrain loss: 322.9193115234375\n",
      "Epoch: 17450\tTrain loss: 320.0716552734375\n",
      "Epoch: 17500\tTrain loss: 317.39520263671875\n",
      "Epoch: 17550\tTrain loss: 314.27374267578125\n",
      "Epoch: 17600\tTrain loss: 311.3648986816406\n",
      "Epoch: 17650\tTrain loss: 308.5096435546875\n",
      "Epoch: 17700\tTrain loss: 305.7315368652344\n",
      "Epoch: 17750\tTrain loss: 303.01239013671875\n",
      "Epoch: 17800\tTrain loss: 300.32684326171875\n",
      "Epoch: 17850\tTrain loss: 297.8847351074219\n",
      "Epoch: 17900\tTrain loss: 294.9755859375\n",
      "Epoch: 17950\tTrain loss: 292.33868408203125\n",
      "Epoch: 18000\tTrain loss: 289.7128601074219\n",
      "Epoch: 18050\tTrain loss: 287.0677795410156\n",
      "Epoch: 18100\tTrain loss: 284.3870544433594\n",
      "Epoch: 18150\tTrain loss: 281.6527099609375\n",
      "Epoch: 18200\tTrain loss: 278.8425598144531\n",
      "Epoch: 18250\tTrain loss: 275.9359130859375\n",
      "Epoch: 18300\tTrain loss: 272.9146728515625\n",
      "Epoch: 18350\tTrain loss: 269.7674865722656\n",
      "Epoch: 18400\tTrain loss: 266.4925537109375\n",
      "Epoch: 18450\tTrain loss: 263.0887145996094\n",
      "Epoch: 18500\tTrain loss: 259.5056457519531\n",
      "Epoch: 18550\tTrain loss: 255.57614135742188\n",
      "Epoch: 18600\tTrain loss: 251.40208435058594\n",
      "Epoch: 18650\tTrain loss: 247.50990295410156\n",
      "Epoch: 18700\tTrain loss: 243.9312286376953\n",
      "Epoch: 18750\tTrain loss: 240.52963256835938\n",
      "Epoch: 18800\tTrain loss: 237.21163940429688\n",
      "Epoch: 18850\tTrain loss: 233.8995361328125\n",
      "Epoch: 18900\tTrain loss: 230.47642517089844\n",
      "Epoch: 18950\tTrain loss: 226.53077697753906\n",
      "Epoch: 19000\tTrain loss: 214.8555450439453\n",
      "Epoch: 19050\tTrain loss: 305.2576904296875\n",
      "Epoch: 19100\tTrain loss: 240.74465942382812\n",
      "Epoch: 19150\tTrain loss: 237.10877990722656\n",
      "Epoch: 19200\tTrain loss: 226.73458862304688\n",
      "Epoch: 19250\tTrain loss: 217.50062561035156\n",
      "Epoch: 19300\tTrain loss: 214.3499298095703\n",
      "Epoch: 19350\tTrain loss: 211.59878540039062\n",
      "Epoch: 19400\tTrain loss: 208.9272918701172\n",
      "Epoch: 19450\tTrain loss: 206.2821807861328\n",
      "Epoch: 19500\tTrain loss: 203.62979125976562\n",
      "Epoch: 19550\tTrain loss: 200.93045043945312\n",
      "Epoch: 19600\tTrain loss: 198.10263061523438\n",
      "Epoch: 19650\tTrain loss: 194.92437744140625\n",
      "Epoch: 19700\tTrain loss: 191.06961059570312\n",
      "Epoch: 19750\tTrain loss: 187.68333435058594\n",
      "Epoch: 19800\tTrain loss: 184.57846069335938\n",
      "Epoch: 19850\tTrain loss: 180.9830322265625\n",
      "Epoch: 19900\tTrain loss: 174.51512145996094\n",
      "Epoch: 19950\tTrain loss: 203.33079528808594\n",
      "Epoch: 20000\tTrain loss: 161.48312377929688\n",
      "Epoch: 20050\tTrain loss: 157.24368286132812\n",
      "Epoch: 20100\tTrain loss: 154.4205780029297\n",
      "Epoch: 20150\tTrain loss: 152.0049591064453\n",
      "Epoch: 20200\tTrain loss: 149.8383026123047\n",
      "Epoch: 20250\tTrain loss: 147.84922790527344\n",
      "Epoch: 20300\tTrain loss: 145.9812774658203\n",
      "Epoch: 20350\tTrain loss: 144.1217498779297\n",
      "Epoch: 20400\tTrain loss: 142.4884033203125\n",
      "Epoch: 20450\tTrain loss: 140.93673706054688\n",
      "Epoch: 20500\tTrain loss: 182.7727813720703\n",
      "Epoch: 20550\tTrain loss: 140.0710906982422\n",
      "Epoch: 20600\tTrain loss: 137.8366241455078\n",
      "Epoch: 20650\tTrain loss: 136.18724060058594\n",
      "Epoch: 20700\tTrain loss: 134.71481323242188\n",
      "Epoch: 20750\tTrain loss: 133.33987426757812\n",
      "Epoch: 20800\tTrain loss: 132.03224182128906\n",
      "Epoch: 20850\tTrain loss: 130.7761688232422\n",
      "Epoch: 20900\tTrain loss: 129.56312561035156\n",
      "Epoch: 20950\tTrain loss: 128.38656616210938\n",
      "Epoch: 21000\tTrain loss: 127.24209594726562\n",
      "Epoch: 21050\tTrain loss: 126.12610626220703\n",
      "Epoch: 21100\tTrain loss: 125.0356216430664\n",
      "Epoch: 21150\tTrain loss: 123.96746063232422\n",
      "Epoch: 21200\tTrain loss: 122.91958618164062\n",
      "Epoch: 21250\tTrain loss: 121.88861846923828\n",
      "Epoch: 21300\tTrain loss: 120.87203216552734\n",
      "Epoch: 21350\tTrain loss: 163.67901611328125\n",
      "Epoch: 21400\tTrain loss: 120.08776092529297\n",
      "Epoch: 21450\tTrain loss: 118.60536193847656\n",
      "Epoch: 21500\tTrain loss: 117.42106628417969\n",
      "Epoch: 21550\tTrain loss: 116.30864715576172\n",
      "Epoch: 21600\tTrain loss: 115.22130584716797\n",
      "Epoch: 21650\tTrain loss: 114.13441467285156\n",
      "Epoch: 21700\tTrain loss: 113.02557373046875\n",
      "Epoch: 21750\tTrain loss: 111.86610412597656\n",
      "Epoch: 21800\tTrain loss: 110.57669067382812\n",
      "Epoch: 21850\tTrain loss: 109.07877349853516\n",
      "Epoch: 21900\tTrain loss: 107.31046295166016\n",
      "Epoch: 21950\tTrain loss: 105.0772476196289\n",
      "Epoch: 22000\tTrain loss: 102.16962432861328\n",
      "Epoch: 22050\tTrain loss: 99.24441528320312\n",
      "Epoch: 22100\tTrain loss: 96.32789611816406\n",
      "Epoch: 22150\tTrain loss: 94.30968475341797\n",
      "Epoch: 22200\tTrain loss: 91.7769546508789\n",
      "Epoch: 22250\tTrain loss: 88.32097625732422\n",
      "Epoch: 22300\tTrain loss: 84.68449401855469\n",
      "Epoch: 22350\tTrain loss: 80.1834945678711\n",
      "Epoch: 22400\tTrain loss: 74.36839294433594\n",
      "Epoch: 22450\tTrain loss: 70.88089752197266\n",
      "Epoch: 22500\tTrain loss: 64.3053207397461\n",
      "Epoch: 22550\tTrain loss: 58.61289596557617\n",
      "Epoch: 22600\tTrain loss: 239.74630737304688\n",
      "Epoch: 22650\tTrain loss: 168.86654663085938\n",
      "Epoch: 22700\tTrain loss: 148.3546905517578\n",
      "Epoch: 22750\tTrain loss: 134.0188751220703\n",
      "Epoch: 22800\tTrain loss: 120.40614318847656\n",
      "Epoch: 22850\tTrain loss: 177.01878356933594\n",
      "Epoch: 22900\tTrain loss: 4684.64892578125\n",
      "Epoch: 22950\tTrain loss: 471.7186584472656\n",
      "Epoch: 23000\tTrain loss: 367.966064453125\n",
      "Epoch: 23050\tTrain loss: 343.9408874511719\n",
      "Epoch: 23100\tTrain loss: 324.81085205078125\n",
      "Epoch: 23150\tTrain loss: 303.4899597167969\n",
      "Epoch: 23200\tTrain loss: 283.1376647949219\n",
      "Epoch: 23250\tTrain loss: 273.0924072265625\n",
      "Epoch: 23300\tTrain loss: 265.2751159667969\n",
      "Epoch: 23350\tTrain loss: 258.5438537597656\n",
      "Epoch: 23400\tTrain loss: 252.4652557373047\n",
      "Epoch: 23450\tTrain loss: 246.80726623535156\n",
      "Epoch: 23500\tTrain loss: 241.41490173339844\n",
      "Epoch: 23550\tTrain loss: 236.1190643310547\n",
      "Epoch: 23600\tTrain loss: 230.0672149658203\n",
      "Epoch: 23650\tTrain loss: 218.91026306152344\n",
      "Epoch: 23700\tTrain loss: 211.43263244628906\n",
      "Epoch: 23750\tTrain loss: 205.4633026123047\n",
      "Epoch: 23800\tTrain loss: 200.09454345703125\n",
      "Epoch: 23850\tTrain loss: 195.0007781982422\n",
      "Epoch: 23900\tTrain loss: 190.04257202148438\n",
      "Epoch: 23950\tTrain loss: 185.21726989746094\n",
      "Epoch: 24000\tTrain loss: 180.64761352539062\n",
      "Epoch: 24050\tTrain loss: 176.4728546142578\n",
      "Epoch: 24100\tTrain loss: 172.70632934570312\n",
      "Epoch: 24150\tTrain loss: 169.23956298828125\n",
      "Epoch: 24200\tTrain loss: 165.94508361816406\n",
      "Epoch: 24250\tTrain loss: 162.73947143554688\n",
      "Epoch: 24300\tTrain loss: 159.58261108398438\n",
      "Epoch: 24350\tTrain loss: 156.46107482910156\n",
      "Epoch: 24400\tTrain loss: 153.3759307861328\n",
      "Epoch: 24450\tTrain loss: 150.3386688232422\n",
      "Epoch: 24500\tTrain loss: 147.36660766601562\n",
      "Epoch: 24550\tTrain loss: 144.47320556640625\n",
      "Epoch: 24600\tTrain loss: 141.66061401367188\n",
      "Epoch: 24650\tTrain loss: 138.92013549804688\n",
      "Epoch: 24700\tTrain loss: 136.2385711669922\n",
      "Epoch: 24750\tTrain loss: 133.60360717773438\n",
      "Epoch: 24800\tTrain loss: 131.0047149658203\n",
      "Epoch: 24850\tTrain loss: 127.13776397705078\n",
      "Epoch: 24900\tTrain loss: 122.6388931274414\n",
      "Epoch: 24950\tTrain loss: 119.31472778320312\n",
      "Training CFRNN\n",
      "Epoch: 0\tTrain loss: 2436552.25\n",
      "Epoch: 50\tTrain loss: 2405405.25\n",
      "Epoch: 100\tTrain loss: 2373094.5\n",
      "Epoch: 150\tTrain loss: 2342811.5\n",
      "Epoch: 200\tTrain loss: 2313505.75\n",
      "Epoch: 250\tTrain loss: 2284859.0\n",
      "Epoch: 300\tTrain loss: 2256736.75\n",
      "Epoch: 350\tTrain loss: 2229066.5\n",
      "Epoch: 400\tTrain loss: 2201805.25\n",
      "Epoch: 450\tTrain loss: 2174924.0\n",
      "Epoch: 500\tTrain loss: 2148402.75\n",
      "Epoch: 550\tTrain loss: 2122225.5\n",
      "Epoch: 600\tTrain loss: 2096380.375\n",
      "Epoch: 650\tTrain loss: 2070857.625\n",
      "Epoch: 700\tTrain loss: 2045648.375\n",
      "Epoch: 750\tTrain loss: 2020746.0\n",
      "Epoch: 800\tTrain loss: 1996144.375\n",
      "Epoch: 850\tTrain loss: 1971837.25\n",
      "Epoch: 900\tTrain loss: 1947820.0\n",
      "Epoch: 950\tTrain loss: 1924087.5\n",
      "Epoch: 1000\tTrain loss: 1900635.75\n",
      "Epoch: 1050\tTrain loss: 1877460.875\n",
      "Epoch: 1100\tTrain loss: 1854558.5\n",
      "Epoch: 1150\tTrain loss: 1831925.5\n",
      "Epoch: 1200\tTrain loss: 1809558.375\n",
      "Epoch: 1250\tTrain loss: 1787453.625\n",
      "Epoch: 1300\tTrain loss: 1765608.75\n",
      "Epoch: 1350\tTrain loss: 1744020.0\n",
      "Epoch: 1400\tTrain loss: 1722685.0\n",
      "Epoch: 1450\tTrain loss: 1701601.125\n",
      "Epoch: 1500\tTrain loss: 1680765.0\n",
      "Epoch: 1550\tTrain loss: 1660174.625\n",
      "Epoch: 1600\tTrain loss: 1639827.375\n",
      "Epoch: 1650\tTrain loss: 1619720.875\n",
      "Epoch: 1700\tTrain loss: 1599852.875\n",
      "Epoch: 1750\tTrain loss: 1580220.75\n",
      "Epoch: 1800\tTrain loss: 1560822.5\n",
      "Epoch: 1850\tTrain loss: 1541655.875\n",
      "Epoch: 1900\tTrain loss: 1522718.875\n",
      "Epoch: 1950\tTrain loss: 1504009.5\n",
      "Epoch: 2000\tTrain loss: 1485525.5\n",
      "Epoch: 2050\tTrain loss: 1467265.375\n",
      "Epoch: 2100\tTrain loss: 1449226.875\n",
      "Epoch: 2150\tTrain loss: 1431408.25\n",
      "Epoch: 2200\tTrain loss: 1413807.5\n",
      "Epoch: 2250\tTrain loss: 1396423.125\n",
      "Epoch: 2300\tTrain loss: 1379253.125\n",
      "Epoch: 2350\tTrain loss: 1362296.25\n",
      "Epoch: 2400\tTrain loss: 1345550.375\n",
      "Epoch: 2450\tTrain loss: 1329014.125\n",
      "Epoch: 2500\tTrain loss: 1312686.0\n",
      "Epoch: 2550\tTrain loss: 1296564.25\n",
      "Epoch: 2600\tTrain loss: 1280647.375\n",
      "Epoch: 2650\tTrain loss: 1264934.0\n",
      "Epoch: 2700\tTrain loss: 1249422.75\n",
      "Epoch: 2750\tTrain loss: 1234111.625\n",
      "Epoch: 2800\tTrain loss: 1218999.875\n",
      "Epoch: 2850\tTrain loss: 1204085.875\n",
      "Epoch: 2900\tTrain loss: 1189368.125\n",
      "Epoch: 2950\tTrain loss: 1174845.75\n",
      "Epoch: 3000\tTrain loss: 1160516.875\n",
      "Epoch: 3050\tTrain loss: 1146380.625\n",
      "Epoch: 3100\tTrain loss: 1132435.375\n",
      "Epoch: 3150\tTrain loss: 1118680.125\n",
      "Epoch: 3200\tTrain loss: 1105113.625\n",
      "Epoch: 3250\tTrain loss: 1091734.625\n",
      "Epoch: 3300\tTrain loss: 1078541.75\n",
      "Epoch: 3350\tTrain loss: 1065534.125\n",
      "Epoch: 3400\tTrain loss: 1052710.375\n",
      "Epoch: 3450\tTrain loss: 1040069.5625\n",
      "Epoch: 3500\tTrain loss: 1027610.375\n",
      "Epoch: 3550\tTrain loss: 1015331.625\n",
      "Epoch: 3600\tTrain loss: 1003232.3125\n",
      "Epoch: 3650\tTrain loss: 991311.5625\n",
      "Epoch: 3700\tTrain loss: 979567.8125\n",
      "Epoch: 3750\tTrain loss: 968000.1875\n",
      "Epoch: 3800\tTrain loss: 956607.5625\n",
      "Epoch: 3850\tTrain loss: 945389.125\n",
      "Epoch: 3900\tTrain loss: 934343.6875\n",
      "Epoch: 3950\tTrain loss: 923469.875\n",
      "Epoch: 4000\tTrain loss: 912767.125\n",
      "Epoch: 4050\tTrain loss: 902234.125\n",
      "Epoch: 4100\tTrain loss: 891870.0\n",
      "Epoch: 4150\tTrain loss: 881673.375\n",
      "Epoch: 4200\tTrain loss: 871643.625\n",
      "Epoch: 4250\tTrain loss: 861779.5\n",
      "Epoch: 4300\tTrain loss: 852080.1875\n",
      "Epoch: 4350\tTrain loss: 842544.375\n",
      "Epoch: 4400\tTrain loss: 833171.125\n",
      "Epoch: 4450\tTrain loss: 823959.6875\n",
      "Epoch: 4500\tTrain loss: 814908.5\n",
      "Epoch: 4550\tTrain loss: 806017.0625\n",
      "Epoch: 4600\tTrain loss: 797284.125\n",
      "Epoch: 4650\tTrain loss: 788708.5625\n",
      "Epoch: 4700\tTrain loss: 780289.4375\n",
      "Epoch: 4750\tTrain loss: 772025.6875\n",
      "Epoch: 4800\tTrain loss: 763916.4375\n",
      "Epoch: 4850\tTrain loss: 755960.4375\n",
      "Epoch: 4900\tTrain loss: 748156.625\n",
      "Epoch: 4950\tTrain loss: 740504.125\n",
      "Epoch: 5000\tTrain loss: 733001.625\n",
      "Epoch: 5050\tTrain loss: 725648.25\n",
      "Epoch: 5100\tTrain loss: 718442.875\n",
      "Epoch: 5150\tTrain loss: 711384.3125\n",
      "Epoch: 5200\tTrain loss: 704471.625\n",
      "Epoch: 5250\tTrain loss: 697703.4375\n",
      "Epoch: 5300\tTrain loss: 691078.875\n",
      "Epoch: 5350\tTrain loss: 684596.6875\n",
      "Epoch: 5400\tTrain loss: 678255.8125\n",
      "Epoch: 5450\tTrain loss: 672054.9375\n",
      "Epoch: 5500\tTrain loss: 665993.1875\n",
      "Epoch: 5550\tTrain loss: 660069.0625\n",
      "Epoch: 5600\tTrain loss: 654281.5625\n",
      "Epoch: 5650\tTrain loss: 648629.3125\n",
      "Epoch: 5700\tTrain loss: 643111.1875\n",
      "Epoch: 5750\tTrain loss: 637726.0\n",
      "Epoch: 5800\tTrain loss: 632472.4375\n",
      "Epoch: 5850\tTrain loss: 627349.1875\n",
      "Epoch: 5900\tTrain loss: 622354.75\n",
      "Epoch: 5950\tTrain loss: 605356.9375\n",
      "Epoch: 6000\tTrain loss: 598862.5\n",
      "Epoch: 6050\tTrain loss: 535751.375\n",
      "Epoch: 6100\tTrain loss: 512848.0625\n",
      "Epoch: 6150\tTrain loss: 503361.375\n",
      "Epoch: 6200\tTrain loss: 494276.4375\n",
      "Epoch: 6250\tTrain loss: 485443.15625\n",
      "Epoch: 6300\tTrain loss: 476808.59375\n",
      "Epoch: 6350\tTrain loss: 468513.0\n",
      "Epoch: 6400\tTrain loss: 460183.15625\n",
      "Epoch: 6450\tTrain loss: 452131.625\n",
      "Epoch: 6500\tTrain loss: 444275.15625\n",
      "Epoch: 6550\tTrain loss: 436600.625\n",
      "Epoch: 6600\tTrain loss: 429790.4375\n",
      "Epoch: 6650\tTrain loss: 421788.0625\n",
      "Epoch: 6700\tTrain loss: 414551.15625\n",
      "Epoch: 6750\tTrain loss: 407530.75\n",
      "Epoch: 6800\tTrain loss: 400802.0625\n",
      "Epoch: 6850\tTrain loss: 394041.46875\n",
      "Epoch: 6900\tTrain loss: 387460.0\n",
      "Epoch: 6950\tTrain loss: 381037.53125\n",
      "Epoch: 7000\tTrain loss: 374762.75\n",
      "Epoch: 7050\tTrain loss: 368629.5625\n",
      "Epoch: 7100\tTrain loss: 362633.65625\n",
      "Epoch: 7150\tTrain loss: 356771.59375\n",
      "Epoch: 7200\tTrain loss: 351040.28125\n",
      "Epoch: 7250\tTrain loss: 345436.96875\n",
      "Epoch: 7300\tTrain loss: 339959.0625\n",
      "Epoch: 7350\tTrain loss: 334604.03125\n",
      "Epoch: 7400\tTrain loss: 329369.375\n",
      "Epoch: 7450\tTrain loss: 324252.375\n",
      "Epoch: 7500\tTrain loss: 319249.96875\n",
      "Epoch: 7550\tTrain loss: 314357.90625\n",
      "Epoch: 7600\tTrain loss: 309568.09375\n",
      "Epoch: 7650\tTrain loss: 304851.25\n",
      "Epoch: 7700\tTrain loss: 298838.59375\n",
      "Epoch: 7750\tTrain loss: 293120.28125\n",
      "Epoch: 7800\tTrain loss: 288061.21875\n",
      "Epoch: 7850\tTrain loss: 283102.15625\n",
      "Epoch: 7900\tTrain loss: 278232.8125\n",
      "Epoch: 7950\tTrain loss: 273450.8125\n",
      "Epoch: 8000\tTrain loss: 268753.1875\n",
      "Epoch: 8050\tTrain loss: 264133.09375\n",
      "Epoch: 8100\tTrain loss: 259546.234375\n",
      "Epoch: 8150\tTrain loss: 254985.046875\n",
      "Epoch: 8200\tTrain loss: 250467.09375\n",
      "Epoch: 8250\tTrain loss: 245992.5\n",
      "Epoch: 8300\tTrain loss: 241561.859375\n",
      "Epoch: 8350\tTrain loss: 237175.84375\n",
      "Epoch: 8400\tTrain loss: 232834.984375\n",
      "Epoch: 8450\tTrain loss: 228540.234375\n",
      "Epoch: 8500\tTrain loss: 224291.921875\n",
      "Epoch: 8550\tTrain loss: 220089.625\n",
      "Epoch: 8600\tTrain loss: 215932.40625\n",
      "Epoch: 8650\tTrain loss: 211820.375\n",
      "Epoch: 8700\tTrain loss: 207757.140625\n",
      "Epoch: 8750\tTrain loss: 203736.15625\n",
      "Epoch: 8800\tTrain loss: 199763.3125\n",
      "Epoch: 8850\tTrain loss: 195837.578125\n",
      "Epoch: 8900\tTrain loss: 191959.265625\n",
      "Epoch: 8950\tTrain loss: 188128.484375\n",
      "Epoch: 9000\tTrain loss: 184345.40625\n",
      "Epoch: 9050\tTrain loss: 180610.15625\n",
      "Epoch: 9100\tTrain loss: 176922.90625\n",
      "Epoch: 9150\tTrain loss: 173283.75\n",
      "Epoch: 9200\tTrain loss: 169692.765625\n",
      "Epoch: 9250\tTrain loss: 166150.0625\n",
      "Epoch: 9300\tTrain loss: 162655.6875\n",
      "Epoch: 9350\tTrain loss: 159209.671875\n",
      "Epoch: 9400\tTrain loss: 155812.203125\n",
      "Epoch: 9450\tTrain loss: 152462.953125\n",
      "Epoch: 9500\tTrain loss: 149162.078125\n",
      "Epoch: 9550\tTrain loss: 145909.609375\n",
      "Epoch: 9600\tTrain loss: 142705.46875\n",
      "Epoch: 9650\tTrain loss: 139549.609375\n",
      "Epoch: 9700\tTrain loss: 136441.96875\n",
      "Epoch: 9750\tTrain loss: 133382.5\n",
      "Epoch: 9800\tTrain loss: 130371.0703125\n",
      "Epoch: 9850\tTrain loss: 127407.5859375\n",
      "Epoch: 9900\tTrain loss: 124492.8828125\n",
      "Epoch: 9950\tTrain loss: 121624.4765625\n",
      "Epoch: 10000\tTrain loss: 118804.1875\n",
      "Epoch: 10050\tTrain loss: 116031.3828125\n",
      "Epoch: 10100\tTrain loss: 113305.9453125\n",
      "Epoch: 10150\tTrain loss: 110627.6796875\n",
      "Epoch: 10200\tTrain loss: 107997.15625\n",
      "Epoch: 10250\tTrain loss: 105412.1171875\n",
      "Epoch: 10300\tTrain loss: 102874.3203125\n",
      "Epoch: 10350\tTrain loss: 100382.9921875\n",
      "Epoch: 10400\tTrain loss: 97938.796875\n",
      "Epoch: 10450\tTrain loss: 95539.03125\n",
      "Epoch: 10500\tTrain loss: 93185.765625\n",
      "Epoch: 10550\tTrain loss: 90878.1015625\n",
      "Epoch: 10600\tTrain loss: 88619.8125\n",
      "Epoch: 10650\tTrain loss: 86398.7578125\n",
      "Epoch: 10700\tTrain loss: 84226.2109375\n",
      "Epoch: 10750\tTrain loss: 82098.3046875\n",
      "Epoch: 10800\tTrain loss: 80014.6640625\n",
      "Epoch: 10850\tTrain loss: 77982.109375\n",
      "Epoch: 10900\tTrain loss: 75979.3046875\n",
      "Epoch: 10950\tTrain loss: 74026.6640625\n",
      "Epoch: 11000\tTrain loss: 72117.140625\n",
      "Epoch: 11050\tTrain loss: 70250.3359375\n",
      "Epoch: 11100\tTrain loss: 68425.953125\n",
      "Epoch: 11150\tTrain loss: 66644.1796875\n",
      "Epoch: 11200\tTrain loss: 64903.1796875\n",
      "Epoch: 11250\tTrain loss: 63203.9609375\n",
      "Epoch: 11300\tTrain loss: 61545.78125\n",
      "Epoch: 11350\tTrain loss: 59928.21875\n",
      "Epoch: 11400\tTrain loss: 58351.546875\n",
      "Epoch: 11450\tTrain loss: 56813.73828125\n",
      "Epoch: 11500\tTrain loss: 55315.82421875\n",
      "Epoch: 11550\tTrain loss: 53856.9921875\n",
      "Epoch: 11600\tTrain loss: 52436.8671875\n",
      "Epoch: 11650\tTrain loss: 51054.984375\n",
      "Epoch: 11700\tTrain loss: 49711.5625\n",
      "Epoch: 11750\tTrain loss: 48404.484375\n",
      "Epoch: 11800\tTrain loss: 47134.75\n",
      "Epoch: 11850\tTrain loss: 45901.51171875\n",
      "Epoch: 11900\tTrain loss: 44704.30859375\n",
      "Epoch: 11950\tTrain loss: 43542.66796875\n",
      "Epoch: 12000\tTrain loss: 42417.71484375\n",
      "Epoch: 12050\tTrain loss: 41324.38671875\n",
      "Epoch: 12100\tTrain loss: 40266.5234375\n",
      "Epoch: 12150\tTrain loss: 39242.28515625\n",
      "Epoch: 12200\tTrain loss: 38251.15625\n",
      "Epoch: 12250\tTrain loss: 37292.6328125\n",
      "Epoch: 12300\tTrain loss: 36366.1875\n",
      "Epoch: 12350\tTrain loss: 35471.703125\n",
      "Epoch: 12400\tTrain loss: 34607.61328125\n",
      "Epoch: 12450\tTrain loss: 33774.19921875\n",
      "Epoch: 12500\tTrain loss: 32970.7421875\n",
      "Epoch: 12550\tTrain loss: 32196.70703125\n",
      "Epoch: 12600\tTrain loss: 31451.986328125\n",
      "Epoch: 12650\tTrain loss: 30734.966796875\n",
      "Epoch: 12700\tTrain loss: 30045.740234375\n",
      "Epoch: 12750\tTrain loss: 29383.470703125\n",
      "Epoch: 12800\tTrain loss: 28747.72265625\n",
      "Epoch: 12850\tTrain loss: 28137.755859375\n",
      "Epoch: 12900\tTrain loss: 27552.826171875\n",
      "Epoch: 12950\tTrain loss: 26992.44140625\n",
      "Epoch: 13000\tTrain loss: 26436.63671875\n",
      "Epoch: 13050\tTrain loss: 25049.51953125\n",
      "Epoch: 13100\tTrain loss: 24042.955078125\n",
      "Epoch: 13150\tTrain loss: 23234.107421875\n",
      "Epoch: 13200\tTrain loss: 22469.470703125\n",
      "Epoch: 13250\tTrain loss: 21736.16015625\n",
      "Epoch: 13300\tTrain loss: 21033.853515625\n",
      "Epoch: 13350\tTrain loss: 20361.36328125\n",
      "Epoch: 13400\tTrain loss: 19591.365234375\n",
      "Epoch: 13450\tTrain loss: 18887.478515625\n",
      "Epoch: 13500\tTrain loss: 18202.052734375\n",
      "Epoch: 13550\tTrain loss: 17532.7109375\n",
      "Epoch: 13600\tTrain loss: 16879.38671875\n",
      "Epoch: 13650\tTrain loss: 16241.85546875\n",
      "Epoch: 13700\tTrain loss: 15620.443359375\n",
      "Epoch: 13750\tTrain loss: 15013.9013671875\n",
      "Epoch: 13800\tTrain loss: 14423.181640625\n",
      "Epoch: 13850\tTrain loss: 13847.994140625\n",
      "Epoch: 13900\tTrain loss: 13288.2177734375\n",
      "Epoch: 13950\tTrain loss: 12744.453125\n",
      "Epoch: 14000\tTrain loss: 12214.7919921875\n",
      "Epoch: 14050\tTrain loss: 11701.4599609375\n",
      "Epoch: 14100\tTrain loss: 11205.212890625\n",
      "Epoch: 14150\tTrain loss: 10729.8203125\n",
      "Epoch: 14200\tTrain loss: 10279.951171875\n",
      "Epoch: 14250\tTrain loss: 9865.0966796875\n",
      "Epoch: 14300\tTrain loss: 9396.265625\n",
      "Epoch: 14350\tTrain loss: 8966.935546875\n",
      "Epoch: 14400\tTrain loss: 8552.275390625\n",
      "Epoch: 14450\tTrain loss: 8151.5068359375\n",
      "Epoch: 14500\tTrain loss: 7764.427734375\n",
      "Epoch: 14550\tTrain loss: 7390.82568359375\n",
      "Epoch: 14600\tTrain loss: 7030.47509765625\n",
      "Epoch: 14650\tTrain loss: 6683.16259765625\n",
      "Epoch: 14700\tTrain loss: 6348.654296875\n",
      "Epoch: 14750\tTrain loss: 6026.728515625\n",
      "Epoch: 14800\tTrain loss: 5717.15673828125\n",
      "Epoch: 14850\tTrain loss: 5416.01025390625\n",
      "Epoch: 14900\tTrain loss: 5128.5341796875\n",
      "Epoch: 14950\tTrain loss: 4855.02587890625\n",
      "Epoch: 15000\tTrain loss: 4592.853515625\n",
      "Epoch: 15050\tTrain loss: 4341.80517578125\n",
      "Epoch: 15100\tTrain loss: 4101.64013671875\n",
      "Epoch: 15150\tTrain loss: 3872.08447265625\n",
      "Epoch: 15200\tTrain loss: 3652.896240234375\n",
      "Epoch: 15250\tTrain loss: 3443.8388671875\n",
      "Epoch: 15300\tTrain loss: 3244.742919921875\n",
      "Epoch: 15350\tTrain loss: 3055.915771484375\n",
      "Epoch: 15400\tTrain loss: 2878.58447265625\n",
      "Epoch: 15450\tTrain loss: 2708.5634765625\n",
      "Epoch: 15500\tTrain loss: 2545.70263671875\n",
      "Epoch: 15550\tTrain loss: 2391.533447265625\n",
      "Epoch: 15600\tTrain loss: 2245.561279296875\n",
      "Epoch: 15650\tTrain loss: 2107.518798828125\n",
      "Epoch: 15700\tTrain loss: 1977.1624755859375\n",
      "Epoch: 15750\tTrain loss: 1854.2645263671875\n",
      "Epoch: 15800\tTrain loss: 1738.113525390625\n",
      "Epoch: 15850\tTrain loss: 1628.754638671875\n",
      "Epoch: 15900\tTrain loss: 1525.9150390625\n",
      "Epoch: 15950\tTrain loss: 1429.3270263671875\n",
      "Epoch: 16000\tTrain loss: 1338.7120361328125\n",
      "Epoch: 16050\tTrain loss: 1253.81591796875\n",
      "Epoch: 16100\tTrain loss: 1174.3731689453125\n",
      "Epoch: 16150\tTrain loss: 1100.16015625\n",
      "Epoch: 16200\tTrain loss: 1030.874755859375\n",
      "Epoch: 16250\tTrain loss: 966.3392944335938\n",
      "Epoch: 16300\tTrain loss: 906.2789306640625\n",
      "Epoch: 16350\tTrain loss: 850.433837890625\n",
      "Epoch: 16400\tTrain loss: 798.5689697265625\n",
      "Epoch: 16450\tTrain loss: 750.4480590820312\n",
      "Epoch: 16500\tTrain loss: 705.891357421875\n",
      "Epoch: 16550\tTrain loss: 664.5905151367188\n",
      "Epoch: 16600\tTrain loss: 626.5437622070312\n",
      "Epoch: 16650\tTrain loss: 591.1641235351562\n",
      "Epoch: 16700\tTrain loss: 558.6190185546875\n",
      "Epoch: 16750\tTrain loss: 528.583984375\n",
      "Epoch: 16800\tTrain loss: 500.8973388671875\n",
      "Epoch: 16850\tTrain loss: 475.39910888671875\n",
      "Epoch: 16900\tTrain loss: 451.8768310546875\n",
      "Epoch: 16950\tTrain loss: 430.21954345703125\n",
      "Epoch: 17000\tTrain loss: 410.227294921875\n",
      "Epoch: 17050\tTrain loss: 391.5989685058594\n",
      "Epoch: 17100\tTrain loss: 373.5643615722656\n",
      "Epoch: 17150\tTrain loss: 357.4420166015625\n",
      "Epoch: 17200\tTrain loss: 341.9903869628906\n",
      "Epoch: 17250\tTrain loss: 327.5509033203125\n",
      "Epoch: 17300\tTrain loss: 314.1530456542969\n",
      "Epoch: 17350\tTrain loss: 301.8364562988281\n",
      "Epoch: 17400\tTrain loss: 290.3492736816406\n",
      "Epoch: 17450\tTrain loss: 279.656005859375\n",
      "Epoch: 17500\tTrain loss: 269.65673828125\n",
      "Epoch: 17550\tTrain loss: 260.27587890625\n",
      "Epoch: 17600\tTrain loss: 251.45437622070312\n",
      "Epoch: 17650\tTrain loss: 243.1370086669922\n",
      "Epoch: 17700\tTrain loss: 235.27517700195312\n",
      "Epoch: 17750\tTrain loss: 227.8155517578125\n",
      "Epoch: 17800\tTrain loss: 220.7218475341797\n",
      "Epoch: 17850\tTrain loss: 213.97357177734375\n",
      "Epoch: 17900\tTrain loss: 207.48056030273438\n",
      "Epoch: 17950\tTrain loss: 201.2465362548828\n",
      "Epoch: 18000\tTrain loss: 195.02792358398438\n",
      "Epoch: 18050\tTrain loss: 188.0513458251953\n",
      "Epoch: 18100\tTrain loss: 180.6113739013672\n",
      "Epoch: 18150\tTrain loss: 173.16357421875\n",
      "Epoch: 18200\tTrain loss: 166.4486846923828\n",
      "Epoch: 18250\tTrain loss: 160.64523315429688\n",
      "Epoch: 18300\tTrain loss: 155.2996368408203\n",
      "Epoch: 18350\tTrain loss: 150.20645141601562\n",
      "Epoch: 18400\tTrain loss: 145.27548217773438\n",
      "Epoch: 18450\tTrain loss: 140.46087646484375\n",
      "Epoch: 18500\tTrain loss: 135.71832275390625\n",
      "Epoch: 18550\tTrain loss: 131.04791259765625\n",
      "Epoch: 18600\tTrain loss: 1029.57470703125\n",
      "Epoch: 18650\tTrain loss: 783.7554321289062\n",
      "Epoch: 18700\tTrain loss: 214.67242431640625\n",
      "Epoch: 18750\tTrain loss: 156.99452209472656\n",
      "Epoch: 18800\tTrain loss: 146.1420135498047\n",
      "Epoch: 18850\tTrain loss: 139.4124755859375\n",
      "Epoch: 18900\tTrain loss: 133.86312866210938\n",
      "Epoch: 18950\tTrain loss: 128.90872192382812\n",
      "Epoch: 19000\tTrain loss: 124.28546905517578\n",
      "Epoch: 19050\tTrain loss: 119.90693664550781\n",
      "Epoch: 19100\tTrain loss: 115.65860748291016\n",
      "Epoch: 19150\tTrain loss: 111.4462661743164\n",
      "Epoch: 19200\tTrain loss: 107.25326538085938\n",
      "Epoch: 19250\tTrain loss: 103.18561553955078\n",
      "Epoch: 19300\tTrain loss: 99.3768081665039\n",
      "Epoch: 19350\tTrain loss: 95.81029510498047\n",
      "Epoch: 19400\tTrain loss: 92.41791534423828\n",
      "Epoch: 19450\tTrain loss: 89.15898895263672\n",
      "Epoch: 19500\tTrain loss: 86.01339721679688\n",
      "Epoch: 19550\tTrain loss: 82.96951293945312\n",
      "Epoch: 19600\tTrain loss: 80.01982879638672\n",
      "Epoch: 19650\tTrain loss: 77.15966796875\n",
      "Epoch: 19700\tTrain loss: 74.38505554199219\n",
      "Epoch: 19750\tTrain loss: 71.6924057006836\n",
      "Epoch: 19800\tTrain loss: 69.07937622070312\n",
      "Epoch: 19850\tTrain loss: 66.5421371459961\n",
      "Epoch: 19900\tTrain loss: 64.07804870605469\n",
      "Epoch: 19950\tTrain loss: 61.68551254272461\n",
      "Epoch: 20000\tTrain loss: 59.36132049560547\n",
      "Epoch: 20050\tTrain loss: 57.104095458984375\n",
      "Epoch: 20100\tTrain loss: 54.91143798828125\n",
      "Epoch: 20150\tTrain loss: 52.78230285644531\n",
      "Epoch: 20200\tTrain loss: 50.714378356933594\n",
      "Epoch: 20250\tTrain loss: 48.706485748291016\n",
      "Epoch: 20300\tTrain loss: 46.757354736328125\n",
      "Epoch: 20350\tTrain loss: 44.865821838378906\n",
      "Epoch: 20400\tTrain loss: 43.03038024902344\n",
      "Epoch: 20450\tTrain loss: 41.249881744384766\n",
      "Epoch: 20500\tTrain loss: 39.5232048034668\n",
      "Epoch: 20550\tTrain loss: 37.84953689575195\n",
      "Epoch: 20600\tTrain loss: 36.22782516479492\n",
      "Epoch: 20650\tTrain loss: 34.65684127807617\n",
      "Epoch: 20700\tTrain loss: 33.135799407958984\n",
      "Epoch: 20750\tTrain loss: 31.663528442382812\n",
      "Epoch: 20800\tTrain loss: 30.239192962646484\n",
      "Epoch: 20850\tTrain loss: 28.86197280883789\n",
      "Epoch: 20900\tTrain loss: 27.53076171875\n",
      "Epoch: 20950\tTrain loss: 26.244829177856445\n",
      "Epoch: 21000\tTrain loss: 25.003278732299805\n",
      "Epoch: 21050\tTrain loss: 23.805028915405273\n",
      "Epoch: 21100\tTrain loss: 22.64935874938965\n",
      "Epoch: 21150\tTrain loss: 21.53533935546875\n",
      "Epoch: 21200\tTrain loss: 20.462596893310547\n",
      "Epoch: 21250\tTrain loss: 19.428150177001953\n",
      "Epoch: 21300\tTrain loss: 18.433359146118164\n",
      "Epoch: 21350\tTrain loss: 17.476806640625\n",
      "Epoch: 21400\tTrain loss: 16.565587997436523\n",
      "Epoch: 21450\tTrain loss: 15.674762725830078\n",
      "Epoch: 21500\tTrain loss: 14.826489448547363\n",
      "Epoch: 21550\tTrain loss: 14.013397216796875\n",
      "Epoch: 21600\tTrain loss: 13.234939575195312\n",
      "Epoch: 21650\tTrain loss: 12.487557411193848\n",
      "Epoch: 21700\tTrain loss: 11.772672653198242\n",
      "Epoch: 21750\tTrain loss: 11.089134216308594\n",
      "Epoch: 21800\tTrain loss: 10.444791793823242\n",
      "Epoch: 21850\tTrain loss: 9.81226634979248\n",
      "Epoch: 21900\tTrain loss: 9.216226577758789\n",
      "Epoch: 21950\tTrain loss: 8.6482515335083\n",
      "Epoch: 22000\tTrain loss: 8.106794357299805\n",
      "Epoch: 22050\tTrain loss: 7.59232759475708\n",
      "Epoch: 22100\tTrain loss: 7.1001505851745605\n",
      "Epoch: 22150\tTrain loss: 6.6333160400390625\n",
      "Epoch: 22200\tTrain loss: 6.190062522888184\n",
      "Epoch: 22250\tTrain loss: 5.798386096954346\n",
      "Epoch: 22300\tTrain loss: 5.3727569580078125\n",
      "Epoch: 22350\tTrain loss: 4.997189044952393\n",
      "Epoch: 22400\tTrain loss: 4.643162250518799\n",
      "Epoch: 22450\tTrain loss: 4.309552192687988\n",
      "Epoch: 22500\tTrain loss: 3.9965577125549316\n",
      "Epoch: 22550\tTrain loss: 3.6997835636138916\n",
      "Epoch: 22600\tTrain loss: 3.4220292568206787\n",
      "Epoch: 22650\tTrain loss: 3.161280870437622\n",
      "Epoch: 22700\tTrain loss: 2.9171972274780273\n",
      "Epoch: 22750\tTrain loss: 2.6876559257507324\n",
      "Epoch: 22800\tTrain loss: 2.473355531692505\n",
      "Epoch: 22850\tTrain loss: 2.273130178451538\n",
      "Epoch: 22900\tTrain loss: 2.0864336490631104\n",
      "Epoch: 22950\tTrain loss: 1.9131132364273071\n",
      "Epoch: 23000\tTrain loss: 1.7500555515289307\n",
      "Epoch: 23050\tTrain loss: 1.5994826555252075\n",
      "Epoch: 23100\tTrain loss: 1.4596822261810303\n",
      "Epoch: 23150\tTrain loss: 1.330335259437561\n",
      "Epoch: 23200\tTrain loss: 1.210267186164856\n",
      "Epoch: 23250\tTrain loss: 1.099515438079834\n",
      "Epoch: 23300\tTrain loss: 0.9974762201309204\n",
      "Epoch: 23350\tTrain loss: 0.9033622145652771\n",
      "Epoch: 23400\tTrain loss: 0.8166432976722717\n",
      "Epoch: 23450\tTrain loss: 0.7371760010719299\n",
      "Epoch: 23500\tTrain loss: 0.6643192768096924\n",
      "Epoch: 23550\tTrain loss: 0.5991676449775696\n",
      "Epoch: 23600\tTrain loss: 0.5369589924812317\n",
      "Epoch: 23650\tTrain loss: 0.48108386993408203\n",
      "Epoch: 23700\tTrain loss: 0.4304799437522888\n",
      "Epoch: 23750\tTrain loss: 0.3873874843120575\n",
      "Epoch: 23800\tTrain loss: 0.3427344262599945\n",
      "Epoch: 23850\tTrain loss: 0.3049146234989166\n",
      "Epoch: 23900\tTrain loss: 0.27075430750846863\n",
      "Epoch: 23950\tTrain loss: 0.24091404676437378\n",
      "Epoch: 24000\tTrain loss: 0.21217308938503265\n",
      "Epoch: 24050\tTrain loss: 0.18722787499427795\n",
      "Epoch: 24100\tTrain loss: 0.16485929489135742\n",
      "Epoch: 24150\tTrain loss: 0.18676628172397614\n",
      "Epoch: 24200\tTrain loss: 0.12707659602165222\n",
      "Epoch: 24250\tTrain loss: 0.11101681739091873\n",
      "Epoch: 24300\tTrain loss: 0.09685694426298141\n",
      "Epoch: 24350\tTrain loss: 0.08430734276771545\n",
      "Epoch: 24400\tTrain loss: 0.10279108583927155\n",
      "Epoch: 24450\tTrain loss: 0.06345352530479431\n",
      "Epoch: 24500\tTrain loss: 0.05473357439041138\n",
      "Epoch: 24550\tTrain loss: 0.0471438467502594\n",
      "Epoch: 24600\tTrain loss: 0.059212349355220795\n",
      "Epoch: 24650\tTrain loss: 0.0348428413271904\n",
      "Epoch: 24700\tTrain loss: 0.029626503586769104\n",
      "Epoch: 24750\tTrain loss: 0.025228865444660187\n",
      "Epoch: 24800\tTrain loss: 0.0345749631524086\n",
      "Epoch: 24850\tTrain loss: 0.01830342784523964\n",
      "Epoch: 24900\tTrain loss: 0.015293458476662636\n",
      "Epoch: 24950\tTrain loss: 0.012864663265645504\n",
      "Training CFRNN\n",
      "Epoch: 0\tTrain loss: 2333946.0\n",
      "Epoch: 50\tTrain loss: 2303077.25\n",
      "Epoch: 100\tTrain loss: 2271499.25\n",
      "Epoch: 150\tTrain loss: 2241772.5\n",
      "Epoch: 200\tTrain loss: 2212960.0\n",
      "Epoch: 250\tTrain loss: 2184776.0\n",
      "Epoch: 300\tTrain loss: 2157096.75\n",
      "Epoch: 350\tTrain loss: 2129857.0\n",
      "Epoch: 400\tTrain loss: 2103017.25\n",
      "Epoch: 450\tTrain loss: 2076550.625\n",
      "Epoch: 500\tTrain loss: 2050438.375\n",
      "Epoch: 550\tTrain loss: 2024665.625\n",
      "Epoch: 600\tTrain loss: 1999221.75\n",
      "Epoch: 650\tTrain loss: 1974097.125\n",
      "Epoch: 700\tTrain loss: 1949283.375\n",
      "Epoch: 750\tTrain loss: 1924774.0\n",
      "Epoch: 800\tTrain loss: 1900563.375\n",
      "Epoch: 850\tTrain loss: 1876645.5\n",
      "Epoch: 900\tTrain loss: 1853015.625\n",
      "Epoch: 950\tTrain loss: 1829669.375\n",
      "Epoch: 1000\tTrain loss: 1806602.25\n",
      "Epoch: 1050\tTrain loss: 1783810.5\n",
      "Epoch: 1100\tTrain loss: 1761290.375\n",
      "Epoch: 1150\tTrain loss: 1739038.25\n",
      "Epoch: 1200\tTrain loss: 1717050.75\n",
      "Epoch: 1250\tTrain loss: 1695324.75\n",
      "Epoch: 1300\tTrain loss: 1673857.375\n",
      "Epoch: 1350\tTrain loss: 1652645.625\n",
      "Epoch: 1400\tTrain loss: 1631686.125\n",
      "Epoch: 1450\tTrain loss: 1610976.75\n",
      "Epoch: 1500\tTrain loss: 1590514.75\n",
      "Epoch: 1550\tTrain loss: 1570297.5\n",
      "Epoch: 1600\tTrain loss: 1550322.25\n",
      "Epoch: 1650\tTrain loss: 1530586.75\n",
      "Epoch: 1700\tTrain loss: 1511088.875\n",
      "Epoch: 1750\tTrain loss: 1491826.5\n",
      "Epoch: 1800\tTrain loss: 1472796.75\n",
      "Epoch: 1850\tTrain loss: 1453997.875\n",
      "Epoch: 1900\tTrain loss: 1435427.875\n",
      "Epoch: 1950\tTrain loss: 1417084.375\n",
      "Epoch: 2000\tTrain loss: 1398965.75\n",
      "Epoch: 2050\tTrain loss: 1381069.875\n",
      "Epoch: 2100\tTrain loss: 1363394.75\n",
      "Epoch: 2150\tTrain loss: 1345938.75\n",
      "Epoch: 2200\tTrain loss: 1328699.75\n",
      "Epoch: 2250\tTrain loss: 1311676.25\n",
      "Epoch: 2300\tTrain loss: 1294866.5\n",
      "Epoch: 2350\tTrain loss: 1278268.75\n",
      "Epoch: 2400\tTrain loss: 1261881.125\n",
      "Epoch: 2450\tTrain loss: 1245702.5\n",
      "Epoch: 2500\tTrain loss: 1229730.75\n",
      "Epoch: 2550\tTrain loss: 1213964.625\n",
      "Epoch: 2600\tTrain loss: 1198402.5\n",
      "Epoch: 2650\tTrain loss: 1183042.875\n",
      "Epoch: 2700\tTrain loss: 1167884.375\n",
      "Epoch: 2750\tTrain loss: 1152925.5\n",
      "Epoch: 2800\tTrain loss: 1138164.75\n",
      "Epoch: 2850\tTrain loss: 1123600.625\n",
      "Epoch: 2900\tTrain loss: 1109232.25\n",
      "Epoch: 2950\tTrain loss: 1095057.75\n",
      "Epoch: 3000\tTrain loss: 1081076.0\n",
      "Epoch: 3050\tTrain loss: 1067285.75\n",
      "Epoch: 3100\tTrain loss: 1053685.625\n",
      "Epoch: 3150\tTrain loss: 1040274.4375\n",
      "Epoch: 3200\tTrain loss: 1027051.0\n",
      "Epoch: 3250\tTrain loss: 1014014.0\n",
      "Epoch: 3300\tTrain loss: 1001162.25\n",
      "Epoch: 3350\tTrain loss: 988494.3125\n",
      "Epoch: 3400\tTrain loss: 976009.25\n",
      "Epoch: 3450\tTrain loss: 963706.125\n",
      "Epoch: 3500\tTrain loss: 951583.4375\n",
      "Epoch: 3550\tTrain loss: 939640.125\n",
      "Epoch: 3600\tTrain loss: 927875.0625\n",
      "Epoch: 3650\tTrain loss: 916287.125\n",
      "Epoch: 3700\tTrain loss: 904875.3125\n",
      "Epoch: 3750\tTrain loss: 893638.375\n",
      "Epoch: 3800\tTrain loss: 882575.1875\n",
      "Epoch: 3850\tTrain loss: 871684.9375\n",
      "Epoch: 3900\tTrain loss: 860966.25\n",
      "Epoch: 3950\tTrain loss: 850418.125\n",
      "Epoch: 4000\tTrain loss: 840039.5625\n",
      "Epoch: 4050\tTrain loss: 829829.625\n",
      "Epoch: 4100\tTrain loss: 819787.0\n",
      "Epoch: 4150\tTrain loss: 809910.75\n",
      "Epoch: 4200\tTrain loss: 800199.8125\n",
      "Epoch: 4250\tTrain loss: 790653.125\n",
      "Epoch: 4300\tTrain loss: 781269.6875\n",
      "Epoch: 4350\tTrain loss: 772048.4375\n",
      "Epoch: 4400\tTrain loss: 762988.25\n",
      "Epoch: 4450\tTrain loss: 754088.25\n",
      "Epoch: 4500\tTrain loss: 745347.1875\n",
      "Epoch: 4550\tTrain loss: 736764.125\n",
      "Epoch: 4600\tTrain loss: 728338.0\n",
      "Epoch: 4650\tTrain loss: 720067.625\n",
      "Epoch: 4700\tTrain loss: 711952.125\n",
      "Epoch: 4750\tTrain loss: 703990.375\n",
      "Epoch: 4800\tTrain loss: 696181.25\n",
      "Epoch: 4850\tTrain loss: 688523.75\n",
      "Epoch: 4900\tTrain loss: 681016.6875\n",
      "Epoch: 4950\tTrain loss: 673659.0\n",
      "Epoch: 5000\tTrain loss: 666449.625\n",
      "Epoch: 5050\tTrain loss: 659387.4375\n",
      "Epoch: 5100\tTrain loss: 652471.375\n",
      "Epoch: 5150\tTrain loss: 645700.3125\n",
      "Epoch: 5200\tTrain loss: 639072.9375\n",
      "Epoch: 5250\tTrain loss: 632588.25\n",
      "Epoch: 5300\tTrain loss: 626245.125\n",
      "Epoch: 5350\tTrain loss: 620042.375\n",
      "Epoch: 5400\tTrain loss: 613978.6875\n",
      "Epoch: 5450\tTrain loss: 608053.0\n",
      "Epoch: 5500\tTrain loss: 602264.1875\n",
      "Epoch: 5550\tTrain loss: 596610.75\n",
      "Epoch: 5600\tTrain loss: 591091.75\n",
      "Epoch: 5650\tTrain loss: 585705.6875\n",
      "Epoch: 5700\tTrain loss: 580451.4375\n",
      "Epoch: 5750\tTrain loss: 575327.6875\n",
      "Epoch: 5800\tTrain loss: 570333.1875\n",
      "Epoch: 5850\tTrain loss: 565466.5625\n",
      "Epoch: 5900\tTrain loss: 560726.4375\n",
      "Epoch: 5950\tTrain loss: 556111.5625\n",
      "Epoch: 6000\tTrain loss: 551620.5625\n",
      "Epoch: 6050\tTrain loss: 547251.9375\n",
      "Epoch: 6100\tTrain loss: 543004.375\n",
      "Epoch: 6150\tTrain loss: 538876.4375\n",
      "Epoch: 6200\tTrain loss: 534866.625\n",
      "Epoch: 6250\tTrain loss: 530973.625\n",
      "Epoch: 6300\tTrain loss: 527195.75\n",
      "Epoch: 6350\tTrain loss: 523531.625\n",
      "Epoch: 6400\tTrain loss: 519979.625\n",
      "Epoch: 6450\tTrain loss: 507448.3125\n",
      "Epoch: 6500\tTrain loss: 490371.0\n",
      "Epoch: 6550\tTrain loss: 468012.75\n",
      "Epoch: 6600\tTrain loss: 446528.96875\n",
      "Epoch: 6650\tTrain loss: 438652.46875\n",
      "Epoch: 6700\tTrain loss: 431133.1875\n",
      "Epoch: 6750\tTrain loss: 423831.6875\n",
      "Epoch: 6800\tTrain loss: 416701.375\n",
      "Epoch: 6850\tTrain loss: 409726.28125\n",
      "Epoch: 6900\tTrain loss: 402897.875\n",
      "Epoch: 6950\tTrain loss: 396208.1875\n",
      "Epoch: 7000\tTrain loss: 389653.5\n",
      "Epoch: 7050\tTrain loss: 383229.65625\n",
      "Epoch: 7100\tTrain loss: 376931.71875\n",
      "Epoch: 7150\tTrain loss: 370756.53125\n",
      "Epoch: 7200\tTrain loss: 364700.0625\n",
      "Epoch: 7250\tTrain loss: 358756.90625\n",
      "Epoch: 7300\tTrain loss: 352913.96875\n",
      "Epoch: 7350\tTrain loss: 346985.34375\n",
      "Epoch: 7400\tTrain loss: 341125.4375\n",
      "Epoch: 7450\tTrain loss: 335341.0\n",
      "Epoch: 7500\tTrain loss: 329622.65625\n",
      "Epoch: 7550\tTrain loss: 323988.75\n",
      "Epoch: 7600\tTrain loss: 318393.25\n",
      "Epoch: 7650\tTrain loss: 312865.5625\n",
      "Epoch: 7700\tTrain loss: 307401.53125\n",
      "Epoch: 7750\tTrain loss: 302000.59375\n",
      "Epoch: 7800\tTrain loss: 296662.28125\n",
      "Epoch: 7850\tTrain loss: 291386.09375\n",
      "Epoch: 7900\tTrain loss: 286171.75\n",
      "Epoch: 7950\tTrain loss: 281018.6875\n",
      "Epoch: 8000\tTrain loss: 275926.65625\n",
      "Epoch: 8050\tTrain loss: 270895.78125\n",
      "Epoch: 8100\tTrain loss: 265927.0\n",
      "Epoch: 8150\tTrain loss: 261015.6875\n",
      "Epoch: 8200\tTrain loss: 256166.40625\n",
      "Epoch: 8250\tTrain loss: 251375.84375\n",
      "Epoch: 8300\tTrain loss: 246644.9375\n",
      "Epoch: 8350\tTrain loss: 241973.40625\n",
      "Epoch: 8400\tTrain loss: 237361.0625\n",
      "Epoch: 8450\tTrain loss: 232807.4375\n",
      "Epoch: 8500\tTrain loss: 229339.203125\n",
      "Epoch: 8550\tTrain loss: 224448.34375\n",
      "Epoch: 8600\tTrain loss: 219429.578125\n",
      "Epoch: 8650\tTrain loss: 214902.765625\n",
      "Epoch: 8700\tTrain loss: 210646.03125\n",
      "Epoch: 8750\tTrain loss: 206449.25\n",
      "Epoch: 8800\tTrain loss: 202310.203125\n",
      "Epoch: 8850\tTrain loss: 198228.03125\n",
      "Epoch: 8900\tTrain loss: 194202.3125\n",
      "Epoch: 8950\tTrain loss: 190232.5625\n",
      "Epoch: 9000\tTrain loss: 186318.375\n",
      "Epoch: 9050\tTrain loss: 182459.375\n",
      "Epoch: 9100\tTrain loss: 178655.234375\n",
      "Epoch: 9150\tTrain loss: 174905.671875\n",
      "Epoch: 9200\tTrain loss: 171210.515625\n",
      "Epoch: 9250\tTrain loss: 167569.609375\n",
      "Epoch: 9300\tTrain loss: 163982.9375\n",
      "Epoch: 9350\tTrain loss: 160450.3125\n",
      "Epoch: 9400\tTrain loss: 156968.484375\n",
      "Epoch: 9450\tTrain loss: 153539.421875\n",
      "Epoch: 9500\tTrain loss: 150162.703125\n",
      "Epoch: 9550\tTrain loss: 146838.59375\n",
      "Epoch: 9600\tTrain loss: 143566.78125\n",
      "Epoch: 9650\tTrain loss: 140347.5625\n",
      "Epoch: 9700\tTrain loss: 137179.59375\n",
      "Epoch: 9750\tTrain loss: 134063.375\n",
      "Epoch: 9800\tTrain loss: 130998.4765625\n",
      "Epoch: 9850\tTrain loss: 127984.6640625\n",
      "Epoch: 9900\tTrain loss: 125021.6796875\n",
      "Epoch: 9950\tTrain loss: 122109.2421875\n",
      "Epoch: 10000\tTrain loss: 119247.0703125\n",
      "Epoch: 10050\tTrain loss: 116434.90625\n",
      "Epoch: 10100\tTrain loss: 113672.453125\n",
      "Epoch: 10150\tTrain loss: 110959.46875\n",
      "Epoch: 10200\tTrain loss: 108295.6171875\n",
      "Epoch: 10250\tTrain loss: 105680.6484375\n",
      "Epoch: 10300\tTrain loss: 103114.2421875\n",
      "Epoch: 10350\tTrain loss: 100596.1171875\n",
      "Epoch: 10400\tTrain loss: 98125.9765625\n",
      "Epoch: 10450\tTrain loss: 95703.578125\n",
      "Epoch: 10500\tTrain loss: 93328.3984375\n",
      "Epoch: 10550\tTrain loss: 91000.3046875\n",
      "Epoch: 10600\tTrain loss: 88718.9609375\n",
      "Epoch: 10650\tTrain loss: 86484.1640625\n",
      "Epoch: 10700\tTrain loss: 84295.21875\n",
      "Epoch: 10750\tTrain loss: 82152.0390625\n",
      "Epoch: 10800\tTrain loss: 80054.265625\n",
      "Epoch: 10850\tTrain loss: 78002.546875\n",
      "Epoch: 10900\tTrain loss: 75994.21875\n",
      "Epoch: 10950\tTrain loss: 74030.7890625\n",
      "Epoch: 11000\tTrain loss: 72112.8671875\n",
      "Epoch: 11050\tTrain loss: 70236.4140625\n",
      "Epoch: 11100\tTrain loss: 68405.1875\n",
      "Epoch: 11150\tTrain loss: 66614.8046875\n",
      "Epoch: 11200\tTrain loss: 64867.078125\n",
      "Epoch: 11250\tTrain loss: 63161.44140625\n",
      "Epoch: 11300\tTrain loss: 61497.47265625\n",
      "Epoch: 11350\tTrain loss: 59874.78515625\n",
      "Epoch: 11400\tTrain loss: 58292.9453125\n",
      "Epoch: 11450\tTrain loss: 56751.546875\n",
      "Epoch: 11500\tTrain loss: 55250.12890625\n",
      "Epoch: 11550\tTrain loss: 53788.28515625\n",
      "Epoch: 11600\tTrain loss: 52365.5546875\n",
      "Epoch: 11650\tTrain loss: 50981.4921875\n",
      "Epoch: 11700\tTrain loss: 49635.6328125\n",
      "Epoch: 11750\tTrain loss: 48327.52734375\n",
      "Epoch: 11800\tTrain loss: 47056.7109375\n",
      "Epoch: 11850\tTrain loss: 45822.71875\n",
      "Epoch: 11900\tTrain loss: 44625.03515625\n",
      "Epoch: 11950\tTrain loss: 43463.29296875\n",
      "Epoch: 12000\tTrain loss: 42336.72265625\n",
      "Epoch: 12050\tTrain loss: 41245.078125\n",
      "Epoch: 12100\tTrain loss: 40187.80859375\n",
      "Epoch: 12150\tTrain loss: 39164.28125\n",
      "Epoch: 12200\tTrain loss: 38174.078125\n",
      "Epoch: 12250\tTrain loss: 37216.64453125\n",
      "Epoch: 12300\tTrain loss: 36291.44140625\n",
      "Epoch: 12350\tTrain loss: 35397.94140625\n",
      "Epoch: 12400\tTrain loss: 34504.3984375\n",
      "Epoch: 12450\tTrain loss: 33594.125\n",
      "Epoch: 12500\tTrain loss: 99956.375\n",
      "Epoch: 12550\tTrain loss: 95584.7109375\n",
      "Epoch: 12600\tTrain loss: 90564.8984375\n",
      "Epoch: 12650\tTrain loss: 62740.15234375\n",
      "Epoch: 12700\tTrain loss: 60644.03515625\n",
      "Epoch: 12750\tTrain loss: 58666.640625\n",
      "Epoch: 12800\tTrain loss: 56777.30859375\n",
      "Epoch: 12850\tTrain loss: 54969.75390625\n",
      "Epoch: 12900\tTrain loss: 53239.328125\n",
      "Epoch: 12950\tTrain loss: 51581.9140625\n",
      "Epoch: 13000\tTrain loss: 49993.8359375\n",
      "Epoch: 13050\tTrain loss: 48471.70703125\n",
      "Epoch: 13100\tTrain loss: 47012.4921875\n",
      "Epoch: 13150\tTrain loss: 45613.328125\n",
      "Epoch: 13200\tTrain loss: 44271.60546875\n",
      "Epoch: 13250\tTrain loss: 42984.8828125\n",
      "Epoch: 13300\tTrain loss: 41750.90625\n",
      "Epoch: 13350\tTrain loss: 40567.55859375\n",
      "Epoch: 13400\tTrain loss: 39432.84375\n",
      "Epoch: 13450\tTrain loss: 38344.92578125\n",
      "Epoch: 13500\tTrain loss: 37302.01953125\n",
      "Epoch: 13550\tTrain loss: 36302.46875\n",
      "Epoch: 13600\tTrain loss: 35344.69921875\n",
      "Epoch: 13650\tTrain loss: 34427.20703125\n",
      "Epoch: 13700\tTrain loss: 33463.55859375\n",
      "Epoch: 13750\tTrain loss: 32522.986328125\n",
      "Epoch: 13800\tTrain loss: 31596.306640625\n",
      "Epoch: 13850\tTrain loss: 30685.330078125\n",
      "Epoch: 13900\tTrain loss: 29790.052734375\n",
      "Epoch: 13950\tTrain loss: 28910.505859375\n",
      "Epoch: 14000\tTrain loss: 28046.666015625\n",
      "Epoch: 14050\tTrain loss: 66892.671875\n",
      "Epoch: 14100\tTrain loss: 63928.76171875\n",
      "Epoch: 14150\tTrain loss: 61192.20703125\n",
      "Epoch: 14200\tTrain loss: 58655.94140625\n",
      "Epoch: 14250\tTrain loss: 56295.57421875\n",
      "Epoch: 14300\tTrain loss: 54091.57421875\n",
      "Epoch: 14350\tTrain loss: 52027.875\n",
      "Epoch: 14400\tTrain loss: 50091.078125\n",
      "Epoch: 14450\tTrain loss: 48269.8828125\n",
      "Epoch: 14500\tTrain loss: 46554.59375\n",
      "Epoch: 14550\tTrain loss: 44936.84765625\n",
      "Epoch: 14600\tTrain loss: 43409.31640625\n",
      "Epoch: 14650\tTrain loss: 41965.56640625\n",
      "Epoch: 14700\tTrain loss: 40599.88671875\n",
      "Epoch: 14750\tTrain loss: 39307.203125\n",
      "Epoch: 14800\tTrain loss: 38082.9296875\n",
      "Epoch: 14850\tTrain loss: 36922.91796875\n",
      "Epoch: 14900\tTrain loss: 35823.421875\n",
      "Epoch: 14950\tTrain loss: 34781.0625\n",
      "Epoch: 15000\tTrain loss: 33792.6484375\n",
      "Epoch: 15050\tTrain loss: 32855.3125\n",
      "Epoch: 15100\tTrain loss: 31966.404296875\n",
      "Epoch: 15150\tTrain loss: 31123.416015625\n",
      "Epoch: 15200\tTrain loss: 30324.076171875\n",
      "Epoch: 15250\tTrain loss: 29566.240234375\n",
      "Epoch: 15300\tTrain loss: 28847.8828125\n",
      "Epoch: 15350\tTrain loss: 28167.1171875\n",
      "Epoch: 15400\tTrain loss: 27522.166015625\n",
      "Epoch: 15450\tTrain loss: 26911.33984375\n",
      "Epoch: 15500\tTrain loss: 26333.04296875\n",
      "Epoch: 15550\tTrain loss: 68786.0546875\n",
      "Epoch: 15600\tTrain loss: 40053.94140625\n",
      "Epoch: 15650\tTrain loss: 38606.484375\n",
      "Epoch: 15700\tTrain loss: 37272.3046875\n",
      "Epoch: 15750\tTrain loss: 36026.96484375\n",
      "Epoch: 15800\tTrain loss: 34862.60546875\n",
      "Epoch: 15850\tTrain loss: 33772.5078125\n",
      "Epoch: 15900\tTrain loss: 32750.806640625\n",
      "Epoch: 15950\tTrain loss: 31792.470703125\n",
      "Epoch: 16000\tTrain loss: 30892.63671875\n",
      "Epoch: 16050\tTrain loss: 30047.392578125\n",
      "Epoch: 16100\tTrain loss: 29253.013671875\n",
      "Epoch: 16150\tTrain loss: 28506.4609375\n",
      "Epoch: 16200\tTrain loss: 27803.5859375\n",
      "Epoch: 16250\tTrain loss: 27140.7265625\n",
      "Epoch: 16300\tTrain loss: 25621.755859375\n",
      "Epoch: 16350\tTrain loss: 24585.7421875\n",
      "Epoch: 16400\tTrain loss: 23665.080078125\n",
      "Epoch: 16450\tTrain loss: 22782.263671875\n",
      "Epoch: 16500\tTrain loss: 21929.677734375\n",
      "Epoch: 16550\tTrain loss: 21105.298828125\n",
      "Epoch: 16600\tTrain loss: 20302.880859375\n",
      "Epoch: 16650\tTrain loss: 19544.482421875\n",
      "Epoch: 16700\tTrain loss: 18800.0703125\n",
      "Epoch: 16750\tTrain loss: 18067.91015625\n",
      "Epoch: 16800\tTrain loss: 17363.306640625\n",
      "Epoch: 16850\tTrain loss: 16681.482421875\n",
      "Epoch: 16900\tTrain loss: 16021.046875\n",
      "Epoch: 16950\tTrain loss: 15381.43359375\n",
      "Epoch: 17000\tTrain loss: 14762.6669921875\n",
      "Epoch: 17050\tTrain loss: 14162.86328125\n",
      "Epoch: 17100\tTrain loss: 13582.3076171875\n",
      "Epoch: 17150\tTrain loss: 13021.71484375\n",
      "Epoch: 17200\tTrain loss: 12477.3876953125\n",
      "Epoch: 17250\tTrain loss: 11953.5556640625\n",
      "Epoch: 17300\tTrain loss: 11442.7568359375\n",
      "Epoch: 17350\tTrain loss: 10951.005859375\n",
      "Epoch: 17400\tTrain loss: 10475.6982421875\n",
      "Epoch: 17450\tTrain loss: 10017.8076171875\n",
      "Epoch: 17500\tTrain loss: 9572.7412109375\n",
      "Epoch: 17550\tTrain loss: 9143.7490234375\n",
      "Epoch: 17600\tTrain loss: 8729.45703125\n",
      "Epoch: 17650\tTrain loss: 8330.0302734375\n",
      "Epoch: 17700\tTrain loss: 7944.7783203125\n",
      "Epoch: 17750\tTrain loss: 7573.50732421875\n",
      "Epoch: 17800\tTrain loss: 7215.8017578125\n",
      "Epoch: 17850\tTrain loss: 6871.3466796875\n",
      "Epoch: 17900\tTrain loss: 6539.7470703125\n",
      "Epoch: 17950\tTrain loss: 6220.7724609375\n",
      "Epoch: 18000\tTrain loss: 5914.1416015625\n",
      "Epoch: 18050\tTrain loss: 5619.552734375\n",
      "Epoch: 18100\tTrain loss: 5336.53857421875\n",
      "Epoch: 18150\tTrain loss: 5064.97607421875\n",
      "Epoch: 18200\tTrain loss: 4804.4384765625\n",
      "Epoch: 18250\tTrain loss: 4554.69921875\n",
      "Epoch: 18300\tTrain loss: 4315.43310546875\n",
      "Epoch: 18350\tTrain loss: 4086.353759765625\n",
      "Epoch: 18400\tTrain loss: 3867.179931640625\n",
      "Epoch: 18450\tTrain loss: 3657.631591796875\n",
      "Epoch: 18500\tTrain loss: 3457.431640625\n",
      "Epoch: 18550\tTrain loss: 3266.300537109375\n",
      "Epoch: 18600\tTrain loss: 3083.9736328125\n",
      "Epoch: 18650\tTrain loss: 2910.183349609375\n",
      "Epoch: 18700\tTrain loss: 2744.672119140625\n",
      "Epoch: 18750\tTrain loss: 2587.1708984375\n",
      "Epoch: 18800\tTrain loss: 2437.42626953125\n",
      "Epoch: 18850\tTrain loss: 2387.081298828125\n",
      "Epoch: 18900\tTrain loss: 2179.362548828125\n",
      "Epoch: 18950\tTrain loss: 2044.527099609375\n",
      "Epoch: 19000\tTrain loss: 1920.2568359375\n",
      "Epoch: 19050\tTrain loss: 1803.72705078125\n",
      "Epoch: 19100\tTrain loss: 1694.053955078125\n",
      "Epoch: 19150\tTrain loss: 1590.742919921875\n",
      "Epoch: 19200\tTrain loss: 1493.43994140625\n",
      "Epoch: 19250\tTrain loss: 1401.841552734375\n",
      "Epoch: 19300\tTrain loss: 1315.6971435546875\n",
      "Epoch: 19350\tTrain loss: 1234.7279052734375\n",
      "Epoch: 19400\tTrain loss: 1158.7225341796875\n",
      "Epoch: 19450\tTrain loss: 1087.5394287109375\n",
      "Epoch: 19500\tTrain loss: 1020.681884765625\n",
      "Epoch: 19550\tTrain loss: 958.2112426757812\n",
      "Epoch: 19600\tTrain loss: 899.9464721679688\n",
      "Epoch: 19650\tTrain loss: 845.3189697265625\n",
      "Epoch: 19700\tTrain loss: 794.4947509765625\n",
      "Epoch: 19750\tTrain loss: 747.1713256835938\n",
      "Epoch: 19800\tTrain loss: 703.1162719726562\n",
      "Epoch: 19850\tTrain loss: 662.1882934570312\n",
      "Epoch: 19900\tTrain loss: 624.187744140625\n",
      "Epoch: 19950\tTrain loss: 588.9462890625\n",
      "Epoch: 20000\tTrain loss: 556.2758178710938\n",
      "Epoch: 20050\tTrain loss: 525.9945068359375\n",
      "Epoch: 20100\tTrain loss: 497.8955078125\n",
      "Epoch: 20150\tTrain loss: 471.7161865234375\n",
      "Epoch: 20200\tTrain loss: 455.3031921386719\n",
      "Epoch: 20250\tTrain loss: 426.8563232421875\n",
      "Epoch: 20300\tTrain loss: 21256.392578125\n",
      "Epoch: 20350\tTrain loss: 789.657470703125\n",
      "Epoch: 20400\tTrain loss: 458.76678466796875\n",
      "Epoch: 20450\tTrain loss: 447.5586853027344\n",
      "Epoch: 20500\tTrain loss: 438.9098815917969\n",
      "Epoch: 20550\tTrain loss: 431.0560302734375\n",
      "Epoch: 20600\tTrain loss: 423.7935485839844\n",
      "Epoch: 20650\tTrain loss: 416.99334716796875\n",
      "Epoch: 20700\tTrain loss: 410.56689453125\n",
      "Epoch: 20750\tTrain loss: 404.4521179199219\n",
      "Epoch: 20800\tTrain loss: 398.60540771484375\n",
      "Epoch: 20850\tTrain loss: 392.9949035644531\n",
      "Epoch: 20900\tTrain loss: 387.593994140625\n",
      "Epoch: 20950\tTrain loss: 382.3773498535156\n",
      "Epoch: 21000\tTrain loss: 377.3227844238281\n",
      "Epoch: 21050\tTrain loss: 372.40899658203125\n",
      "Epoch: 21100\tTrain loss: 367.6185302734375\n",
      "Epoch: 21150\tTrain loss: 362.9342956542969\n",
      "Epoch: 21200\tTrain loss: 358.3433837890625\n",
      "Epoch: 21250\tTrain loss: 353.8330078125\n",
      "Epoch: 21300\tTrain loss: 349.39410400390625\n",
      "Epoch: 21350\tTrain loss: 345.01806640625\n",
      "Epoch: 21400\tTrain loss: 340.6969909667969\n",
      "Epoch: 21450\tTrain loss: 336.42498779296875\n",
      "Epoch: 21500\tTrain loss: 332.1970520019531\n",
      "Epoch: 21550\tTrain loss: 328.0083923339844\n",
      "Epoch: 21600\tTrain loss: 323.85528564453125\n",
      "Epoch: 21650\tTrain loss: 319.8161315917969\n",
      "Epoch: 21700\tTrain loss: 315.6634826660156\n",
      "Epoch: 21750\tTrain loss: 311.591552734375\n",
      "Epoch: 21800\tTrain loss: 307.54791259765625\n",
      "Epoch: 21850\tTrain loss: 303.528564453125\n",
      "Epoch: 21900\tTrain loss: 299.532470703125\n",
      "Epoch: 21950\tTrain loss: 295.55950927734375\n",
      "Epoch: 22000\tTrain loss: 291.6080322265625\n",
      "Epoch: 22050\tTrain loss: 287.6777648925781\n",
      "Epoch: 22100\tTrain loss: 283.7675476074219\n",
      "Epoch: 22150\tTrain loss: 279.8772888183594\n",
      "Epoch: 22200\tTrain loss: 276.0068359375\n",
      "Epoch: 22250\tTrain loss: 272.1551208496094\n",
      "Epoch: 22300\tTrain loss: 268.322509765625\n",
      "Epoch: 22350\tTrain loss: 264.5085754394531\n",
      "Epoch: 22400\tTrain loss: 260.7132263183594\n",
      "Epoch: 22450\tTrain loss: 256.93572998046875\n",
      "Epoch: 22500\tTrain loss: 253.17578125\n",
      "Epoch: 22550\tTrain loss: 249.43348693847656\n",
      "Epoch: 22600\tTrain loss: 245.7079620361328\n",
      "Epoch: 22650\tTrain loss: 241.98838806152344\n",
      "Epoch: 22700\tTrain loss: 238.07064819335938\n",
      "Epoch: 22750\tTrain loss: 234.18399047851562\n",
      "Epoch: 22800\tTrain loss: 230.39434814453125\n",
      "Epoch: 22850\tTrain loss: 226.67208862304688\n",
      "Epoch: 22900\tTrain loss: 223.02435302734375\n",
      "Epoch: 22950\tTrain loss: 231.36936950683594\n",
      "Epoch: 23000\tTrain loss: 218.74488830566406\n",
      "Epoch: 23050\tTrain loss: 215.319091796875\n",
      "Epoch: 23100\tTrain loss: 212.0751495361328\n",
      "Epoch: 23150\tTrain loss: 208.869873046875\n",
      "Epoch: 23200\tTrain loss: 205.70103454589844\n",
      "Epoch: 23250\tTrain loss: 202.5684814453125\n",
      "Epoch: 23300\tTrain loss: 199.47093200683594\n",
      "Epoch: 23350\tTrain loss: 196.40745544433594\n",
      "Epoch: 23400\tTrain loss: 193.3780975341797\n",
      "Epoch: 23450\tTrain loss: 190.38169860839844\n",
      "Epoch: 23500\tTrain loss: 187.41786193847656\n",
      "Epoch: 23550\tTrain loss: 184.4859619140625\n",
      "Epoch: 23600\tTrain loss: 181.58547973632812\n",
      "Epoch: 23650\tTrain loss: 178.71542358398438\n",
      "Epoch: 23700\tTrain loss: 175.8752899169922\n",
      "Epoch: 23750\tTrain loss: 173.0648956298828\n",
      "Epoch: 23800\tTrain loss: 170.28273010253906\n",
      "Epoch: 23850\tTrain loss: 167.52845764160156\n",
      "Epoch: 23900\tTrain loss: 164.801025390625\n",
      "Epoch: 23950\tTrain loss: 162.0999755859375\n",
      "Epoch: 24000\tTrain loss: 159.42376708984375\n",
      "Epoch: 24050\tTrain loss: 156.7723388671875\n",
      "Epoch: 24100\tTrain loss: 154.14492797851562\n",
      "Epoch: 24150\tTrain loss: 151.53976440429688\n",
      "Epoch: 24200\tTrain loss: 148.9568634033203\n",
      "Epoch: 24250\tTrain loss: 146.3943634033203\n",
      "Epoch: 24300\tTrain loss: 143.8521728515625\n",
      "Epoch: 24350\tTrain loss: 141.32940673828125\n",
      "Epoch: 24400\tTrain loss: 138.8249053955078\n",
      "Epoch: 24450\tTrain loss: 136.33816528320312\n",
      "Epoch: 24500\tTrain loss: 133.86813354492188\n",
      "Epoch: 24550\tTrain loss: 132.10308837890625\n",
      "Epoch: 24600\tTrain loss: 129.4746551513672\n",
      "Epoch: 24650\tTrain loss: 127.01444244384766\n",
      "Epoch: 24700\tTrain loss: 124.58528900146484\n",
      "Epoch: 24750\tTrain loss: 122.16885375976562\n",
      "Epoch: 24800\tTrain loss: 119.73931121826172\n",
      "Epoch: 24850\tTrain loss: 114.45425415039062\n",
      "Epoch: 24900\tTrain loss: 109.1114501953125\n",
      "Epoch: 24950\tTrain loss: 104.62261962890625\n",
      "Training CFRNN\n",
      "Epoch: 0\tTrain loss: 6902796.5\n",
      "Epoch: 50\tTrain loss: 6861355.5\n",
      "Epoch: 100\tTrain loss: 6820004.5\n",
      "Epoch: 150\tTrain loss: 6780618.0\n",
      "Epoch: 200\tTrain loss: 6742201.0\n",
      "Epoch: 250\tTrain loss: 6704447.0\n",
      "Epoch: 300\tTrain loss: 6667220.5\n",
      "Epoch: 350\tTrain loss: 6630451.0\n",
      "Epoch: 400\tTrain loss: 6594094.5\n",
      "Epoch: 450\tTrain loss: 6558123.5\n",
      "Epoch: 500\tTrain loss: 6522517.0\n",
      "Epoch: 550\tTrain loss: 6487259.5\n",
      "Epoch: 600\tTrain loss: 6452340.0\n",
      "Epoch: 650\tTrain loss: 6417748.5\n",
      "Epoch: 700\tTrain loss: 6383477.0\n",
      "Epoch: 750\tTrain loss: 6349517.5\n",
      "Epoch: 800\tTrain loss: 6315865.0\n",
      "Epoch: 850\tTrain loss: 6282514.5\n",
      "Epoch: 900\tTrain loss: 6249460.5\n",
      "Epoch: 950\tTrain loss: 6216698.0\n",
      "Epoch: 1000\tTrain loss: 6184224.5\n",
      "Epoch: 1050\tTrain loss: 6152035.0\n",
      "Epoch: 1100\tTrain loss: 6120126.0\n",
      "Epoch: 1150\tTrain loss: 6088495.0\n",
      "Epoch: 1200\tTrain loss: 6057136.5\n",
      "Epoch: 1250\tTrain loss: 6026051.5\n",
      "Epoch: 1300\tTrain loss: 5995233.5\n",
      "Epoch: 1350\tTrain loss: 5964682.0\n",
      "Epoch: 1400\tTrain loss: 5934392.5\n",
      "Epoch: 1450\tTrain loss: 5904363.5\n",
      "Epoch: 1500\tTrain loss: 5874593.0\n",
      "Epoch: 1550\tTrain loss: 5845077.5\n",
      "Epoch: 1600\tTrain loss: 5815815.5\n",
      "Epoch: 1650\tTrain loss: 5786804.0\n",
      "Epoch: 1700\tTrain loss: 5758042.5\n",
      "Epoch: 1750\tTrain loss: 5729527.5\n",
      "Epoch: 1800\tTrain loss: 5701257.0\n",
      "Epoch: 1850\tTrain loss: 5673229.0\n",
      "Epoch: 1900\tTrain loss: 5645443.5\n",
      "Epoch: 1950\tTrain loss: 5617895.5\n",
      "Epoch: 2000\tTrain loss: 5590586.0\n",
      "Epoch: 2050\tTrain loss: 5563512.0\n",
      "Epoch: 2100\tTrain loss: 5536671.0\n",
      "Epoch: 2150\tTrain loss: 5510063.0\n",
      "Epoch: 2200\tTrain loss: 5483684.5\n",
      "Epoch: 2250\tTrain loss: 5457536.0\n",
      "Epoch: 2300\tTrain loss: 5431614.5\n",
      "Epoch: 2350\tTrain loss: 5405919.5\n",
      "Epoch: 2400\tTrain loss: 5380449.0\n",
      "Epoch: 2450\tTrain loss: 5355201.5\n",
      "Epoch: 2500\tTrain loss: 5330175.5\n",
      "Epoch: 2550\tTrain loss: 5305369.5\n",
      "Epoch: 2600\tTrain loss: 5280782.5\n",
      "Epoch: 2650\tTrain loss: 5256414.5\n",
      "Epoch: 2700\tTrain loss: 5232262.0\n",
      "Epoch: 2750\tTrain loss: 5208324.5\n",
      "Epoch: 2800\tTrain loss: 5184602.0\n",
      "Epoch: 2850\tTrain loss: 5161092.5\n",
      "Epoch: 2900\tTrain loss: 5137793.5\n",
      "Epoch: 2950\tTrain loss: 5114705.0\n",
      "Epoch: 3000\tTrain loss: 5091827.0\n",
      "Epoch: 3050\tTrain loss: 5069156.5\n",
      "Epoch: 3100\tTrain loss: 5046694.5\n",
      "Epoch: 3150\tTrain loss: 5024438.0\n",
      "Epoch: 3200\tTrain loss: 5002387.0\n",
      "Epoch: 3250\tTrain loss: 4980540.5\n",
      "Epoch: 3300\tTrain loss: 4958897.5\n",
      "Epoch: 3350\tTrain loss: 4937457.0\n",
      "Epoch: 3400\tTrain loss: 4916218.5\n",
      "Epoch: 3450\tTrain loss: 4895180.0\n",
      "Epoch: 3500\tTrain loss: 4874341.0\n",
      "Epoch: 3550\tTrain loss: 4853700.5\n",
      "Epoch: 3600\tTrain loss: 4833258.5\n",
      "Epoch: 3650\tTrain loss: 4813014.5\n",
      "Epoch: 3700\tTrain loss: 4792966.5\n",
      "Epoch: 3750\tTrain loss: 4773113.5\n",
      "Epoch: 3800\tTrain loss: 4753455.0\n",
      "Epoch: 3850\tTrain loss: 4733991.5\n",
      "Epoch: 3900\tTrain loss: 4714720.0\n",
      "Epoch: 3950\tTrain loss: 4695642.5\n",
      "Epoch: 4000\tTrain loss: 4676755.5\n",
      "Epoch: 4050\tTrain loss: 4658059.5\n",
      "Epoch: 4100\tTrain loss: 4639554.0\n",
      "Epoch: 4150\tTrain loss: 4621238.0\n",
      "Epoch: 4200\tTrain loss: 4603111.0\n",
      "Epoch: 4250\tTrain loss: 4585172.0\n",
      "Epoch: 4300\tTrain loss: 4567419.5\n",
      "Epoch: 4350\tTrain loss: 4549856.0\n",
      "Epoch: 4400\tTrain loss: 4532477.0\n",
      "Epoch: 4450\tTrain loss: 4515283.5\n",
      "Epoch: 4500\tTrain loss: 4498275.0\n",
      "Epoch: 4550\tTrain loss: 4481451.0\n",
      "Epoch: 4600\tTrain loss: 4464810.0\n",
      "Epoch: 4650\tTrain loss: 4448352.0\n",
      "Epoch: 4700\tTrain loss: 4432077.0\n",
      "Epoch: 4750\tTrain loss: 4415982.5\n",
      "Epoch: 4800\tTrain loss: 4400069.0\n",
      "Epoch: 4850\tTrain loss: 4384336.0\n",
      "Epoch: 4900\tTrain loss: 4368783.0\n",
      "Epoch: 4950\tTrain loss: 4353409.0\n",
      "Epoch: 5000\tTrain loss: 4338214.0\n",
      "Epoch: 5050\tTrain loss: 4323196.5\n",
      "Epoch: 5100\tTrain loss: 4308356.5\n",
      "Epoch: 5150\tTrain loss: 4293692.0\n",
      "Epoch: 5200\tTrain loss: 4279205.0\n",
      "Epoch: 5250\tTrain loss: 4264892.5\n",
      "Epoch: 5300\tTrain loss: 4250755.5\n",
      "Epoch: 5350\tTrain loss: 4236792.5\n",
      "Epoch: 5400\tTrain loss: 4223003.0\n",
      "Epoch: 5450\tTrain loss: 4209387.0\n",
      "Epoch: 5500\tTrain loss: 4195943.0\n",
      "Epoch: 5550\tTrain loss: 4182671.0\n",
      "Epoch: 5600\tTrain loss: 4169570.25\n",
      "Epoch: 5650\tTrain loss: 4156639.5\n",
      "Epoch: 5700\tTrain loss: 4143879.75\n",
      "Epoch: 5750\tTrain loss: 4131289.25\n",
      "Epoch: 5800\tTrain loss: 4118866.75\n",
      "Epoch: 5850\tTrain loss: 4106613.25\n",
      "Epoch: 5900\tTrain loss: 4094525.75\n",
      "Epoch: 5950\tTrain loss: 4082606.0\n",
      "Epoch: 6000\tTrain loss: 4070852.75\n",
      "Epoch: 6050\tTrain loss: 4059265.25\n",
      "Epoch: 6100\tTrain loss: 4047842.25\n",
      "Epoch: 6150\tTrain loss: 4036583.25\n",
      "Epoch: 6200\tTrain loss: 4025488.25\n",
      "Epoch: 6250\tTrain loss: 4014556.25\n",
      "Epoch: 6300\tTrain loss: 4003786.25\n",
      "Epoch: 6350\tTrain loss: 3993178.0\n",
      "Epoch: 6400\tTrain loss: 3982730.25\n",
      "Epoch: 6450\tTrain loss: 3972443.25\n",
      "Epoch: 6500\tTrain loss: 3962315.0\n",
      "Epoch: 6550\tTrain loss: 3952346.5\n",
      "Epoch: 6600\tTrain loss: 3942534.75\n",
      "Epoch: 6650\tTrain loss: 3932881.5\n",
      "Epoch: 6700\tTrain loss: 3902354.75\n",
      "Epoch: 6750\tTrain loss: 3857177.5\n",
      "Epoch: 6800\tTrain loss: 3797384.5\n",
      "Epoch: 6850\tTrain loss: 3780104.5\n",
      "Epoch: 6900\tTrain loss: 3763129.25\n",
      "Epoch: 6950\tTrain loss: 3746239.5\n",
      "Epoch: 7000\tTrain loss: 3729437.5\n",
      "Epoch: 7050\tTrain loss: 3712721.0\n",
      "Epoch: 7100\tTrain loss: 3696090.0\n",
      "Epoch: 7150\tTrain loss: 3679542.5\n",
      "Epoch: 7200\tTrain loss: 3663077.5\n",
      "Epoch: 7250\tTrain loss: 3646690.25\n",
      "Epoch: 7300\tTrain loss: 3630379.5\n",
      "Epoch: 7350\tTrain loss: 3614141.5\n",
      "Epoch: 7400\tTrain loss: 3597973.25\n",
      "Epoch: 7450\tTrain loss: 3581874.0\n",
      "Epoch: 7500\tTrain loss: 3565841.0\n",
      "Epoch: 7550\tTrain loss: 3549874.0\n",
      "Epoch: 7600\tTrain loss: 3533970.25\n",
      "Epoch: 7650\tTrain loss: 3518129.0\n",
      "Epoch: 7700\tTrain loss: 3502348.25\n",
      "Epoch: 7750\tTrain loss: 3486627.75\n",
      "Epoch: 7800\tTrain loss: 3470964.25\n",
      "Epoch: 7850\tTrain loss: 3455357.75\n",
      "Epoch: 7900\tTrain loss: 3439807.25\n",
      "Epoch: 7950\tTrain loss: 3424310.5\n",
      "Epoch: 8000\tTrain loss: 3408867.25\n",
      "Epoch: 8050\tTrain loss: 3393475.75\n",
      "Epoch: 8100\tTrain loss: 3378136.0\n",
      "Epoch: 8150\tTrain loss: 3362845.0\n",
      "Epoch: 8200\tTrain loss: 3347605.25\n",
      "Epoch: 8250\tTrain loss: 3332414.5\n",
      "Epoch: 8300\tTrain loss: 3317272.5\n",
      "Epoch: 8350\tTrain loss: 3302178.25\n",
      "Epoch: 8400\tTrain loss: 3287132.25\n",
      "Epoch: 8450\tTrain loss: 3272132.75\n",
      "Epoch: 8500\tTrain loss: 3257179.5\n",
      "Epoch: 8550\tTrain loss: 3707527.25\n",
      "Epoch: 8600\tTrain loss: 3690575.25\n",
      "Epoch: 8650\tTrain loss: 3673743.25\n",
      "Epoch: 8700\tTrain loss: 3657009.0\n",
      "Epoch: 8750\tTrain loss: 3640370.0\n",
      "Epoch: 8800\tTrain loss: 3623821.75\n",
      "Epoch: 8850\tTrain loss: 3607361.75\n",
      "Epoch: 8900\tTrain loss: 3590986.25\n",
      "Epoch: 8950\tTrain loss: 3574693.25\n",
      "Epoch: 9000\tTrain loss: 3558479.25\n",
      "Epoch: 9050\tTrain loss: 3542342.0\n",
      "Epoch: 9100\tTrain loss: 3526279.5\n",
      "Epoch: 9150\tTrain loss: 3510289.5\n",
      "Epoch: 9200\tTrain loss: 3494369.75\n",
      "Epoch: 9250\tTrain loss: 3478518.25\n",
      "Epoch: 9300\tTrain loss: 3462734.0\n",
      "Epoch: 9350\tTrain loss: 3447014.5\n",
      "Epoch: 9400\tTrain loss: 3431357.75\n",
      "Epoch: 9450\tTrain loss: 3415763.5\n",
      "Epoch: 9500\tTrain loss: 3400229.0\n",
      "Epoch: 9550\tTrain loss: 3384753.25\n",
      "Epoch: 9600\tTrain loss: 3369335.0\n",
      "Epoch: 9650\tTrain loss: 3353973.25\n",
      "Epoch: 9700\tTrain loss: 3338666.75\n",
      "Epoch: 9750\tTrain loss: 3323414.0\n",
      "Epoch: 9800\tTrain loss: 3308214.5\n",
      "Epoch: 9850\tTrain loss: 3293067.75\n",
      "Epoch: 9900\tTrain loss: 3277972.0\n",
      "Epoch: 9950\tTrain loss: 3262926.75\n",
      "Epoch: 10000\tTrain loss: 3247931.25\n",
      "Epoch: 10050\tTrain loss: 3232984.5\n",
      "Epoch: 10100\tTrain loss: 3218086.5\n",
      "Epoch: 10150\tTrain loss: 3203235.5\n",
      "Epoch: 10200\tTrain loss: 3188431.5\n",
      "Epoch: 10250\tTrain loss: 3173673.75\n",
      "Epoch: 10300\tTrain loss: 3158962.25\n",
      "Epoch: 10350\tTrain loss: 3144295.75\n",
      "Epoch: 10400\tTrain loss: 3129673.75\n",
      "Epoch: 10450\tTrain loss: 3115095.75\n",
      "Epoch: 10500\tTrain loss: 3100563.25\n",
      "Epoch: 10550\tTrain loss: 3086070.5\n",
      "Epoch: 10600\tTrain loss: 3071622.75\n",
      "Epoch: 10650\tTrain loss: 3057218.0\n",
      "Epoch: 10700\tTrain loss: 3042853.0\n",
      "Epoch: 10750\tTrain loss: 3028531.5\n",
      "Epoch: 10800\tTrain loss: 3014251.5\n",
      "Epoch: 10850\tTrain loss: 3000012.0\n",
      "Epoch: 10900\tTrain loss: 2985813.0\n",
      "Epoch: 10950\tTrain loss: 2971655.25\n",
      "Epoch: 11000\tTrain loss: 2957537.5\n",
      "Epoch: 11050\tTrain loss: 2943459.0\n",
      "Epoch: 11100\tTrain loss: 2929420.25\n",
      "Epoch: 11150\tTrain loss: 2915421.25\n",
      "Epoch: 11200\tTrain loss: 2901461.5\n",
      "Epoch: 11250\tTrain loss: 2887549.5\n",
      "Epoch: 11300\tTrain loss: 2873659.0\n",
      "Epoch: 11350\tTrain loss: 2859815.25\n",
      "Epoch: 11400\tTrain loss: 2846009.75\n",
      "Epoch: 11450\tTrain loss: 2832243.0\n",
      "Epoch: 11500\tTrain loss: 2818514.0\n",
      "Epoch: 11550\tTrain loss: 2804823.5\n",
      "Epoch: 11600\tTrain loss: 2791170.25\n",
      "Epoch: 11650\tTrain loss: 2777554.25\n",
      "Epoch: 11700\tTrain loss: 2763976.0\n",
      "Epoch: 11750\tTrain loss: 2750434.75\n",
      "Epoch: 11800\tTrain loss: 2736931.25\n",
      "Epoch: 11850\tTrain loss: 2723464.75\n",
      "Epoch: 11900\tTrain loss: 2710035.0\n",
      "Epoch: 11950\tTrain loss: 2696642.25\n",
      "Epoch: 12000\tTrain loss: 2683285.75\n",
      "Epoch: 12050\tTrain loss: 2669966.0\n",
      "Epoch: 12100\tTrain loss: 2656695.5\n",
      "Epoch: 12150\tTrain loss: 2643436.5\n",
      "Epoch: 12200\tTrain loss: 2630224.25\n",
      "Epoch: 12250\tTrain loss: 2564121.25\n",
      "Epoch: 12300\tTrain loss: 2551838.25\n",
      "Epoch: 12350\tTrain loss: 2539600.75\n",
      "Epoch: 12400\tTrain loss: 2527397.75\n",
      "Epoch: 12450\tTrain loss: 2515204.0\n",
      "Epoch: 12500\tTrain loss: 2503045.0\n",
      "Epoch: 12550\tTrain loss: 2490912.5\n",
      "Epoch: 12600\tTrain loss: 2478808.0\n",
      "Epoch: 12650\tTrain loss: 2466728.25\n",
      "Epoch: 12700\tTrain loss: 2454677.25\n",
      "Epoch: 12750\tTrain loss: 2442652.75\n",
      "Epoch: 12800\tTrain loss: 2430672.5\n",
      "Epoch: 12850\tTrain loss: 2418689.5\n",
      "Epoch: 12900\tTrain loss: 2406748.5\n",
      "Epoch: 12950\tTrain loss: 2394836.75\n",
      "Epoch: 13000\tTrain loss: 2382955.0\n",
      "Epoch: 13050\tTrain loss: 2371102.5\n",
      "Epoch: 13100\tTrain loss: 2359276.5\n",
      "Epoch: 13150\tTrain loss: 2347474.75\n",
      "Epoch: 13200\tTrain loss: 2335706.0\n",
      "Epoch: 13250\tTrain loss: 2323966.0\n",
      "Epoch: 13300\tTrain loss: 2312255.5\n",
      "Epoch: 13350\tTrain loss: 2300574.0\n",
      "Epoch: 13400\tTrain loss: 2288925.25\n",
      "Epoch: 13450\tTrain loss: 2277301.0\n",
      "Epoch: 13500\tTrain loss: 2265710.0\n",
      "Epoch: 13550\tTrain loss: 2254148.25\n",
      "Epoch: 13600\tTrain loss: 2242613.75\n",
      "Epoch: 13650\tTrain loss: 2231110.5\n",
      "Epoch: 13700\tTrain loss: 2219632.75\n",
      "Epoch: 13750\tTrain loss: 2208192.0\n",
      "Epoch: 13800\tTrain loss: 2196775.75\n",
      "Epoch: 13850\tTrain loss: 2185393.0\n",
      "Epoch: 13900\tTrain loss: 2174038.5\n",
      "Epoch: 13950\tTrain loss: 2162719.25\n",
      "Epoch: 14000\tTrain loss: 2151427.0\n",
      "Epoch: 14050\tTrain loss: 2140160.75\n",
      "Epoch: 14100\tTrain loss: 2128930.25\n",
      "Epoch: 14150\tTrain loss: 2117726.0\n",
      "Epoch: 14200\tTrain loss: 2106557.25\n",
      "Epoch: 14250\tTrain loss: 2095414.375\n",
      "Epoch: 14300\tTrain loss: 2084304.0\n",
      "Epoch: 14350\tTrain loss: 2073227.125\n",
      "Epoch: 14400\tTrain loss: 2062176.875\n",
      "Epoch: 14450\tTrain loss: 2051158.125\n",
      "Epoch: 14500\tTrain loss: 2040170.25\n",
      "Epoch: 14550\tTrain loss: 2029214.5\n",
      "Epoch: 14600\tTrain loss: 2018288.875\n",
      "Epoch: 14650\tTrain loss: 2007392.375\n",
      "Epoch: 14700\tTrain loss: 1996530.75\n",
      "Epoch: 14750\tTrain loss: 1985694.125\n",
      "Epoch: 14800\tTrain loss: 1974899.375\n",
      "Epoch: 14850\tTrain loss: 1964120.375\n",
      "Epoch: 14900\tTrain loss: 1953379.875\n",
      "Epoch: 14950\tTrain loss: 1942669.5\n",
      "Epoch: 15000\tTrain loss: 1931989.75\n",
      "Epoch: 15050\tTrain loss: 1921343.375\n",
      "Epoch: 15100\tTrain loss: 1910724.875\n",
      "Epoch: 15150\tTrain loss: 1900138.625\n",
      "Epoch: 15200\tTrain loss: 1889583.75\n",
      "Epoch: 15250\tTrain loss: 1879059.375\n",
      "Epoch: 15300\tTrain loss: 1868566.375\n",
      "Epoch: 15350\tTrain loss: 1858104.5\n",
      "Epoch: 15400\tTrain loss: 1847673.625\n",
      "Epoch: 15450\tTrain loss: 1837273.625\n",
      "Epoch: 15500\tTrain loss: 1826904.75\n",
      "Epoch: 15550\tTrain loss: 1816569.375\n",
      "Epoch: 15600\tTrain loss: 1806259.875\n",
      "Epoch: 15650\tTrain loss: 1795984.0\n",
      "Epoch: 15700\tTrain loss: 1785739.375\n",
      "Epoch: 15750\tTrain loss: 1775525.625\n",
      "Epoch: 15800\tTrain loss: 1765343.125\n",
      "Epoch: 15850\tTrain loss: 1742246.375\n",
      "Epoch: 15900\tTrain loss: 1731401.0\n",
      "Epoch: 15950\tTrain loss: 1721380.375\n",
      "Epoch: 16000\tTrain loss: 1711390.0\n",
      "Epoch: 16050\tTrain loss: 1701430.125\n",
      "Epoch: 16100\tTrain loss: 1691500.125\n",
      "Epoch: 16150\tTrain loss: 1681601.125\n",
      "Epoch: 16200\tTrain loss: 1671731.625\n",
      "Epoch: 16250\tTrain loss: 1661892.75\n",
      "Epoch: 16300\tTrain loss: 1652084.25\n",
      "Epoch: 16350\tTrain loss: 1642305.875\n",
      "Epoch: 16400\tTrain loss: 1632557.875\n",
      "Epoch: 16450\tTrain loss: 1622840.375\n",
      "Epoch: 16500\tTrain loss: 1613153.125\n",
      "Epoch: 16550\tTrain loss: 1603496.0\n",
      "Epoch: 16600\tTrain loss: 1593869.5\n",
      "Epoch: 16650\tTrain loss: 1584273.125\n",
      "Epoch: 16700\tTrain loss: 1574707.25\n",
      "Epoch: 16750\tTrain loss: 1565171.75\n",
      "Epoch: 16800\tTrain loss: 1555666.5\n",
      "Epoch: 16850\tTrain loss: 1546192.0\n",
      "Epoch: 16900\tTrain loss: 1536747.625\n",
      "Epoch: 16950\tTrain loss: 1527334.125\n",
      "Epoch: 17000\tTrain loss: 1517950.375\n",
      "Epoch: 17050\tTrain loss: 1508597.5\n",
      "Epoch: 17100\tTrain loss: 1499275.125\n",
      "Epoch: 17150\tTrain loss: 1489983.0\n",
      "Epoch: 17200\tTrain loss: 1480722.25\n",
      "Epoch: 17250\tTrain loss: 1471491.625\n",
      "Epoch: 17300\tTrain loss: 1462291.875\n",
      "Epoch: 17350\tTrain loss: 1453123.0\n",
      "Epoch: 17400\tTrain loss: 1443984.0\n",
      "Epoch: 17450\tTrain loss: 1434875.75\n",
      "Epoch: 17500\tTrain loss: 1425798.25\n",
      "Epoch: 17550\tTrain loss: 1416751.5\n",
      "Epoch: 17600\tTrain loss: 1407735.25\n",
      "Epoch: 17650\tTrain loss: 1398749.75\n",
      "Epoch: 17700\tTrain loss: 1389794.5\n",
      "Epoch: 17750\tTrain loss: 1380870.25\n",
      "Epoch: 17800\tTrain loss: 1371976.875\n",
      "Epoch: 17850\tTrain loss: 1363113.875\n",
      "Epoch: 17900\tTrain loss: 1354281.625\n",
      "Epoch: 17950\tTrain loss: 1345479.875\n",
      "Epoch: 18000\tTrain loss: 1336709.125\n",
      "Epoch: 18050\tTrain loss: 1327969.125\n",
      "Epoch: 18100\tTrain loss: 1319259.625\n",
      "Epoch: 18150\tTrain loss: 1310581.125\n",
      "Epoch: 18200\tTrain loss: 1301933.375\n",
      "Epoch: 18250\tTrain loss: 1293316.0\n",
      "Epoch: 18300\tTrain loss: 1284729.5\n",
      "Epoch: 18350\tTrain loss: 1276173.625\n",
      "Epoch: 18400\tTrain loss: 1267648.375\n",
      "Epoch: 18450\tTrain loss: 1259154.375\n",
      "Epoch: 18500\tTrain loss: 1250691.0\n",
      "Epoch: 18550\tTrain loss: 1242258.0\n",
      "Epoch: 18600\tTrain loss: 1233855.75\n",
      "Epoch: 18650\tTrain loss: 1225484.125\n",
      "Epoch: 18700\tTrain loss: 1217143.375\n",
      "Epoch: 18750\tTrain loss: 1208833.625\n",
      "Epoch: 18800\tTrain loss: 1200554.125\n",
      "Epoch: 18850\tTrain loss: 1192305.875\n",
      "Epoch: 18900\tTrain loss: 1184088.125\n",
      "Epoch: 18950\tTrain loss: 1175901.0\n",
      "Epoch: 19000\tTrain loss: 1167747.625\n",
      "Epoch: 19050\tTrain loss: 1159619.375\n",
      "Epoch: 19100\tTrain loss: 1151524.25\n",
      "Epoch: 19150\tTrain loss: 1143460.125\n",
      "Epoch: 19200\tTrain loss: 1135439.375\n",
      "Epoch: 19250\tTrain loss: 1127423.25\n",
      "Epoch: 19300\tTrain loss: 1119451.0\n",
      "Epoch: 19350\tTrain loss: 1111509.5\n",
      "Epoch: 19400\tTrain loss: 1103598.5\n",
      "Epoch: 19450\tTrain loss: 1095719.0\n",
      "Epoch: 19500\tTrain loss: 1087869.125\n",
      "Epoch: 19550\tTrain loss: 1080050.125\n",
      "Epoch: 19600\tTrain loss: 1072261.625\n",
      "Epoch: 19650\tTrain loss: 1064505.625\n",
      "Epoch: 19700\tTrain loss: 1056776.5\n",
      "Epoch: 19750\tTrain loss: 1049080.25\n",
      "Epoch: 19800\tTrain loss: 1041414.375\n",
      "Epoch: 19850\tTrain loss: 1033779.75\n",
      "Epoch: 19900\tTrain loss: 1026174.625\n",
      "Epoch: 19950\tTrain loss: 1018600.5625\n",
      "Epoch: 20000\tTrain loss: 1011056.875\n",
      "Epoch: 20050\tTrain loss: 1003544.125\n",
      "Epoch: 20100\tTrain loss: 996063.0625\n",
      "Epoch: 20150\tTrain loss: 988609.6875\n",
      "Epoch: 20200\tTrain loss: 981188.5\n",
      "Epoch: 20250\tTrain loss: 973797.625\n",
      "Epoch: 20300\tTrain loss: 966437.4375\n",
      "Epoch: 20350\tTrain loss: 959107.75\n",
      "Epoch: 20400\tTrain loss: 951811.1875\n",
      "Epoch: 20450\tTrain loss: 944540.0625\n",
      "Epoch: 20500\tTrain loss: 937301.75\n",
      "Epoch: 20550\tTrain loss: 930093.75\n",
      "Epoch: 20600\tTrain loss: 922917.125\n",
      "Epoch: 20650\tTrain loss: 915769.9375\n",
      "Epoch: 20700\tTrain loss: 908653.4375\n",
      "Epoch: 20750\tTrain loss: 901567.6875\n",
      "Epoch: 20800\tTrain loss: 894512.0\n",
      "Epoch: 20850\tTrain loss: 887486.75\n",
      "Epoch: 20900\tTrain loss: 880492.5\n",
      "Epoch: 20950\tTrain loss: 873527.4375\n",
      "Epoch: 21000\tTrain loss: 866593.625\n",
      "Epoch: 21050\tTrain loss: 859689.875\n",
      "Epoch: 21100\tTrain loss: 852816.8125\n",
      "Epoch: 21150\tTrain loss: 845974.6875\n",
      "Epoch: 21200\tTrain loss: 839161.6875\n",
      "Epoch: 21250\tTrain loss: 832379.0625\n",
      "Epoch: 21300\tTrain loss: 825626.8125\n",
      "Epoch: 21350\tTrain loss: 818905.1875\n",
      "Epoch: 21400\tTrain loss: 812216.5\n",
      "Epoch: 21450\tTrain loss: 805553.1875\n",
      "Epoch: 21500\tTrain loss: 798922.4375\n",
      "Epoch: 21550\tTrain loss: 792321.875\n",
      "Epoch: 21600\tTrain loss: 785751.25\n",
      "Epoch: 21650\tTrain loss: 779211.25\n",
      "Epoch: 21700\tTrain loss: 772701.8125\n",
      "Epoch: 21750\tTrain loss: 766221.625\n",
      "Epoch: 21800\tTrain loss: 759772.0625\n",
      "Epoch: 21850\tTrain loss: 753352.75\n",
      "Epoch: 21900\tTrain loss: 746963.3125\n",
      "Epoch: 21950\tTrain loss: 740611.1875\n",
      "Epoch: 22000\tTrain loss: 734275.6875\n",
      "Epoch: 22050\tTrain loss: 727976.75\n",
      "Epoch: 22100\tTrain loss: 721707.8125\n",
      "Epoch: 22150\tTrain loss: 715469.25\n",
      "Epoch: 22200\tTrain loss: 709260.5625\n",
      "Epoch: 22250\tTrain loss: 703094.375\n",
      "Epoch: 22300\tTrain loss: 696933.5625\n",
      "Epoch: 22350\tTrain loss: 690815.1875\n",
      "Epoch: 22400\tTrain loss: 684726.75\n",
      "Epoch: 22450\tTrain loss: 678668.1875\n",
      "Epoch: 22500\tTrain loss: 672639.625\n",
      "Epoch: 22550\tTrain loss: 666657.3125\n",
      "Epoch: 22600\tTrain loss: 660672.5625\n",
      "Epoch: 22650\tTrain loss: 654733.875\n",
      "Epoch: 22700\tTrain loss: 648825.1875\n",
      "Epoch: 22750\tTrain loss: 642946.375\n",
      "Epoch: 22800\tTrain loss: 637097.25\n",
      "Epoch: 22850\tTrain loss: 631278.3125\n",
      "Epoch: 22900\tTrain loss: 625519.25\n",
      "Epoch: 22950\tTrain loss: 619729.875\n",
      "Epoch: 23000\tTrain loss: 613999.8125\n",
      "Epoch: 23050\tTrain loss: 608300.0\n",
      "Epoch: 23100\tTrain loss: 602629.625\n",
      "Epoch: 23150\tTrain loss: 596999.125\n",
      "Epoch: 23200\tTrain loss: 591379.0\n",
      "Epoch: 23250\tTrain loss: 585798.0625\n",
      "Epoch: 23300\tTrain loss: 580246.625\n",
      "Epoch: 23350\tTrain loss: 574724.9375\n",
      "Epoch: 23400\tTrain loss: 569232.625\n",
      "Epoch: 23450\tTrain loss: 563771.5625\n",
      "Epoch: 23500\tTrain loss: 558338.3125\n",
      "Epoch: 23550\tTrain loss: 552935.1875\n",
      "Epoch: 23600\tTrain loss: 547561.625\n",
      "Epoch: 23650\tTrain loss: 542217.25\n",
      "Epoch: 23700\tTrain loss: 536902.625\n",
      "Epoch: 23750\tTrain loss: 531661.5\n",
      "Epoch: 23800\tTrain loss: 526362.9375\n",
      "Epoch: 23850\tTrain loss: 521136.84375\n",
      "Epoch: 23900\tTrain loss: 515940.09375\n",
      "Epoch: 23950\tTrain loss: 510772.625\n",
      "Epoch: 24000\tTrain loss: 505634.46875\n",
      "Epoch: 24050\tTrain loss: 500546.875\n",
      "Epoch: 24100\tTrain loss: 495448.0625\n",
      "Epoch: 24150\tTrain loss: 490398.09375\n",
      "Epoch: 24200\tTrain loss: 485377.65625\n",
      "Epoch: 24250\tTrain loss: 480386.125\n",
      "Epoch: 24300\tTrain loss: 475423.6875\n",
      "Epoch: 24350\tTrain loss: 470625.96875\n",
      "Epoch: 24400\tTrain loss: 465588.75\n",
      "Epoch: 24450\tTrain loss: 460713.71875\n",
      "Epoch: 24500\tTrain loss: 455867.9375\n",
      "Epoch: 24550\tTrain loss: 451051.25\n",
      "Epoch: 24600\tTrain loss: 446263.8125\n",
      "Epoch: 24650\tTrain loss: 441510.59375\n",
      "Epoch: 24700\tTrain loss: 436777.0\n",
      "Epoch: 24750\tTrain loss: 432076.75\n",
      "Epoch: 24800\tTrain loss: 427405.1875\n",
      "Epoch: 24850\tTrain loss: 422762.40625\n",
      "Epoch: 24900\tTrain loss: 418180.0\n",
      "Epoch: 24950\tTrain loss: 413564.40625\n"
     ]
    }
   ],
   "source": [
    "for baseline in ['CFRNN']:\n",
    "    for seed in range(5):\n",
    "        run_medical_experiments(dataset='electricity', \n",
    "                                baseline=baseline,\n",
    "                                save_model=True, \n",
    "                                save_results=True,\n",
    "                                seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a45a98a5-3175-40ce-9c70-5c0b236300b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CFRNN\n",
      "30.0 \\(\\pm\\) 40.0\\%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for baseline in ['CFRNN']:\n",
    "    print(baseline)\n",
    "    coverages_mean, coverages_std = get_joint_medical_coverages(baseline, 'electricity', seeds=range(5))\n",
    "    \n",
    "    print('{:.1f} \\\\(\\\\pm\\\\) {:.1f}\\\\%'.format(coverages_mean, coverages_std))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4074c07c-9cca-49c8-83b3-156d108530a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CFRNN\n",
      "2664.8687811279297\n",
      "3014.737054807446\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for baseline in ['CFRNN']:\n",
    "    print(baseline)\n",
    "    widths_mean, widths_std = get_medical_interval_widths(baseline, 'electricity', seeds=range(5))\n",
    "    \n",
    "    print(widths_mean)\n",
    "    print(widths_std)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b6d7af-a7c6-46eb-aead-23336fa9a665",
   "metadata": {},
   "source": [
    "## Ablation: Uncorrected calibration scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c58171e-7675-4983-851c-5a963d25aeec",
   "metadata": {},
   "source": [
    "#### Electricity Consumption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "752fa9ad-f6cc-43d3-9fb5-5bfdb071888e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30.0 \\(\\pm\\) 40.0\\%\n"
     ]
    }
   ],
   "source": [
    "coverages_mean, coverages_std = get_joint_medical_coverages('CFRNN', 'electricity', seeds=range(5), correct_conformal=True)\n",
    "    \n",
    "print('{:.1f} \\\\(\\\\pm\\\\) {:.1f}\\\\%'.format(coverages_mean, coverages_std))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "86d905e4-dc31-47ae-9b47-bda58c4c59aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30.0 \\(\\pm\\) 40.0\\%\n"
     ]
    }
   ],
   "source": [
    "coverages_mean, coverages_std = get_joint_medical_coverages('CFRNN', 'electricity', seeds=range(5), correct_conformal=False)\n",
    "    \n",
    "print('{:.1f} \\\\(\\\\pm\\\\) {:.1f}\\\\%'.format(coverages_mean, coverages_std))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "66c5f5a2-4423-4aff-b2e4-91cc7a3ce3be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100.0\\%, 100.0\\%]\n",
      "[0.0\\%, 0.0\\%]\n",
      "[0.0\\%, 50.0\\%]\n",
      "[0.0\\%, 50.0\\%]\n",
      "[50.0\\%, 50.0\\%]\n"
     ]
    }
   ],
   "source": [
    "for seed in range(5):\n",
    "    results = load_medical_results(dataset='electricity', baseline='CFRNN', seed=seed)\n",
    "    independent_coverages = results['Mean independent coverage']\n",
    "    print('[{:.1f}\\\\%, {:.1f}\\\\%]'.format(independent_coverages.min() * 100, independent_coverages.max() * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ab9b269d-13d6-4f21-8848-d6eaeb6bc8cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100.0\\%, 100.0\\%]\n",
      "[0.0\\%, 0.0\\%]\n",
      "[0.0\\%, 50.0\\%]\n",
      "[0.0\\%, 50.0\\%]\n",
      "[50.0\\%, 50.0\\%]\n"
     ]
    }
   ],
   "source": [
    "for seed in range(5):\n",
    "    uncorrected_mimic_results = get_uncorrected_medical_results(dataset='electricity', seed=seed)\n",
    "    independent_coverages = uncorrected_mimic_results['Mean independent coverage']\n",
    "    print('[{:.1f}\\\\%, {:.1f}\\\\%]'.format(independent_coverages.min() * 100, independent_coverages.max() * 100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('lab')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "55cd0a945cf9041a238b2dc7f6f29da44cde9fabaada4bd04363a8e3e2e8be13"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
