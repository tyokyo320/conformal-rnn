{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bfaf9e81-c095-4d66-812e-3459dbc93c65",
   "metadata": {},
   "source": [
    "# Experiments on real-world data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e8602d8b-cb7c-488b-9787-1e624204f8f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from utils.train_medical import run_medical_experiments\n",
    "from utils.results import (\n",
    "    get_joint_medical_coverages, \n",
    "    get_medical_interval_widths, \n",
    "    load_medical_results, \n",
    "    get_uncorrected_medical_results\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9455748f-ff7b-4fa6-8c02-45dfd5509810",
   "metadata": {},
   "source": [
    "To obtain the results as presented in the paper, run the following three sections."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20370c7c-eaca-49aa-8e45-7f91c01c4faa",
   "metadata": {},
   "source": [
    "## Electricity Consumption dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "84cb28a9-05b8-4be6-9656-44cc61953beb",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training CFRNN\n",
      "Epoch: 0\tTrain loss: 1213638.71875\n",
      "Epoch: 50\tTrain loss: 1168497.40625\n",
      "Epoch: 100\tTrain loss: 1127004.1875\n",
      "Epoch: 150\tTrain loss: 1087577.671875\n",
      "Epoch: 200\tTrain loss: 1049781.34375\n",
      "Epoch: 250\tTrain loss: 1013212.828125\n",
      "Epoch: 300\tTrain loss: 977936.875\n",
      "Epoch: 350\tTrain loss: 943840.78125\n",
      "Epoch: 400\tTrain loss: 910861.1875\n",
      "Epoch: 450\tTrain loss: 878956.90625\n",
      "Epoch: 500\tTrain loss: 848265.921875\n",
      "Epoch: 550\tTrain loss: 818477.09375\n",
      "Epoch: 600\tTrain loss: 789696.84375\n",
      "Epoch: 650\tTrain loss: 761832.28125\n",
      "Epoch: 700\tTrain loss: 735018.90625\n",
      "Epoch: 750\tTrain loss: 709002.75\n",
      "Epoch: 800\tTrain loss: 683954.1171875\n",
      "Epoch: 850\tTrain loss: 659686.03125\n",
      "Epoch: 900\tTrain loss: 636332.4921875\n",
      "Epoch: 950\tTrain loss: 613718.375\n",
      "Epoch: 1000\tTrain loss: 591996.15625\n",
      "Epoch: 1050\tTrain loss: 571058.56640625\n",
      "Epoch: 1100\tTrain loss: 550825.78125\n",
      "Epoch: 1150\tTrain loss: 531368.4375\n",
      "Epoch: 1200\tTrain loss: 512665.609375\n",
      "Epoch: 1250\tTrain loss: 494675.640625\n",
      "Epoch: 1300\tTrain loss: 477348.16796875\n",
      "Epoch: 1350\tTrain loss: 460762.125\n",
      "Epoch: 1400\tTrain loss: 444813.40625\n",
      "Epoch: 1450\tTrain loss: 429522.390625\n",
      "Epoch: 1500\tTrain loss: 414864.15234375\n",
      "Epoch: 1550\tTrain loss: 400875.9375\n",
      "Epoch: 1600\tTrain loss: 387473.953125\n",
      "Epoch: 1650\tTrain loss: 374676.015625\n",
      "Epoch: 1700\tTrain loss: 362480.75\n",
      "Epoch: 1750\tTrain loss: 350830.20703125\n",
      "Epoch: 1800\tTrain loss: 339762.99609375\n",
      "Epoch: 1850\tTrain loss: 329273.125\n",
      "Epoch: 1900\tTrain loss: 319275.515625\n",
      "Epoch: 1950\tTrain loss: 309864.859375\n",
      "Epoch: 2000\tTrain loss: 300905.3251953125\n",
      "Epoch: 2050\tTrain loss: 292466.6875\n",
      "Epoch: 2100\tTrain loss: 284570.453125\n",
      "Epoch: 2150\tTrain loss: 277072.921875\n",
      "Epoch: 2200\tTrain loss: 270013.4033203125\n",
      "Epoch: 2250\tTrain loss: 263461.90625\n",
      "Epoch: 2300\tTrain loss: 257299.6484375\n",
      "Epoch: 2350\tTrain loss: 251541.84375\n",
      "Epoch: 2400\tTrain loss: 246158.435546875\n",
      "Epoch: 2450\tTrain loss: 241077.2734375\n",
      "Epoch: 2500\tTrain loss: 236434.3994140625\n",
      "Epoch: 2550\tTrain loss: 232068.09375\n",
      "Epoch: 2600\tTrain loss: 228042.595703125\n",
      "Epoch: 2650\tTrain loss: 224344.27734375\n",
      "Epoch: 2700\tTrain loss: 220967.1484375\n",
      "Epoch: 2750\tTrain loss: 217829.091796875\n",
      "Epoch: 2800\tTrain loss: 215014.15625\n",
      "Epoch: 2850\tTrain loss: 212428.9296875\n",
      "Epoch: 2900\tTrain loss: 210114.64453125\n",
      "Epoch: 2950\tTrain loss: 207980.5625\n",
      "Epoch: 3000\tTrain loss: 196344.859375\n",
      "Epoch: 3050\tTrain loss: 170189.58837890625\n",
      "Epoch: 3100\tTrain loss: 158383.59375\n",
      "Epoch: 3150\tTrain loss: 151392.0859375\n",
      "Epoch: 3200\tTrain loss: 144630.66015625\n",
      "Epoch: 3250\tTrain loss: 138107.51953125\n",
      "Epoch: 3300\tTrain loss: 131736.34765625\n",
      "Epoch: 3350\tTrain loss: 125585.15625\n",
      "Epoch: 3400\tTrain loss: 119685.58529663086\n",
      "Epoch: 3450\tTrain loss: 113932.59680175781\n",
      "Epoch: 3500\tTrain loss: 108363.1015625\n",
      "Epoch: 3550\tTrain loss: 103035.046875\n",
      "Epoch: 3600\tTrain loss: 97872.22720336914\n",
      "Epoch: 3650\tTrain loss: 92906.1640625\n",
      "Epoch: 3700\tTrain loss: 88109.80859375\n",
      "Epoch: 3750\tTrain loss: 83465.5078125\n",
      "Epoch: 3800\tTrain loss: 78992.71875\n",
      "Epoch: 3850\tTrain loss: 74641.14462280273\n",
      "Epoch: 3900\tTrain loss: 70526.96560668945\n",
      "Epoch: 3950\tTrain loss: 66507.57421875\n",
      "Epoch: 4000\tTrain loss: 62662.791015625\n",
      "Epoch: 4050\tTrain loss: 58955.4296875\n",
      "Epoch: 4100\tTrain loss: 55401.408203125\n",
      "Epoch: 4150\tTrain loss: 51998.046875\n",
      "Epoch: 4200\tTrain loss: 48735.405700683594\n",
      "Epoch: 4250\tTrain loss: 45609.375549316406\n",
      "Epoch: 4300\tTrain loss: 42625.86993408203\n",
      "Epoch: 4350\tTrain loss: 39785.154296875\n",
      "Epoch: 4400\tTrain loss: 37065.060546875\n",
      "Epoch: 4450\tTrain loss: 34485.361328125\n",
      "Epoch: 4500\tTrain loss: 32021.330078125\n",
      "Epoch: 4550\tTrain loss: 29683.8056640625\n",
      "Epoch: 4600\tTrain loss: 27477.267852783203\n",
      "Epoch: 4650\tTrain loss: 25360.9189453125\n",
      "Epoch: 4700\tTrain loss: 23386.099639892578\n",
      "Epoch: 4750\tTrain loss: 21511.77459716797\n",
      "Epoch: 4800\tTrain loss: 19735.7109375\n",
      "Epoch: 4850\tTrain loss: 18086.192901611328\n",
      "Epoch: 4900\tTrain loss: 16519.38330078125\n",
      "Epoch: 4950\tTrain loss: 15058.960388183594\n",
      "Epoch: 5000\tTrain loss: 13704.1005859375\n",
      "Epoch: 5050\tTrain loss: 12462.889068603516\n",
      "Epoch: 5100\tTrain loss: 11273.860107421875\n",
      "Epoch: 5150\tTrain loss: 10190.76171875\n",
      "Epoch: 5200\tTrain loss: 9188.966064453125\n",
      "Epoch: 5250\tTrain loss: 8263.963562011719\n",
      "Epoch: 5300\tTrain loss: 7424.7099609375\n",
      "Epoch: 5350\tTrain loss: 6656.215576171875\n",
      "Epoch: 5400\tTrain loss: 5958.4429931640625\n",
      "Epoch: 5450\tTrain loss: 5327.6898193359375\n",
      "Epoch: 5500\tTrain loss: 4755.4794921875\n",
      "Epoch: 5550\tTrain loss: 4404.985382080078\n",
      "Epoch: 5600\tTrain loss: 3841.0779571533203\n",
      "Epoch: 5650\tTrain loss: 3418.12353515625\n",
      "Epoch: 5700\tTrain loss: 3053.9572143554688\n",
      "Epoch: 5750\tTrain loss: 2738.183349609375\n",
      "Epoch: 5800\tTrain loss: 2461.672821044922\n",
      "Epoch: 5850\tTrain loss: 2225.533447265625\n",
      "Epoch: 5900\tTrain loss: 2020.3604125976562\n",
      "Epoch: 5950\tTrain loss: 1847.0692749023438\n",
      "Epoch: 6000\tTrain loss: 1698.8075561523438\n",
      "Epoch: 6050\tTrain loss: 1575.8509826660156\n",
      "Epoch: 6100\tTrain loss: 1472.3619995117188\n",
      "Epoch: 6150\tTrain loss: 1386.6255187988281\n",
      "Epoch: 6200\tTrain loss: 1317.6044311523438\n",
      "Epoch: 6250\tTrain loss: 1261.3550109863281\n",
      "Epoch: 6300\tTrain loss: 1212.884033203125\n",
      "Epoch: 6350\tTrain loss: 1175.2969360351562\n",
      "Epoch: 6400\tTrain loss: 1144.7129821777344\n",
      "Epoch: 6450\tTrain loss: 1122.384521484375\n",
      "Epoch: 6500\tTrain loss: 1102.1194763183594\n",
      "Epoch: 6550\tTrain loss: 1086.4063110351562\n",
      "Epoch: 6600\tTrain loss: 1073.7265930175781\n",
      "Epoch: 6650\tTrain loss: 1063.5328979492188\n",
      "Epoch: 6700\tTrain loss: 1062.5918273925781\n",
      "Epoch: 6750\tTrain loss: 1047.5307922363281\n",
      "Epoch: 6800\tTrain loss: 1040.5462036132812\n",
      "Epoch: 6850\tTrain loss: 1034.5064392089844\n",
      "Epoch: 6900\tTrain loss: 1028.8764343261719\n",
      "Epoch: 6950\tTrain loss: 1023.6775817871094\n",
      "Epoch: 7000\tTrain loss: 1018.8086853027344\n",
      "Epoch: 7050\tTrain loss: 1014.1378173828125\n",
      "Epoch: 7100\tTrain loss: 1010.45849609375\n",
      "Epoch: 7150\tTrain loss: 1004.9336853027344\n",
      "Epoch: 7200\tTrain loss: 1011.6733093261719\n",
      "Epoch: 7250\tTrain loss: 996.3814086914062\n",
      "Epoch: 7300\tTrain loss: 992.2013244628906\n",
      "Epoch: 7350\tTrain loss: 987.7442626953125\n",
      "Epoch: 7400\tTrain loss: 983.9986572265625\n",
      "Epoch: 7450\tTrain loss: 979.5431518554688\n",
      "Epoch: 7500\tTrain loss: 975.7129211425781\n",
      "Epoch: 7550\tTrain loss: 971.7121887207031\n",
      "Epoch: 7600\tTrain loss: 967.4529724121094\n",
      "Epoch: 7650\tTrain loss: 964.1569519042969\n",
      "Epoch: 7700\tTrain loss: 959.5611724853516\n",
      "Epoch: 7750\tTrain loss: 956.5412902832031\n",
      "Epoch: 7800\tTrain loss: 952.4109497070312\n",
      "Epoch: 7850\tTrain loss: 949.6016845703125\n",
      "Epoch: 7900\tTrain loss: 943.9001159667969\n",
      "Epoch: 7950\tTrain loss: 941.5216674804688\n",
      "Epoch: 8000\tTrain loss: 1084.7589416503906\n",
      "Epoch: 8050\tTrain loss: 1034.2011413574219\n",
      "Epoch: 8100\tTrain loss: 1089.7322082519531\n",
      "Epoch: 8150\tTrain loss: 1020.1589050292969\n",
      "Epoch: 8200\tTrain loss: 1007.6795043945312\n",
      "Epoch: 8250\tTrain loss: 996.710693359375\n",
      "Epoch: 8300\tTrain loss: 987.0949249267578\n",
      "Epoch: 8350\tTrain loss: 1032.3955993652344\n",
      "Epoch: 8400\tTrain loss: 983.8627624511719\n",
      "Epoch: 8450\tTrain loss: 978.5246887207031\n",
      "Epoch: 8500\tTrain loss: 972.3828125\n",
      "Epoch: 8550\tTrain loss: 966.0499267578125\n",
      "Epoch: 8600\tTrain loss: 959.8574523925781\n",
      "Epoch: 8650\tTrain loss: 952.4781188964844\n",
      "Epoch: 8700\tTrain loss: 943.1878356933594\n",
      "Epoch: 8750\tTrain loss: 939.6851196289062\n",
      "Epoch: 8800\tTrain loss: 931.3511047363281\n",
      "Epoch: 8850\tTrain loss: 919.1665344238281\n",
      "Epoch: 8900\tTrain loss: 914.7059631347656\n",
      "Epoch: 8950\tTrain loss: 899.9795837402344\n",
      "Epoch: 9000\tTrain loss: 895.098388671875\n",
      "Epoch: 9050\tTrain loss: 880.9965972900391\n",
      "Epoch: 9100\tTrain loss: 873.7852325439453\n",
      "Epoch: 9150\tTrain loss: 880.9747009277344\n",
      "Epoch: 9200\tTrain loss: 863.5279693603516\n",
      "Epoch: 9250\tTrain loss: 849.7486572265625\n",
      "Epoch: 9300\tTrain loss: 838.85791015625\n",
      "Epoch: 9350\tTrain loss: 834.9828643798828\n",
      "Epoch: 9400\tTrain loss: 841.8512268066406\n",
      "Epoch: 9450\tTrain loss: 827.7477416992188\n",
      "Epoch: 9500\tTrain loss: 808.2123947143555\n",
      "Epoch: 9550\tTrain loss: 804.3377685546875\n",
      "Epoch: 9600\tTrain loss: 788.8002624511719\n",
      "Epoch: 9650\tTrain loss: 780.1406555175781\n",
      "Epoch: 9700\tTrain loss: 776.3097229003906\n",
      "Epoch: 9750\tTrain loss: 10775.5341796875\n",
      "Epoch: 9800\tTrain loss: 3570.59326171875\n",
      "Epoch: 9850\tTrain loss: 2505.6168212890625\n",
      "Epoch: 9900\tTrain loss: 1903.1508178710938\n",
      "Epoch: 9950\tTrain loss: 1490.9056091308594\n",
      "Epoch: 10000\tTrain loss: 1310.6952514648438\n",
      "Epoch: 10050\tTrain loss: 1133.401123046875\n",
      "Epoch: 10100\tTrain loss: 1045.4962463378906\n",
      "Epoch: 10150\tTrain loss: 986.0430297851562\n",
      "Epoch: 10200\tTrain loss: 951.6044311523438\n",
      "Epoch: 10250\tTrain loss: 1151.7234954833984\n",
      "Epoch: 10300\tTrain loss: 1112.9313049316406\n",
      "Epoch: 10350\tTrain loss: 1093.2300109863281\n",
      "Epoch: 10400\tTrain loss: 1077.2980346679688\n",
      "Epoch: 10450\tTrain loss: 1064.3131713867188\n",
      "Epoch: 10500\tTrain loss: 1052.2224731445312\n",
      "Epoch: 10550\tTrain loss: 1045.7798614501953\n",
      "Epoch: 10600\tTrain loss: 1031.1143798828125\n",
      "Epoch: 10650\tTrain loss: 1025.1493225097656\n",
      "Epoch: 10700\tTrain loss: 1008.0325622558594\n",
      "Epoch: 10750\tTrain loss: 999.6171569824219\n",
      "Epoch: 10800\tTrain loss: 959.07373046875\n",
      "Epoch: 10850\tTrain loss: 944.3709716796875\n",
      "Epoch: 10900\tTrain loss: 936.5543518066406\n",
      "Epoch: 10950\tTrain loss: 921.9988403320312\n",
      "Epoch: 11000\tTrain loss: 891.0094299316406\n",
      "Epoch: 11050\tTrain loss: 875.7135314941406\n",
      "Epoch: 11100\tTrain loss: 867.8478698730469\n",
      "Epoch: 11150\tTrain loss: 884.52392578125\n",
      "Epoch: 11200\tTrain loss: 856.0330810546875\n",
      "Epoch: 11250\tTrain loss: 850.6634216308594\n",
      "Epoch: 11300\tTrain loss: 845.6722412109375\n",
      "Epoch: 11350\tTrain loss: 846.1582336425781\n",
      "Epoch: 11400\tTrain loss: 836.4446105957031\n",
      "Epoch: 11450\tTrain loss: 831.6778564453125\n",
      "Epoch: 11500\tTrain loss: 827.2088012695312\n",
      "Epoch: 11550\tTrain loss: 847.3431396484375\n",
      "Epoch: 11600\tTrain loss: 839.5992126464844\n",
      "Epoch: 11650\tTrain loss: 828.898681640625\n",
      "Epoch: 11700\tTrain loss: 815.4588317871094\n",
      "Epoch: 11750\tTrain loss: 808.9364624023438\n",
      "Epoch: 11800\tTrain loss: 796.9925537109375\n",
      "Epoch: 11850\tTrain loss: 788.2269668579102\n",
      "Epoch: 11900\tTrain loss: 773.5708923339844\n",
      "Epoch: 11950\tTrain loss: 765.304443359375\n",
      "Epoch: 12000\tTrain loss: 756.2035827636719\n",
      "Epoch: 12050\tTrain loss: 745.3479385375977\n",
      "Epoch: 12100\tTrain loss: 741.7632141113281\n",
      "Epoch: 12150\tTrain loss: 733.93408203125\n",
      "Epoch: 12200\tTrain loss: 1169.8481750488281\n",
      "Epoch: 12250\tTrain loss: 1266.7755432128906\n",
      "Epoch: 12300\tTrain loss: 3970.1116943359375\n",
      "Epoch: 12350\tTrain loss: 1095.380615234375\n",
      "Epoch: 12400\tTrain loss: 987.2306823730469\n",
      "Epoch: 12450\tTrain loss: 1020.6697082519531\n",
      "Epoch: 12500\tTrain loss: 896.7606811523438\n",
      "Epoch: 12550\tTrain loss: 856.0502166748047\n",
      "Epoch: 12600\tTrain loss: 844.7446899414062\n",
      "Epoch: 12650\tTrain loss: 825.7280654907227\n",
      "Epoch: 12700\tTrain loss: 822.4892272949219\n",
      "Epoch: 12750\tTrain loss: 810.9963989257812\n",
      "Epoch: 12800\tTrain loss: 838.3719177246094\n",
      "Epoch: 12850\tTrain loss: 811.9235610961914\n",
      "Epoch: 12900\tTrain loss: 809.0348205566406\n",
      "Epoch: 12950\tTrain loss: 782.7212219238281\n",
      "Epoch: 13000\tTrain loss: 775.7756195068359\n",
      "Epoch: 13050\tTrain loss: 761.0579528808594\n",
      "Epoch: 13100\tTrain loss: 677.8883819580078\n",
      "Epoch: 13150\tTrain loss: 491.70252990722656\n",
      "Epoch: 13200\tTrain loss: 484.07415771484375\n",
      "Epoch: 13250\tTrain loss: 471.5660095214844\n",
      "Epoch: 13300\tTrain loss: 486.0608673095703\n",
      "Epoch: 13350\tTrain loss: 472.4460906982422\n",
      "Epoch: 13400\tTrain loss: 452.2274932861328\n",
      "Epoch: 13450\tTrain loss: 448.9548645019531\n",
      "Epoch: 13500\tTrain loss: 441.37245178222656\n",
      "Epoch: 13550\tTrain loss: 476.30162048339844\n",
      "Epoch: 13600\tTrain loss: 436.7477111816406\n",
      "Epoch: 13650\tTrain loss: 425.12471771240234\n",
      "Epoch: 13700\tTrain loss: 413.84693908691406\n",
      "Epoch: 13750\tTrain loss: 435.2739562988281\n",
      "Epoch: 13800\tTrain loss: 422.7044906616211\n",
      "Epoch: 13850\tTrain loss: 401.7076721191406\n",
      "Epoch: 13900\tTrain loss: 391.6390075683594\n",
      "Epoch: 13950\tTrain loss: 384.4670715332031\n",
      "Epoch: 14000\tTrain loss: 385.096923828125\n",
      "Epoch: 14050\tTrain loss: 379.6621551513672\n",
      "Epoch: 14100\tTrain loss: 365.4842224121094\n",
      "Epoch: 14150\tTrain loss: 598.0914001464844\n",
      "Epoch: 14200\tTrain loss: 564.8299865722656\n",
      "Epoch: 14250\tTrain loss: 517.7301940917969\n",
      "Epoch: 14300\tTrain loss: 499.23614501953125\n",
      "Epoch: 14350\tTrain loss: 484.4771423339844\n",
      "Epoch: 14400\tTrain loss: 471.8482360839844\n",
      "Epoch: 14450\tTrain loss: 461.39491271972656\n",
      "Epoch: 14500\tTrain loss: 449.8472595214844\n",
      "Epoch: 14550\tTrain loss: 440.52781677246094\n",
      "Epoch: 14600\tTrain loss: 1121.5601654052734\n",
      "Epoch: 14650\tTrain loss: 898.5044555664062\n",
      "Epoch: 14700\tTrain loss: 846.0614624023438\n",
      "Epoch: 14750\tTrain loss: 825.5503082275391\n",
      "Epoch: 14800\tTrain loss: 3134.1243896484375\n",
      "Epoch: 14850\tTrain loss: 6316.87939453125\n",
      "Epoch: 14900\tTrain loss: 958.8441009521484\n",
      "Epoch: 14950\tTrain loss: 1165.941650390625\n",
      "Epoch: 15000\tTrain loss: 1075.4208374023438\n",
      "Epoch: 15050\tTrain loss: 1055.5477905273438\n",
      "Epoch: 15100\tTrain loss: 1039.7930908203125\n",
      "Epoch: 15150\tTrain loss: 1026.7756958007812\n",
      "Epoch: 15200\tTrain loss: 1110.792724609375\n",
      "Epoch: 15250\tTrain loss: 968.2740783691406\n",
      "Epoch: 15300\tTrain loss: 951.5726928710938\n",
      "Epoch: 15350\tTrain loss: 872.9750366210938\n",
      "Epoch: 15400\tTrain loss: 866.9342041015625\n",
      "Epoch: 15450\tTrain loss: 863.6405639648438\n",
      "Epoch: 15500\tTrain loss: 712.2221069335938\n",
      "Epoch: 15550\tTrain loss: 876.5132446289062\n",
      "Epoch: 15600\tTrain loss: 855.3893432617188\n",
      "Epoch: 15650\tTrain loss: 841.0718994140625\n",
      "Epoch: 15700\tTrain loss: 828.1106262207031\n",
      "Epoch: 15750\tTrain loss: 813.0493774414062\n",
      "Epoch: 15800\tTrain loss: 787.224609375\n",
      "Epoch: 15850\tTrain loss: 823.3609008789062\n",
      "Epoch: 15900\tTrain loss: 794.1342468261719\n",
      "Epoch: 15950\tTrain loss: 781.3799743652344\n",
      "Epoch: 16000\tTrain loss: 539.7246170043945\n",
      "Epoch: 16050\tTrain loss: 461.4718017578125\n",
      "Epoch: 16100\tTrain loss: 425.9566650390625\n",
      "Epoch: 16150\tTrain loss: 409.82432556152344\n",
      "Epoch: 16200\tTrain loss: 394.97889709472656\n",
      "Epoch: 16250\tTrain loss: 397.57118225097656\n",
      "Epoch: 16300\tTrain loss: 379.22454833984375\n",
      "Epoch: 16350\tTrain loss: 372.2273406982422\n",
      "Epoch: 16400\tTrain loss: 370.0949401855469\n",
      "Epoch: 16450\tTrain loss: 361.9608612060547\n",
      "Epoch: 16500\tTrain loss: 436.4732971191406\n",
      "Epoch: 16550\tTrain loss: 396.0078887939453\n",
      "Epoch: 16600\tTrain loss: 378.42152404785156\n",
      "Epoch: 16650\tTrain loss: 411.8139953613281\n",
      "Epoch: 16700\tTrain loss: 389.1861114501953\n",
      "Epoch: 16750\tTrain loss: 378.7927551269531\n",
      "Epoch: 16800\tTrain loss: 371.97496032714844\n",
      "Epoch: 16850\tTrain loss: 366.12449645996094\n",
      "Epoch: 16900\tTrain loss: 360.37109375\n",
      "Epoch: 16950\tTrain loss: 354.2273178100586\n",
      "Epoch: 17000\tTrain loss: 347.9404296875\n",
      "Epoch: 17050\tTrain loss: 341.11936950683594\n",
      "Epoch: 17100\tTrain loss: 333.6135711669922\n",
      "Epoch: 17150\tTrain loss: 330.5701904296875\n",
      "Epoch: 17200\tTrain loss: 322.1748352050781\n",
      "Epoch: 17250\tTrain loss: 313.8031005859375\n",
      "Epoch: 17300\tTrain loss: 460.6469268798828\n",
      "Epoch: 17350\tTrain loss: 321.34185791015625\n",
      "Epoch: 17400\tTrain loss: 310.6841125488281\n",
      "Epoch: 17450\tTrain loss: 302.2278366088867\n",
      "Epoch: 17500\tTrain loss: 295.42156982421875\n",
      "Epoch: 17550\tTrain loss: 288.8133773803711\n",
      "Epoch: 17600\tTrain loss: 285.516357421875\n",
      "Epoch: 17650\tTrain loss: 280.87376403808594\n",
      "Epoch: 17700\tTrain loss: 273.88536071777344\n",
      "Epoch: 17750\tTrain loss: 268.7604293823242\n",
      "Epoch: 17800\tTrain loss: 264.138370513916\n",
      "Epoch: 17850\tTrain loss: 259.9272003173828\n",
      "Epoch: 17900\tTrain loss: 255.7422637939453\n",
      "Epoch: 17950\tTrain loss: 251.8764877319336\n",
      "Epoch: 18000\tTrain loss: 248.0404052734375\n",
      "Epoch: 18050\tTrain loss: 244.3376235961914\n",
      "Epoch: 18100\tTrain loss: 240.56239318847656\n",
      "Epoch: 18150\tTrain loss: 236.97416305541992\n",
      "Epoch: 18200\tTrain loss: 233.3947296142578\n",
      "Epoch: 18250\tTrain loss: 229.8050079345703\n",
      "Epoch: 18300\tTrain loss: 225.8933563232422\n",
      "Epoch: 18350\tTrain loss: 221.45720672607422\n",
      "Epoch: 18400\tTrain loss: 1278.0077819824219\n",
      "Epoch: 18450\tTrain loss: 242.33715057373047\n",
      "Epoch: 18500\tTrain loss: 1007.9886093139648\n",
      "Epoch: 18550\tTrain loss: 209.51132202148438\n",
      "Epoch: 18600\tTrain loss: 203.95045471191406\n",
      "Epoch: 18650\tTrain loss: 189.41014862060547\n",
      "Epoch: 18700\tTrain loss: 183.53504943847656\n",
      "Epoch: 18750\tTrain loss: 175.33123779296875\n",
      "Epoch: 18800\tTrain loss: 170.0483856201172\n",
      "Epoch: 18850\tTrain loss: 1460.44287109375\n",
      "Epoch: 18900\tTrain loss: 906.4124450683594\n",
      "Epoch: 18950\tTrain loss: 385.34112548828125\n",
      "Epoch: 19000\tTrain loss: 244.3705291748047\n",
      "Epoch: 19050\tTrain loss: 210.91431427001953\n",
      "Epoch: 19100\tTrain loss: 187.51663208007812\n",
      "Epoch: 19150\tTrain loss: 170.2454833984375\n",
      "Epoch: 19200\tTrain loss: 317.0075378417969\n",
      "Epoch: 19250\tTrain loss: 248.8931121826172\n",
      "Epoch: 19300\tTrain loss: 232.70980834960938\n",
      "Epoch: 19350\tTrain loss: 218.5218276977539\n",
      "Epoch: 19400\tTrain loss: 204.6616096496582\n",
      "Epoch: 19450\tTrain loss: 192.87506866455078\n",
      "Epoch: 19500\tTrain loss: 184.5391082763672\n",
      "Epoch: 19550\tTrain loss: 177.327392578125\n",
      "Epoch: 19600\tTrain loss: 170.59024810791016\n",
      "Epoch: 19650\tTrain loss: 163.8003387451172\n",
      "Epoch: 19700\tTrain loss: 156.76208114624023\n",
      "Epoch: 19750\tTrain loss: 149.45596313476562\n",
      "Epoch: 19800\tTrain loss: 141.73120498657227\n",
      "Epoch: 19850\tTrain loss: 133.4465560913086\n",
      "Epoch: 19900\tTrain loss: 124.93567085266113\n",
      "Epoch: 19950\tTrain loss: 117.1982421875\n",
      "Epoch: 20000\tTrain loss: 110.26696586608887\n",
      "Epoch: 20050\tTrain loss: 104.08565902709961\n",
      "Epoch: 20100\tTrain loss: 859.7668762207031\n",
      "Epoch: 20150\tTrain loss: 802.5774917602539\n",
      "Epoch: 20200\tTrain loss: 671.3549423217773\n",
      "Epoch: 20250\tTrain loss: 862.7386627197266\n",
      "Epoch: 20300\tTrain loss: 765.5289916992188\n",
      "Epoch: 20350\tTrain loss: 3165.7399291992188\n",
      "Epoch: 20400\tTrain loss: 1314.7330017089844\n",
      "Epoch: 20450\tTrain loss: 1199.3242797851562\n",
      "Epoch: 20500\tTrain loss: 1147.8682556152344\n",
      "Epoch: 20550\tTrain loss: 1104.8176574707031\n",
      "Epoch: 20600\tTrain loss: 1061.5770568847656\n",
      "Epoch: 20650\tTrain loss: 1017.3475646972656\n",
      "Epoch: 20700\tTrain loss: 987.8035583496094\n",
      "Epoch: 20750\tTrain loss: 967.4596862792969\n",
      "Epoch: 20800\tTrain loss: 949.4319458007812\n",
      "Epoch: 20850\tTrain loss: 933.7439880371094\n",
      "Epoch: 20900\tTrain loss: 919.5482177734375\n",
      "Epoch: 20950\tTrain loss: 907.2249755859375\n",
      "Epoch: 21000\tTrain loss: 896.1395874023438\n",
      "Epoch: 21050\tTrain loss: 886.8318176269531\n",
      "Epoch: 21100\tTrain loss: 878.485107421875\n",
      "Epoch: 21150\tTrain loss: 870.961669921875\n",
      "Epoch: 21200\tTrain loss: 864.3307800292969\n",
      "Epoch: 21250\tTrain loss: 858.0848693847656\n",
      "Epoch: 21300\tTrain loss: 852.1370239257812\n",
      "Epoch: 21350\tTrain loss: 847.3703308105469\n",
      "Epoch: 21400\tTrain loss: 842.2542877197266\n",
      "Epoch: 21450\tTrain loss: 838.1532897949219\n",
      "Epoch: 21500\tTrain loss: 833.7277679443359\n",
      "Epoch: 21550\tTrain loss: 829.9689559936523\n",
      "Epoch: 21600\tTrain loss: 826.452018737793\n",
      "Epoch: 21650\tTrain loss: 823.3023071289062\n",
      "Epoch: 21700\tTrain loss: 820.1214904785156\n",
      "Epoch: 21750\tTrain loss: 817.3272094726562\n",
      "Epoch: 21800\tTrain loss: 815.0961303710938\n",
      "Epoch: 21850\tTrain loss: 812.5641174316406\n",
      "Epoch: 21900\tTrain loss: 809.9480895996094\n",
      "Epoch: 21950\tTrain loss: 808.0994567871094\n",
      "Epoch: 22000\tTrain loss: 805.7149047851562\n",
      "Epoch: 22050\tTrain loss: 803.8374633789062\n",
      "Epoch: 22100\tTrain loss: 801.938850402832\n",
      "Epoch: 22150\tTrain loss: 800.4364929199219\n",
      "Epoch: 22200\tTrain loss: 798.4572448730469\n",
      "Epoch: 22250\tTrain loss: 796.9891052246094\n",
      "Epoch: 22300\tTrain loss: 795.3344116210938\n",
      "Epoch: 22350\tTrain loss: 793.8782958984375\n",
      "Epoch: 22400\tTrain loss: 792.286865234375\n",
      "Epoch: 22450\tTrain loss: 790.9027709960938\n",
      "Epoch: 22500\tTrain loss: 789.1681976318359\n",
      "Epoch: 22550\tTrain loss: 787.7218933105469\n",
      "Epoch: 22600\tTrain loss: 786.4773254394531\n",
      "Epoch: 22650\tTrain loss: 785.0310821533203\n",
      "Epoch: 22700\tTrain loss: 784.1502990722656\n",
      "Epoch: 22750\tTrain loss: 782.8906555175781\n",
      "Epoch: 22800\tTrain loss: 781.6110229492188\n",
      "Epoch: 22850\tTrain loss: 780.0183639526367\n",
      "Epoch: 22900\tTrain loss: 778.9493713378906\n",
      "Epoch: 22950\tTrain loss: 777.6895751953125\n",
      "Epoch: 23000\tTrain loss: 776.2257385253906\n",
      "Epoch: 23050\tTrain loss: 774.271484375\n",
      "Epoch: 23100\tTrain loss: 773.2210998535156\n",
      "Epoch: 23150\tTrain loss: 809.8121948242188\n",
      "Epoch: 23200\tTrain loss: 802.7803039550781\n",
      "Epoch: 23250\tTrain loss: 782.9168090820312\n",
      "Epoch: 23300\tTrain loss: 775.3255004882812\n",
      "Epoch: 23350\tTrain loss: 768.7207946777344\n",
      "Epoch: 23400\tTrain loss: 762.6112976074219\n",
      "Epoch: 23450\tTrain loss: 756.0037384033203\n",
      "Epoch: 23500\tTrain loss: 752.6243286132812\n",
      "Epoch: 23550\tTrain loss: 746.4745483398438\n",
      "Epoch: 23600\tTrain loss: 743.9013977050781\n",
      "Epoch: 23650\tTrain loss: 749.4231262207031\n",
      "Epoch: 23700\tTrain loss: 736.778564453125\n",
      "Epoch: 23750\tTrain loss: 734.3464584350586\n",
      "Epoch: 23800\tTrain loss: 728.95361328125\n",
      "Epoch: 23850\tTrain loss: 724.4075927734375\n",
      "Epoch: 23900\tTrain loss: 719.8191833496094\n",
      "Epoch: 23950\tTrain loss: 714.5546264648438\n",
      "Epoch: 24000\tTrain loss: 708.8087158203125\n",
      "Epoch: 24050\tTrain loss: 775.337158203125\n",
      "Epoch: 24100\tTrain loss: 775.5534057617188\n",
      "Epoch: 24150\tTrain loss: 834.8988723754883\n",
      "Epoch: 24200\tTrain loss: 732.7237968444824\n",
      "Epoch: 24250\tTrain loss: 700.1082153320312\n",
      "Epoch: 24300\tTrain loss: 696.6618041992188\n",
      "Epoch: 24350\tTrain loss: 690.8940658569336\n",
      "Epoch: 24400\tTrain loss: 737.9853515625\n",
      "Epoch: 24450\tTrain loss: 678.9785079956055\n",
      "Epoch: 24500\tTrain loss: 383.75146484375\n",
      "Epoch: 24550\tTrain loss: 366.3562469482422\n",
      "Epoch: 24600\tTrain loss: 359.2771759033203\n",
      "Epoch: 24650\tTrain loss: 342.32139587402344\n",
      "Epoch: 24700\tTrain loss: 337.0819625854492\n",
      "Epoch: 24750\tTrain loss: 317.07911682128906\n",
      "Epoch: 24800\tTrain loss: 301.9549102783203\n",
      "Epoch: 24850\tTrain loss: 287.72203826904297\n",
      "Epoch: 24900\tTrain loss: 283.48169708251953\n",
      "Epoch: 24950\tTrain loss: 269.5100154876709\n",
      "Epoch: 25000\tTrain loss: 258.47786712646484\n",
      "Epoch: 25050\tTrain loss: 249.24522972106934\n",
      "Epoch: 25100\tTrain loss: 242.32604217529297\n",
      "Epoch: 25150\tTrain loss: 236.451416015625\n",
      "Epoch: 25200\tTrain loss: 228.9190444946289\n",
      "Epoch: 25250\tTrain loss: 224.3773422241211\n",
      "Epoch: 25300\tTrain loss: 216.51126098632812\n",
      "Epoch: 25350\tTrain loss: 203.59848022460938\n",
      "Epoch: 25400\tTrain loss: 175.5152473449707\n",
      "Epoch: 25450\tTrain loss: 161.71113204956055\n",
      "Epoch: 25500\tTrain loss: 152.66153526306152\n",
      "Epoch: 25550\tTrain loss: 144.40446281433105\n",
      "Epoch: 25600\tTrain loss: 136.7542953491211\n",
      "Epoch: 25650\tTrain loss: 128.11535930633545\n",
      "Epoch: 25700\tTrain loss: 120.93626403808594\n",
      "Epoch: 25750\tTrain loss: 113.20100355148315\n",
      "Epoch: 25800\tTrain loss: 106.39755439758301\n",
      "Epoch: 25850\tTrain loss: 99.90250396728516\n",
      "Epoch: 25900\tTrain loss: 95.52746963500977\n",
      "Epoch: 25950\tTrain loss: 1287.1827392578125\n",
      "Epoch: 26000\tTrain loss: 952.9132690429688\n",
      "Epoch: 26050\tTrain loss: 910.2608032226562\n",
      "Epoch: 26100\tTrain loss: 877.8250274658203\n",
      "Epoch: 26150\tTrain loss: 857.2372741699219\n",
      "Epoch: 26200\tTrain loss: 836.8383483886719\n",
      "Epoch: 26250\tTrain loss: 807.8351898193359\n",
      "Epoch: 26300\tTrain loss: 786.5629577636719\n",
      "Epoch: 26350\tTrain loss: 779.0869445800781\n",
      "Epoch: 26400\tTrain loss: 5757.8759765625\n",
      "Epoch: 26450\tTrain loss: 1419.9816589355469\n",
      "Epoch: 26500\tTrain loss: 1223.8730163574219\n",
      "Epoch: 26550\tTrain loss: 1144.7563171386719\n",
      "Epoch: 26600\tTrain loss: 1088.5077514648438\n",
      "Epoch: 26650\tTrain loss: 1109.7496643066406\n",
      "Epoch: 26700\tTrain loss: 1048.1047973632812\n",
      "Epoch: 26750\tTrain loss: 1011.1942138671875\n",
      "Epoch: 26800\tTrain loss: 998.0245361328125\n",
      "Epoch: 26850\tTrain loss: 972.5220642089844\n",
      "Epoch: 26900\tTrain loss: 956.713134765625\n",
      "Epoch: 26950\tTrain loss: 943.3450012207031\n",
      "Epoch: 27000\tTrain loss: 933.3147583007812\n",
      "Epoch: 27050\tTrain loss: 925.8880310058594\n",
      "Epoch: 27100\tTrain loss: 919.1059875488281\n",
      "Epoch: 27150\tTrain loss: 912.5619812011719\n",
      "Epoch: 27200\tTrain loss: 907.8003845214844\n",
      "Epoch: 27250\tTrain loss: 902.5921630859375\n",
      "Epoch: 27300\tTrain loss: 897.9631652832031\n",
      "Epoch: 27350\tTrain loss: 893.6380920410156\n",
      "Epoch: 27400\tTrain loss: 889.3953857421875\n",
      "Epoch: 27450\tTrain loss: 884.907958984375\n",
      "Epoch: 27500\tTrain loss: 879.3648986816406\n",
      "Epoch: 27550\tTrain loss: 876.1070251464844\n",
      "Epoch: 27600\tTrain loss: 870.1552734375\n",
      "Epoch: 27650\tTrain loss: 867.1155090332031\n",
      "Epoch: 27700\tTrain loss: 862.7595520019531\n",
      "Epoch: 27750\tTrain loss: 856.3193359375\n",
      "Epoch: 27800\tTrain loss: 850.1007232666016\n",
      "Epoch: 27850\tTrain loss: 844.2270812988281\n",
      "Epoch: 27900\tTrain loss: 837.7525482177734\n",
      "Epoch: 27950\tTrain loss: 833.4782104492188\n",
      "Epoch: 28000\tTrain loss: 823.4403228759766\n",
      "Epoch: 28050\tTrain loss: 818.833251953125\n",
      "Epoch: 28100\tTrain loss: 807.8059844970703\n",
      "Epoch: 28150\tTrain loss: 798.540771484375\n",
      "Epoch: 28200\tTrain loss: 787.1469116210938\n",
      "Epoch: 28250\tTrain loss: 773.8865966796875\n",
      "Epoch: 28300\tTrain loss: 737.9371948242188\n",
      "Epoch: 28350\tTrain loss: 992.1139068603516\n",
      "Epoch: 28400\tTrain loss: 441.29193115234375\n",
      "Epoch: 28450\tTrain loss: 419.58924102783203\n",
      "Epoch: 28500\tTrain loss: 374.051513671875\n",
      "Epoch: 28550\tTrain loss: 352.1925354003906\n",
      "Epoch: 28600\tTrain loss: 332.0760269165039\n",
      "Epoch: 28650\tTrain loss: 308.79551696777344\n",
      "Epoch: 28700\tTrain loss: 288.5144958496094\n",
      "Epoch: 28750\tTrain loss: 272.2585220336914\n",
      "Epoch: 28800\tTrain loss: 242.39891815185547\n",
      "Epoch: 28850\tTrain loss: 279.3628158569336\n",
      "Epoch: 28900\tTrain loss: 268.24007415771484\n",
      "Epoch: 28950\tTrain loss: 259.1334915161133\n",
      "Epoch: 29000\tTrain loss: 248.22421264648438\n",
      "Epoch: 29050\tTrain loss: 240.0128173828125\n",
      "Epoch: 29100\tTrain loss: 232.64305114746094\n",
      "Epoch: 29150\tTrain loss: 224.8686294555664\n",
      "Epoch: 29200\tTrain loss: 217.52496337890625\n",
      "Epoch: 29250\tTrain loss: 210.64716339111328\n",
      "Epoch: 29300\tTrain loss: 205.35133361816406\n",
      "Epoch: 29350\tTrain loss: 195.6309051513672\n",
      "Epoch: 29400\tTrain loss: 186.65191650390625\n",
      "Epoch: 29450\tTrain loss: 4363.0692138671875\n",
      "Epoch: 29500\tTrain loss: 784.227783203125\n",
      "Epoch: 29550\tTrain loss: 993.4665832519531\n",
      "Epoch: 29600\tTrain loss: 733.6943511962891\n",
      "Epoch: 29650\tTrain loss: 479.80448150634766\n",
      "Epoch: 29700\tTrain loss: 450.635986328125\n",
      "Epoch: 29750\tTrain loss: 369.35579681396484\n",
      "Epoch: 29800\tTrain loss: 1046.3437805175781\n",
      "Epoch: 29850\tTrain loss: 1037.9294128417969\n",
      "Epoch: 29900\tTrain loss: 1301.4200744628906\n",
      "Epoch: 29950\tTrain loss: 1138.6445922851562\n",
      "Epoch: 30000\tTrain loss: 1075.9847412109375\n",
      "Epoch: 30050\tTrain loss: 970.9867553710938\n",
      "Epoch: 30100\tTrain loss: 687.410888671875\n",
      "Epoch: 30150\tTrain loss: 619.478271484375\n",
      "Epoch: 30200\tTrain loss: 575.2015075683594\n",
      "Epoch: 30250\tTrain loss: 507.3179016113281\n",
      "Epoch: 30300\tTrain loss: 487.0770263671875\n",
      "Epoch: 30350\tTrain loss: 473.3274841308594\n",
      "Epoch: 30400\tTrain loss: 462.1919250488281\n",
      "Epoch: 30450\tTrain loss: 451.5389709472656\n",
      "Epoch: 30500\tTrain loss: 440.5152587890625\n",
      "Epoch: 30550\tTrain loss: 432.55389404296875\n",
      "Epoch: 30600\tTrain loss: 422.6261291503906\n",
      "Epoch: 30650\tTrain loss: 416.7948913574219\n",
      "Epoch: 30700\tTrain loss: 408.8970642089844\n",
      "Epoch: 30750\tTrain loss: 400.87171936035156\n",
      "Epoch: 30800\tTrain loss: 389.5116882324219\n",
      "Epoch: 30850\tTrain loss: 391.9099578857422\n",
      "Epoch: 30900\tTrain loss: 386.05963134765625\n",
      "Epoch: 30950\tTrain loss: 380.33746337890625\n",
      "Epoch: 31000\tTrain loss: 374.31353759765625\n",
      "Epoch: 31050\tTrain loss: 368.7446594238281\n",
      "Epoch: 31100\tTrain loss: 358.35462951660156\n",
      "Epoch: 31150\tTrain loss: 346.9103126525879\n",
      "Epoch: 31200\tTrain loss: 334.57923126220703\n",
      "Epoch: 31250\tTrain loss: 323.4981231689453\n",
      "Epoch: 31300\tTrain loss: 351.36363220214844\n",
      "Epoch: 31350\tTrain loss: 307.7130126953125\n",
      "Epoch: 31400\tTrain loss: 287.8826599121094\n",
      "Epoch: 31450\tTrain loss: 295.8412322998047\n",
      "Epoch: 31500\tTrain loss: 266.31226348876953\n",
      "Epoch: 31550\tTrain loss: 318.0091667175293\n",
      "Epoch: 31600\tTrain loss: 282.84326553344727\n",
      "Epoch: 31650\tTrain loss: 274.57788848876953\n",
      "Epoch: 31700\tTrain loss: 277.680908203125\n",
      "Epoch: 31750\tTrain loss: 269.6780242919922\n",
      "Epoch: 31800\tTrain loss: 265.2952880859375\n",
      "Epoch: 31850\tTrain loss: 261.1957702636719\n",
      "Epoch: 31900\tTrain loss: 363.64329528808594\n",
      "Epoch: 31950\tTrain loss: 339.3513412475586\n",
      "Epoch: 32000\tTrain loss: 333.46622467041016\n",
      "Epoch: 32050\tTrain loss: 328.65513610839844\n",
      "Epoch: 32100\tTrain loss: 323.5869140625\n",
      "Epoch: 32150\tTrain loss: 256.40174865722656\n",
      "Epoch: 32200\tTrain loss: 243.61261749267578\n",
      "Epoch: 32250\tTrain loss: 237.73409271240234\n",
      "Epoch: 32300\tTrain loss: 245.25665283203125\n",
      "Epoch: 32350\tTrain loss: 230.46142578125\n",
      "Epoch: 32400\tTrain loss: 226.6319122314453\n",
      "Epoch: 32450\tTrain loss: 222.1692352294922\n",
      "Epoch: 32500\tTrain loss: 227.9290313720703\n",
      "Epoch: 32550\tTrain loss: 215.2641372680664\n",
      "Epoch: 32600\tTrain loss: 221.4004669189453\n",
      "Epoch: 32650\tTrain loss: 207.40613555908203\n",
      "Epoch: 32700\tTrain loss: 202.13285064697266\n",
      "Epoch: 32750\tTrain loss: 200.4144229888916\n",
      "Epoch: 32800\tTrain loss: 195.9366912841797\n",
      "Epoch: 32850\tTrain loss: 202.9599952697754\n",
      "Epoch: 32900\tTrain loss: 189.12435722351074\n",
      "Epoch: 32950\tTrain loss: 184.30308532714844\n",
      "Epoch: 33000\tTrain loss: 180.59403228759766\n",
      "Epoch: 33050\tTrain loss: 224.06201171875\n",
      "Epoch: 33100\tTrain loss: 172.84241485595703\n",
      "Epoch: 33150\tTrain loss: 165.90958404541016\n",
      "Epoch: 33200\tTrain loss: 165.96245574951172\n",
      "Epoch: 33250\tTrain loss: 158.46418571472168\n",
      "Epoch: 33300\tTrain loss: 153.11928749084473\n",
      "Epoch: 33350\tTrain loss: 148.91239166259766\n",
      "Epoch: 33400\tTrain loss: 143.99442386627197\n",
      "Epoch: 33450\tTrain loss: 139.77124786376953\n",
      "Epoch: 33500\tTrain loss: 135.3743896484375\n",
      "Epoch: 33550\tTrain loss: 131.4616813659668\n",
      "Epoch: 33600\tTrain loss: 130.04987001419067\n",
      "Epoch: 33650\tTrain loss: 171.53070831298828\n",
      "Epoch: 33700\tTrain loss: 113.20973587036133\n",
      "Epoch: 33750\tTrain loss: 103.24681091308594\n",
      "Epoch: 33800\tTrain loss: 3129.0043334960938\n",
      "Epoch: 33850\tTrain loss: 725.9133758544922\n",
      "Epoch: 33900\tTrain loss: 548.6773834228516\n",
      "Epoch: 33950\tTrain loss: 450.5202331542969\n",
      "Epoch: 34000\tTrain loss: 426.8840789794922\n",
      "Epoch: 34050\tTrain loss: 361.8779754638672\n",
      "Epoch: 34100\tTrain loss: 338.80360412597656\n",
      "Epoch: 34150\tTrain loss: 320.1746063232422\n",
      "Epoch: 34200\tTrain loss: 304.89622497558594\n",
      "Epoch: 34250\tTrain loss: 283.1775894165039\n",
      "Epoch: 34300\tTrain loss: 270.842529296875\n",
      "Epoch: 34350\tTrain loss: 264.17091369628906\n",
      "Epoch: 34400\tTrain loss: 275.84867095947266\n",
      "Epoch: 34450\tTrain loss: 254.44134521484375\n",
      "Epoch: 34500\tTrain loss: 249.58867645263672\n",
      "Epoch: 34550\tTrain loss: 244.86408233642578\n",
      "Epoch: 34600\tTrain loss: 240.21332931518555\n",
      "Epoch: 34650\tTrain loss: 235.5877685546875\n",
      "Epoch: 34700\tTrain loss: 230.75720596313477\n",
      "Epoch: 34750\tTrain loss: 225.65554809570312\n",
      "Epoch: 34800\tTrain loss: 221.06104278564453\n",
      "Epoch: 34850\tTrain loss: 214.48755645751953\n",
      "Epoch: 34900\tTrain loss: 213.32432556152344\n",
      "Epoch: 34950\tTrain loss: 204.1961326599121\n",
      "Epoch: 35000\tTrain loss: 185.52112579345703\n",
      "Epoch: 35050\tTrain loss: 199.03324127197266\n",
      "Epoch: 35100\tTrain loss: 189.01849174499512\n",
      "Epoch: 35150\tTrain loss: 175.35828590393066\n",
      "Epoch: 35200\tTrain loss: 143.94189834594727\n",
      "Epoch: 35250\tTrain loss: 131.6656265258789\n",
      "Epoch: 35300\tTrain loss: 118.1859016418457\n",
      "Epoch: 35350\tTrain loss: 108.63956069946289\n",
      "Epoch: 35400\tTrain loss: 110.14767456054688\n",
      "Epoch: 35450\tTrain loss: 107.73275756835938\n",
      "Epoch: 35500\tTrain loss: 138.47939682006836\n",
      "Epoch: 35550\tTrain loss: 95.9318618774414\n",
      "Epoch: 35600\tTrain loss: 89.4558334350586\n",
      "Epoch: 35650\tTrain loss: 84.4932861328125\n",
      "Epoch: 35700\tTrain loss: 92.64822769165039\n",
      "Epoch: 35750\tTrain loss: 77.63767623901367\n",
      "Epoch: 35800\tTrain loss: 77.62994384765625\n",
      "Epoch: 35850\tTrain loss: 62.462080001831055\n",
      "Epoch: 35900\tTrain loss: 58.02746772766113\n",
      "Epoch: 35950\tTrain loss: 53.80893325805664\n",
      "Epoch: 36000\tTrain loss: 51.116750717163086\n",
      "Epoch: 36050\tTrain loss: 47.70807647705078\n",
      "Epoch: 36100\tTrain loss: 46.29737186431885\n",
      "Epoch: 36150\tTrain loss: 52.94428730010986\n",
      "Epoch: 36200\tTrain loss: 42.42498588562012\n",
      "Epoch: 36250\tTrain loss: 37.73525428771973\n",
      "Epoch: 36300\tTrain loss: 37.31416702270508\n",
      "Epoch: 36350\tTrain loss: 36.96522617340088\n",
      "Epoch: 36400\tTrain loss: 33.13402080535889\n",
      "Epoch: 36450\tTrain loss: 29.99352741241455\n",
      "Epoch: 36500\tTrain loss: 29.771130561828613\n",
      "Epoch: 36550\tTrain loss: 42.79448699951172\n",
      "Epoch: 36600\tTrain loss: 36.356812477111816\n",
      "Epoch: 36650\tTrain loss: 20.849348068237305\n",
      "Epoch: 36700\tTrain loss: 22.752923011779785\n",
      "Epoch: 36750\tTrain loss: 13.959184646606445\n",
      "Epoch: 36800\tTrain loss: 140.21873092651367\n",
      "Epoch: 36850\tTrain loss: 13.084993362426758\n",
      "Epoch: 36900\tTrain loss: 10.696192264556885\n",
      "Epoch: 36950\tTrain loss: 8.675848603248596\n",
      "Epoch: 37000\tTrain loss: 7.614797115325928\n",
      "Epoch: 37050\tTrain loss: 6.718252658843994\n",
      "Epoch: 37100\tTrain loss: 5.933125972747803\n",
      "Epoch: 37150\tTrain loss: 5.2510985136032104\n",
      "Epoch: 37200\tTrain loss: 4.652589797973633\n",
      "Epoch: 37250\tTrain loss: 4.116007208824158\n",
      "Epoch: 37300\tTrain loss: 3.6336532831192017\n",
      "Epoch: 37350\tTrain loss: 3.2042856216430664\n",
      "Epoch: 37400\tTrain loss: 2.8768023252487183\n",
      "Epoch: 37450\tTrain loss: 2.686758875846863\n",
      "Epoch: 37500\tTrain loss: 2.278332829475403\n",
      "Epoch: 37550\tTrain loss: 1.9569470882415771\n",
      "Epoch: 37600\tTrain loss: 2.0619813799858093\n",
      "Epoch: 37650\tTrain loss: 2.106160670518875\n",
      "Epoch: 37700\tTrain loss: 1.490158885717392\n",
      "Epoch: 37750\tTrain loss: 1.1589550077915192\n",
      "Epoch: 37800\tTrain loss: 1.8854428231716156\n",
      "Epoch: 37850\tTrain loss: 1.012946456670761\n",
      "Epoch: 37900\tTrain loss: 1.4919418692588806\n",
      "Epoch: 37950\tTrain loss: 1.5303500294685364\n",
      "Epoch: 38000\tTrain loss: 0.6987136006355286\n",
      "Epoch: 38050\tTrain loss: 0.5630904659628868\n",
      "Epoch: 38100\tTrain loss: 0.47032202780246735\n",
      "Epoch: 38150\tTrain loss: 0.3967403434216976\n",
      "Epoch: 38200\tTrain loss: 0.9083702564239502\n",
      "Epoch: 38250\tTrain loss: 0.4203651025891304\n",
      "Epoch: 38300\tTrain loss: 0.2851949632167816\n",
      "Epoch: 38350\tTrain loss: 0.2396664284169674\n",
      "Epoch: 38400\tTrain loss: 13.542001724243164\n",
      "Epoch: 38450\tTrain loss: 0.21283936500549316\n",
      "Epoch: 38500\tTrain loss: 0.14354256726801395\n",
      "Epoch: 38550\tTrain loss: 0.11551501974463463\n",
      "Epoch: 38600\tTrain loss: 0.09334305860102177\n",
      "Epoch: 38650\tTrain loss: 0.0754411444067955\n",
      "Epoch: 38700\tTrain loss: 0.06527546420693398\n",
      "Epoch: 38750\tTrain loss: 271.085955619812\n",
      "Epoch: 38800\tTrain loss: 200.3431167602539\n",
      "Epoch: 38850\tTrain loss: 179.50983476638794\n",
      "Epoch: 38900\tTrain loss: 170.17716598510742\n",
      "Epoch: 38950\tTrain loss: 164.65161323547363\n",
      "Epoch: 39000\tTrain loss: 160.73644638061523\n",
      "Epoch: 39050\tTrain loss: 157.71492385864258\n",
      "Epoch: 39100\tTrain loss: 154.91500806808472\n",
      "Epoch: 39150\tTrain loss: 152.6158676147461\n",
      "Epoch: 39200\tTrain loss: 152.34739589691162\n",
      "Epoch: 39250\tTrain loss: 149.08725357055664\n",
      "Epoch: 39300\tTrain loss: 147.5426902770996\n",
      "Epoch: 39350\tTrain loss: 145.57353591918945\n",
      "Epoch: 39400\tTrain loss: 147.0784034729004\n",
      "Epoch: 39450\tTrain loss: 143.03718185424805\n",
      "Epoch: 39500\tTrain loss: 141.37471389770508\n",
      "Epoch: 39550\tTrain loss: 139.80467987060547\n",
      "Epoch: 39600\tTrain loss: 138.31253373622894\n",
      "Epoch: 39650\tTrain loss: 136.94998205080628\n",
      "Epoch: 39700\tTrain loss: 139.15932846069336\n",
      "Epoch: 39750\tTrain loss: 134.78045654296875\n",
      "Epoch: 39800\tTrain loss: 133.69803619384766\n",
      "Epoch: 39850\tTrain loss: 136.71426391601562\n",
      "Epoch: 39900\tTrain loss: 134.30465975403786\n",
      "Epoch: 39950\tTrain loss: 132.94488906860352\n",
      "Epoch: 40000\tTrain loss: 131.80042266845703\n",
      "Epoch: 40050\tTrain loss: 130.75881958007812\n",
      "Epoch: 40100\tTrain loss: 129.6290740966797\n",
      "Epoch: 40150\tTrain loss: 127.9983728826046\n",
      "Epoch: 40200\tTrain loss: 125.3448600769043\n",
      "Epoch: 40250\tTrain loss: 119.91627883911133\n",
      "Epoch: 40300\tTrain loss: 114.17750930786133\n",
      "Epoch: 40350\tTrain loss: 93.93286395072937\n",
      "Epoch: 40400\tTrain loss: 59.857248306274414\n",
      "Epoch: 40450\tTrain loss: 98.37465286254883\n",
      "Epoch: 40500\tTrain loss: 51.410139083862305\n",
      "Epoch: 40550\tTrain loss: 40.03619450330734\n",
      "Epoch: 40600\tTrain loss: 45.44755458831787\n",
      "Epoch: 40650\tTrain loss: 29.349568838253617\n",
      "Epoch: 40700\tTrain loss: 26.049413204193115\n",
      "Epoch: 40750\tTrain loss: 22.698415756225586\n",
      "Epoch: 40800\tTrain loss: 23.82331371307373\n",
      "Epoch: 40850\tTrain loss: 15.42488431930542\n",
      "Epoch: 40900\tTrain loss: 14.103413581848145\n",
      "Epoch: 40950\tTrain loss: 11.9295774102211\n",
      "Epoch: 41000\tTrain loss: 8.301607608795166\n",
      "Epoch: 41050\tTrain loss: 90.61929130554199\n",
      "Epoch: 41100\tTrain loss: 21.55220651626587\n",
      "Epoch: 41150\tTrain loss: 40.726463317871094\n",
      "Epoch: 41200\tTrain loss: 260.1671152114868\n",
      "Epoch: 41250\tTrain loss: 14.628142356872559\n",
      "Epoch: 41300\tTrain loss: 8.566792368888855\n",
      "Epoch: 41350\tTrain loss: 7.09846830368042\n",
      "Epoch: 41400\tTrain loss: 6.057267069816589\n",
      "Epoch: 41450\tTrain loss: 5.210302114486694\n",
      "Epoch: 41500\tTrain loss: 4.5020763874053955\n",
      "Epoch: 41550\tTrain loss: 10.675625562667847\n",
      "Epoch: 41600\tTrain loss: 3.4040241837501526\n",
      "Epoch: 41650\tTrain loss: 2.905316412448883\n",
      "Epoch: 41700\tTrain loss: 2.4756122529506683\n",
      "Epoch: 41750\tTrain loss: 2.165754497051239\n",
      "Epoch: 41800\tTrain loss: 1.8007501363754272\n",
      "Epoch: 41850\tTrain loss: 1.5340745598077774\n",
      "Epoch: 41900\tTrain loss: 2.1671102046966553\n",
      "Epoch: 41950\tTrain loss: 1.1720141470432281\n",
      "Epoch: 42000\tTrain loss: 0.9438655525445938\n",
      "Epoch: 42050\tTrain loss: 0.7743795216083527\n",
      "Epoch: 42100\tTrain loss: 0.6352896466851234\n",
      "Epoch: 42150\tTrain loss: 0.5203772559762001\n",
      "Epoch: 42200\tTrain loss: 0.4271852672100067\n",
      "Epoch: 42250\tTrain loss: 0.35008630238007754\n",
      "Epoch: 42300\tTrain loss: 0.28678358579054475\n",
      "Epoch: 42350\tTrain loss: 0.23454571701586246\n",
      "Epoch: 42400\tTrain loss: 0.19149290770292282\n",
      "Epoch: 42450\tTrain loss: 0.1555083878338337\n",
      "Epoch: 42500\tTrain loss: 0.12560641020536423\n",
      "Epoch: 42550\tTrain loss: 0.10079381428658962\n",
      "Epoch: 42600\tTrain loss: 0.08073423150926828\n",
      "Epoch: 42650\tTrain loss: 0.1328798569738865\n",
      "Epoch: 42700\tTrain loss: 0.050157770747318864\n",
      "Epoch: 42750\tTrain loss: 0.039751334115862846\n",
      "Epoch: 42800\tTrain loss: 0.04249736201018095\n",
      "Epoch: 42850\tTrain loss: 8.326669335365295\n",
      "Epoch: 42900\tTrain loss: 21.97217893600464\n",
      "Epoch: 42950\tTrain loss: 10.951813995838165\n",
      "Epoch: 43000\tTrain loss: 5.799750089645386\n",
      "Epoch: 43050\tTrain loss: 1.7090263962745667\n",
      "Epoch: 43100\tTrain loss: 0.5733602307736874\n",
      "Epoch: 43150\tTrain loss: 0.17258584639057517\n",
      "Epoch: 43200\tTrain loss: 0.09719416685402393\n",
      "Epoch: 43250\tTrain loss: 0.05892959702759981\n",
      "Epoch: 43300\tTrain loss: 0.05937047675251961\n",
      "Epoch: 43350\tTrain loss: 0.033156903460621834\n",
      "Epoch: 43400\tTrain loss: 0.02984710829332471\n",
      "Epoch: 43450\tTrain loss: 0.021052072290331125\n",
      "Epoch: 43500\tTrain loss: 1.349351018667221\n",
      "Epoch: 43550\tTrain loss: 48.676031947135925\n",
      "Epoch: 43600\tTrain loss: 553.9146499633789\n",
      "Epoch: 43650\tTrain loss: 2144.0601806640625\n",
      "Epoch: 43700\tTrain loss: 754.042236328125\n",
      "Epoch: 43750\tTrain loss: 608.7955627441406\n",
      "Epoch: 43800\tTrain loss: 609.7005920410156\n",
      "Epoch: 43850\tTrain loss: 601.2008762359619\n",
      "Epoch: 43900\tTrain loss: 594.2604370117188\n",
      "Epoch: 43950\tTrain loss: 586.72314453125\n",
      "Epoch: 44000\tTrain loss: 562.7292175292969\n",
      "Epoch: 44050\tTrain loss: 356.5233612060547\n",
      "Epoch: 44100\tTrain loss: 348.0746765136719\n",
      "Epoch: 44150\tTrain loss: 341.79060649871826\n",
      "Epoch: 44200\tTrain loss: 336.88280963897705\n",
      "Epoch: 44250\tTrain loss: 335.74472427368164\n",
      "Epoch: 44300\tTrain loss: 327.93443298339844\n",
      "Epoch: 44350\tTrain loss: 325.1487121582031\n",
      "Epoch: 44400\tTrain loss: 322.4229278564453\n",
      "Epoch: 44450\tTrain loss: 317.2359309196472\n",
      "Epoch: 44500\tTrain loss: 314.70875549316406\n",
      "Epoch: 44550\tTrain loss: 311.7907204627991\n",
      "Epoch: 44600\tTrain loss: 309.2371315956116\n",
      "Epoch: 44650\tTrain loss: 307.0574188232422\n",
      "Epoch: 44700\tTrain loss: 304.2401764392853\n",
      "Epoch: 44750\tTrain loss: 302.17535400390625\n",
      "Epoch: 44800\tTrain loss: 299.7554626464844\n",
      "Epoch: 44850\tTrain loss: 297.3833465576172\n",
      "Epoch: 44900\tTrain loss: 294.7136857509613\n",
      "Epoch: 44950\tTrain loss: 292.6282660961151\n",
      "Epoch: 45000\tTrain loss: 293.0013427734375\n",
      "Epoch: 45050\tTrain loss: 288.7298278808594\n",
      "Epoch: 45100\tTrain loss: 286.3500428199768\n",
      "Epoch: 45150\tTrain loss: 304.8575897216797\n",
      "Epoch: 45200\tTrain loss: 286.84472036361694\n",
      "Epoch: 45250\tTrain loss: 282.83220887184143\n",
      "Epoch: 45300\tTrain loss: 280.58197021484375\n",
      "Epoch: 45350\tTrain loss: 278.20867919921875\n",
      "Epoch: 45400\tTrain loss: 275.9346923828125\n",
      "Epoch: 45450\tTrain loss: 273.7681884765625\n",
      "Epoch: 45500\tTrain loss: 271.2530151605606\n",
      "Epoch: 45550\tTrain loss: 269.4631881713867\n",
      "Epoch: 45600\tTrain loss: 267.1073532104492\n",
      "Epoch: 45650\tTrain loss: 687.0346984863281\n",
      "Epoch: 45700\tTrain loss: 403.4795227050781\n",
      "Epoch: 45750\tTrain loss: 1160.9464111328125\n",
      "Epoch: 45800\tTrain loss: 907.0086059570312\n",
      "Epoch: 45850\tTrain loss: 839.4496765136719\n",
      "Epoch: 45900\tTrain loss: 799.4739837646484\n",
      "Epoch: 45950\tTrain loss: 769.1419677734375\n",
      "Epoch: 46000\tTrain loss: 743.8509826660156\n",
      "Epoch: 46050\tTrain loss: 721.8591613769531\n",
      "Epoch: 46100\tTrain loss: 701.5741271972656\n",
      "Epoch: 46150\tTrain loss: 682.7966613769531\n",
      "Epoch: 46200\tTrain loss: 665.6224670410156\n",
      "Epoch: 46250\tTrain loss: 649.0133972167969\n",
      "Epoch: 46300\tTrain loss: 633.3107299804688\n",
      "Epoch: 46350\tTrain loss: 617.9506225585938\n",
      "Epoch: 46400\tTrain loss: 602.7022399902344\n",
      "Epoch: 46450\tTrain loss: 587.8426818847656\n",
      "Epoch: 46500\tTrain loss: 573.2575225830078\n",
      "Epoch: 46550\tTrain loss: 558.6534423828125\n",
      "Epoch: 46600\tTrain loss: 538.1533050537109\n",
      "Epoch: 46650\tTrain loss: 532.8787231445312\n",
      "Epoch: 46700\tTrain loss: 496.8703155517578\n",
      "Epoch: 46750\tTrain loss: 478.4478454589844\n",
      "Epoch: 46800\tTrain loss: 460.81980895996094\n",
      "Epoch: 46850\tTrain loss: 477.07489013671875\n",
      "Epoch: 46900\tTrain loss: 467.4914245605469\n",
      "Epoch: 46950\tTrain loss: 452.39097595214844\n",
      "Epoch: 47000\tTrain loss: 438.9332275390625\n",
      "Epoch: 47050\tTrain loss: 426.3006896972656\n",
      "Epoch: 47100\tTrain loss: 414.1695556640625\n",
      "Epoch: 47150\tTrain loss: 432.9584045410156\n",
      "Epoch: 47200\tTrain loss: 387.5567626953125\n",
      "Epoch: 47250\tTrain loss: 1014.6565246582031\n",
      "Epoch: 47300\tTrain loss: 899.6849365234375\n",
      "Epoch: 47350\tTrain loss: 859.2636108398438\n",
      "Epoch: 47400\tTrain loss: 835.5791320800781\n",
      "Epoch: 47450\tTrain loss: 818.5872802734375\n",
      "Epoch: 47500\tTrain loss: 804.0124816894531\n",
      "Epoch: 47550\tTrain loss: 791.1078491210938\n",
      "Epoch: 47600\tTrain loss: 779.4471435546875\n",
      "Epoch: 47650\tTrain loss: 768.4485473632812\n",
      "Epoch: 47700\tTrain loss: 758.2985534667969\n",
      "Epoch: 47750\tTrain loss: 748.2997131347656\n",
      "Epoch: 47800\tTrain loss: 738.2448425292969\n",
      "Epoch: 47850\tTrain loss: 729.1667785644531\n",
      "Epoch: 47900\tTrain loss: 720.7307739257812\n",
      "Epoch: 47950\tTrain loss: 713.4517517089844\n",
      "Epoch: 48000\tTrain loss: 706.9222412109375\n",
      "Epoch: 48050\tTrain loss: 700.0563278198242\n",
      "Epoch: 48100\tTrain loss: 693.7861328125\n",
      "Epoch: 48150\tTrain loss: 692.9852905273438\n",
      "Epoch: 48200\tTrain loss: 13265.240234375\n",
      "Epoch: 48250\tTrain loss: 12504.83740234375\n",
      "Epoch: 48300\tTrain loss: 12035.63330078125\n",
      "Epoch: 48350\tTrain loss: 11710.1943359375\n",
      "Epoch: 48400\tTrain loss: 11485.075439453125\n",
      "Epoch: 48450\tTrain loss: 11307.8955078125\n",
      "Epoch: 48500\tTrain loss: 11155.18994140625\n",
      "Epoch: 48550\tTrain loss: 11016.21142578125\n",
      "Epoch: 48600\tTrain loss: 10884.89501953125\n",
      "Epoch: 48650\tTrain loss: 10744.39306640625\n",
      "Epoch: 48700\tTrain loss: 10592.25341796875\n",
      "Epoch: 48750\tTrain loss: 10369.170776367188\n",
      "Epoch: 48800\tTrain loss: 10185.1875\n",
      "Epoch: 48850\tTrain loss: 10033.28173828125\n",
      "Epoch: 48900\tTrain loss: 9897.503173828125\n",
      "Epoch: 48950\tTrain loss: 9790.887939453125\n",
      "Epoch: 49000\tTrain loss: 9693.2548828125\n",
      "Epoch: 49050\tTrain loss: 9599.74755859375\n",
      "Epoch: 49100\tTrain loss: 9510.0439453125\n",
      "Epoch: 49150\tTrain loss: 9422.999450683594\n",
      "Epoch: 49200\tTrain loss: 9333.658203125\n",
      "Epoch: 49250\tTrain loss: 9244.73681640625\n",
      "Epoch: 49300\tTrain loss: 9152.601318359375\n",
      "Epoch: 49350\tTrain loss: 9077.711303710938\n",
      "Epoch: 49400\tTrain loss: 8959.194580078125\n",
      "Epoch: 49450\tTrain loss: 8886.718505859375\n",
      "Epoch: 49500\tTrain loss: 8850.46630859375\n",
      "Epoch: 49550\tTrain loss: 8712.50341796875\n",
      "Epoch: 49600\tTrain loss: 8529.285339355469\n",
      "Epoch: 49650\tTrain loss: 16827.52392578125\n",
      "Epoch: 49700\tTrain loss: 9683.65234375\n",
      "Epoch: 49750\tTrain loss: 3387.3236083984375\n",
      "Epoch: 49800\tTrain loss: 3013.586181640625\n",
      "Epoch: 49850\tTrain loss: 3599.24560546875\n",
      "Epoch: 49900\tTrain loss: 3407.0144653320312\n",
      "Epoch: 49950\tTrain loss: 3202.62060546875\n",
      "Training CFRNN\n",
      "Epoch: 0\tTrain loss: 980189.09375\n",
      "Epoch: 50\tTrain loss: 940616.53125\n",
      "Epoch: 100\tTrain loss: 900991.25\n",
      "Epoch: 150\tTrain loss: 864512.28125\n",
      "Epoch: 200\tTrain loss: 829939.71875\n",
      "Epoch: 250\tTrain loss: 796795.71875\n",
      "Epoch: 300\tTrain loss: 764903.921875\n",
      "Epoch: 350\tTrain loss: 734326.28125\n",
      "Epoch: 400\tTrain loss: 704938.59375\n",
      "Epoch: 450\tTrain loss: 676576.203125\n",
      "Epoch: 500\tTrain loss: 649211.4375\n",
      "Epoch: 550\tTrain loss: 623004.15625\n",
      "Epoch: 600\tTrain loss: 597689.6796875\n",
      "Epoch: 650\tTrain loss: 573283.546875\n",
      "Epoch: 700\tTrain loss: 549849.234375\n",
      "Epoch: 750\tTrain loss: 527393.125\n",
      "Epoch: 800\tTrain loss: 505704.5234375\n",
      "Epoch: 850\tTrain loss: 484852.5234375\n",
      "Epoch: 900\tTrain loss: 464875.0546875\n",
      "Epoch: 950\tTrain loss: 445660.15625\n",
      "Epoch: 1000\tTrain loss: 427299.1953125\n",
      "Epoch: 1050\tTrain loss: 409660.71875\n",
      "Epoch: 1100\tTrain loss: 392778.1875\n",
      "Epoch: 1150\tTrain loss: 376622.359375\n",
      "Epoch: 1200\tTrain loss: 361204.52734375\n",
      "Epoch: 1250\tTrain loss: 346389.3984375\n",
      "Epoch: 1300\tTrain loss: 332327.46875\n",
      "Epoch: 1350\tTrain loss: 318882.47265625\n",
      "Epoch: 1400\tTrain loss: 306071.625\n",
      "Epoch: 1450\tTrain loss: 293841.1015625\n",
      "Epoch: 1500\tTrain loss: 282283.22265625\n",
      "Epoch: 1550\tTrain loss: 271267.90625\n",
      "Epoch: 1600\tTrain loss: 260931.076171875\n",
      "Epoch: 1650\tTrain loss: 250995.44873046875\n",
      "Epoch: 1700\tTrain loss: 241754.02587890625\n",
      "Epoch: 1750\tTrain loss: 232897.232421875\n",
      "Epoch: 1800\tTrain loss: 224676.515625\n",
      "Epoch: 1850\tTrain loss: 216853.169921875\n",
      "Epoch: 1900\tTrain loss: 209593.6884765625\n",
      "Epoch: 1950\tTrain loss: 202716.78125\n",
      "Epoch: 2000\tTrain loss: 196325.54296875\n",
      "Epoch: 2050\tTrain loss: 190302.716796875\n",
      "Epoch: 2100\tTrain loss: 184755.48046875\n",
      "Epoch: 2150\tTrain loss: 179606.185546875\n",
      "Epoch: 2200\tTrain loss: 174799.376953125\n",
      "Epoch: 2250\tTrain loss: 170376.2255859375\n",
      "Epoch: 2300\tTrain loss: 166267.625\n",
      "Epoch: 2350\tTrain loss: 162500.50390625\n",
      "Epoch: 2400\tTrain loss: 159034.615234375\n",
      "Epoch: 2450\tTrain loss: 155902.92578125\n",
      "Epoch: 2500\tTrain loss: 153039.791015625\n",
      "Epoch: 2550\tTrain loss: 150393.4365234375\n",
      "Epoch: 2600\tTrain loss: 148039.5234375\n",
      "Epoch: 2650\tTrain loss: 145961.888671875\n",
      "Epoch: 2700\tTrain loss: 144029.4169921875\n",
      "Epoch: 2750\tTrain loss: 142317.173828125\n",
      "Epoch: 2800\tTrain loss: 140803.9306640625\n",
      "Epoch: 2850\tTrain loss: 139470.236328125\n",
      "Epoch: 2900\tTrain loss: 138269.6953125\n",
      "Epoch: 2950\tTrain loss: 137232.84765625\n",
      "Epoch: 3000\tTrain loss: 136290.291015625\n",
      "Epoch: 3050\tTrain loss: 135516.208984375\n",
      "Epoch: 3100\tTrain loss: 107789.48583984375\n",
      "Epoch: 3150\tTrain loss: 102483.8857421875\n",
      "Epoch: 3200\tTrain loss: 97766.07373046875\n",
      "Epoch: 3250\tTrain loss: 93680.14892578125\n",
      "Epoch: 3300\tTrain loss: 90604.99011230469\n",
      "Epoch: 3350\tTrain loss: 86528.97845458984\n",
      "Epoch: 3400\tTrain loss: 83132.22891235352\n",
      "Epoch: 3450\tTrain loss: 80031.54675292969\n",
      "Epoch: 3500\tTrain loss: 76821.74731445312\n",
      "Epoch: 3550\tTrain loss: 73775.60452270508\n",
      "Epoch: 3600\tTrain loss: 70893.65167236328\n",
      "Epoch: 3650\tTrain loss: 68138.71484375\n",
      "Epoch: 3700\tTrain loss: 65539.14685058594\n",
      "Epoch: 3750\tTrain loss: 63106.98696899414\n",
      "Epoch: 3800\tTrain loss: 60763.9690246582\n",
      "Epoch: 3850\tTrain loss: 58549.09591674805\n",
      "Epoch: 3900\tTrain loss: 56420.69583129883\n",
      "Epoch: 3950\tTrain loss: 54216.41928100586\n",
      "Epoch: 4000\tTrain loss: 52115.19467163086\n",
      "Epoch: 4050\tTrain loss: 50040.14463806152\n",
      "Epoch: 4100\tTrain loss: 47988.189880371094\n",
      "Epoch: 4150\tTrain loss: 46000.282653808594\n",
      "Epoch: 4200\tTrain loss: 44069.76480102539\n",
      "Epoch: 4250\tTrain loss: 42154.33270263672\n",
      "Epoch: 4300\tTrain loss: 40298.06735229492\n",
      "Epoch: 4350\tTrain loss: 38493.44470214844\n",
      "Epoch: 4400\tTrain loss: 36752.5729675293\n",
      "Epoch: 4450\tTrain loss: 35021.45617675781\n",
      "Epoch: 4500\tTrain loss: 33372.22088623047\n",
      "Epoch: 4550\tTrain loss: 31751.026794433594\n",
      "Epoch: 4600\tTrain loss: 30173.02752685547\n",
      "Epoch: 4650\tTrain loss: 28632.830688476562\n",
      "Epoch: 4700\tTrain loss: 27214.237182617188\n",
      "Epoch: 4750\tTrain loss: 25743.548736572266\n",
      "Epoch: 4800\tTrain loss: 24359.10415649414\n",
      "Epoch: 4850\tTrain loss: 23019.069061279297\n",
      "Epoch: 4900\tTrain loss: 21717.072540283203\n",
      "Epoch: 4950\tTrain loss: 20480.297821044922\n",
      "Epoch: 5000\tTrain loss: 19309.090881347656\n",
      "Epoch: 5050\tTrain loss: 18125.56591796875\n",
      "Epoch: 5100\tTrain loss: 17017.75277709961\n",
      "Epoch: 5150\tTrain loss: 15962.687683105469\n",
      "Epoch: 5200\tTrain loss: 14938.622833251953\n",
      "Epoch: 5250\tTrain loss: 13962.413208007812\n",
      "Epoch: 5300\tTrain loss: 13036.349639892578\n",
      "Epoch: 5350\tTrain loss: 12150.733657836914\n",
      "Epoch: 5400\tTrain loss: 11302.216857910156\n",
      "Epoch: 5450\tTrain loss: 10496.346984863281\n",
      "Epoch: 5500\tTrain loss: 9726.14599609375\n",
      "Epoch: 5550\tTrain loss: 9013.25814819336\n",
      "Epoch: 5600\tTrain loss: 8315.624114990234\n",
      "Epoch: 5650\tTrain loss: 7665.952606201172\n",
      "Epoch: 5700\tTrain loss: 7059.440811157227\n",
      "Epoch: 5750\tTrain loss: 6479.018600463867\n",
      "Epoch: 5800\tTrain loss: 5941.527069091797\n",
      "Epoch: 5850\tTrain loss: 5436.246734619141\n",
      "Epoch: 5900\tTrain loss: 4968.498382568359\n",
      "Epoch: 5950\tTrain loss: 4529.118667602539\n",
      "Epoch: 6000\tTrain loss: 4119.306976318359\n",
      "Epoch: 6050\tTrain loss: 3743.3273010253906\n",
      "Epoch: 6100\tTrain loss: 3394.1571350097656\n",
      "Epoch: 6150\tTrain loss: 3076.2057495117188\n",
      "Epoch: 6200\tTrain loss: 2785.03173828125\n",
      "Epoch: 6250\tTrain loss: 2518.732391357422\n",
      "Epoch: 6300\tTrain loss: 2276.6337890625\n",
      "Epoch: 6350\tTrain loss: 2049.2792358398438\n",
      "Epoch: 6400\tTrain loss: 1852.4128112792969\n",
      "Epoch: 6450\tTrain loss: 1673.4600219726562\n",
      "Epoch: 6500\tTrain loss: 1511.8773193359375\n",
      "Epoch: 6550\tTrain loss: 1369.2355041503906\n",
      "Epoch: 6600\tTrain loss: 1244.9031982421875\n",
      "Epoch: 6650\tTrain loss: 1130.7472534179688\n",
      "Epoch: 6700\tTrain loss: 1159.7713623046875\n",
      "Epoch: 6750\tTrain loss: 960.0416564941406\n",
      "Epoch: 6800\tTrain loss: 887.0162658691406\n",
      "Epoch: 6850\tTrain loss: 824.5331420898438\n",
      "Epoch: 6900\tTrain loss: 770.549072265625\n",
      "Epoch: 6950\tTrain loss: 725.3614501953125\n",
      "Epoch: 7000\tTrain loss: 686.1458587646484\n",
      "Epoch: 7050\tTrain loss: 651.04296875\n",
      "Epoch: 7100\tTrain loss: 621.3880004882812\n",
      "Epoch: 7150\tTrain loss: 603.6008605957031\n",
      "Epoch: 7200\tTrain loss: 580.2847290039062\n",
      "Epoch: 7250\tTrain loss: 560.8027038574219\n",
      "Epoch: 7300\tTrain loss: 543.86376953125\n",
      "Epoch: 7350\tTrain loss: 528.8722381591797\n",
      "Epoch: 7400\tTrain loss: 515.4710540771484\n",
      "Epoch: 7450\tTrain loss: 503.06524658203125\n",
      "Epoch: 7500\tTrain loss: 491.47662353515625\n",
      "Epoch: 7550\tTrain loss: 480.5170593261719\n",
      "Epoch: 7600\tTrain loss: 469.7873077392578\n",
      "Epoch: 7650\tTrain loss: 459.1619873046875\n",
      "Epoch: 7700\tTrain loss: 448.8033447265625\n",
      "Epoch: 7750\tTrain loss: 438.6452941894531\n",
      "Epoch: 7800\tTrain loss: 428.5668640136719\n",
      "Epoch: 7850\tTrain loss: 418.4645538330078\n",
      "Epoch: 7900\tTrain loss: 408.47798919677734\n",
      "Epoch: 7950\tTrain loss: 398.7532196044922\n",
      "Epoch: 8000\tTrain loss: 389.1161804199219\n",
      "Epoch: 8050\tTrain loss: 379.6880340576172\n",
      "Epoch: 8100\tTrain loss: 384.8994140625\n",
      "Epoch: 8150\tTrain loss: 361.89601135253906\n",
      "Epoch: 8200\tTrain loss: 353.0224609375\n",
      "Epoch: 8250\tTrain loss: 344.3197326660156\n",
      "Epoch: 8300\tTrain loss: 335.7469482421875\n",
      "Epoch: 8350\tTrain loss: 326.9509582519531\n",
      "Epoch: 8400\tTrain loss: 318.15321350097656\n",
      "Epoch: 8450\tTrain loss: 309.28065490722656\n",
      "Epoch: 8500\tTrain loss: 300.4809112548828\n",
      "Epoch: 8550\tTrain loss: 291.6415557861328\n",
      "Epoch: 8600\tTrain loss: 283.14517974853516\n",
      "Epoch: 8650\tTrain loss: 274.83448791503906\n",
      "Epoch: 8700\tTrain loss: 266.9478454589844\n",
      "Epoch: 8750\tTrain loss: 259.2560043334961\n",
      "Epoch: 8800\tTrain loss: 251.84056091308594\n",
      "Epoch: 8850\tTrain loss: 244.7611083984375\n",
      "Epoch: 8900\tTrain loss: 238.06847381591797\n",
      "Epoch: 8950\tTrain loss: 231.66829681396484\n",
      "Epoch: 9000\tTrain loss: 225.4531364440918\n",
      "Epoch: 9050\tTrain loss: 219.68387603759766\n",
      "Epoch: 9100\tTrain loss: 214.1826934814453\n",
      "Epoch: 9150\tTrain loss: 208.80758666992188\n",
      "Epoch: 9200\tTrain loss: 203.66641998291016\n",
      "Epoch: 9250\tTrain loss: 198.84544372558594\n",
      "Epoch: 9300\tTrain loss: 193.72610473632812\n",
      "Epoch: 9350\tTrain loss: 188.84197998046875\n",
      "Epoch: 9400\tTrain loss: 179.52619171142578\n",
      "Epoch: 9450\tTrain loss: 170.92164611816406\n",
      "Epoch: 9500\tTrain loss: 160.89830780029297\n",
      "Epoch: 9550\tTrain loss: 289.44422149658203\n",
      "Epoch: 9600\tTrain loss: 179.6067295074463\n",
      "Epoch: 9650\tTrain loss: 260.7848587036133\n",
      "Epoch: 9700\tTrain loss: 180.99095916748047\n",
      "Epoch: 9750\tTrain loss: 171.9469223022461\n",
      "Epoch: 9800\tTrain loss: 165.90628814697266\n",
      "Epoch: 9850\tTrain loss: 159.67642974853516\n",
      "Epoch: 9900\tTrain loss: 143.83293914794922\n",
      "Epoch: 9950\tTrain loss: 128.8699312210083\n",
      "Epoch: 10000\tTrain loss: 120.21453666687012\n",
      "Epoch: 10050\tTrain loss: 114.38496017456055\n",
      "Epoch: 10100\tTrain loss: 159.93482208251953\n",
      "Epoch: 10150\tTrain loss: 99.95538330078125\n",
      "Epoch: 10200\tTrain loss: 92.64034938812256\n",
      "Epoch: 10250\tTrain loss: 85.58010864257812\n",
      "Epoch: 10300\tTrain loss: 78.38142013549805\n",
      "Epoch: 10350\tTrain loss: 195.85403728485107\n",
      "Epoch: 10400\tTrain loss: 65.39721488952637\n",
      "Epoch: 10450\tTrain loss: 57.64463424682617\n",
      "Epoch: 10500\tTrain loss: 51.26901626586914\n",
      "Epoch: 10550\tTrain loss: 45.61043834686279\n",
      "Epoch: 10600\tTrain loss: 40.51519775390625\n",
      "Epoch: 10650\tTrain loss: 35.860692501068115\n",
      "Epoch: 10700\tTrain loss: 31.57559108734131\n",
      "Epoch: 10750\tTrain loss: 27.69766902923584\n",
      "Epoch: 10800\tTrain loss: 24.307065963745117\n",
      "Epoch: 10850\tTrain loss: 20.989346981048584\n",
      "Epoch: 10900\tTrain loss: 17.87463653087616\n",
      "Epoch: 10950\tTrain loss: 15.189975261688232\n",
      "Epoch: 11000\tTrain loss: 12.812353372573853\n",
      "Epoch: 11050\tTrain loss: 11.31503677368164\n",
      "Epoch: 11100\tTrain loss: 9.752662897109985\n",
      "Epoch: 11150\tTrain loss: 7.725432395935059\n",
      "Epoch: 11200\tTrain loss: 6.19881945848465\n",
      "Epoch: 11250\tTrain loss: 5.152705430984497\n",
      "Epoch: 11300\tTrain loss: 3.844851493835449\n",
      "Epoch: 11350\tTrain loss: 3.095747709274292\n",
      "Epoch: 11400\tTrain loss: 38.66566354036331\n",
      "Epoch: 11450\tTrain loss: 571.2026062011719\n",
      "Epoch: 11500\tTrain loss: 191.1919755935669\n",
      "Epoch: 11550\tTrain loss: 577.4588584899902\n",
      "Epoch: 11600\tTrain loss: 564.9481048583984\n",
      "Epoch: 11650\tTrain loss: 376.82952880859375\n",
      "Epoch: 11700\tTrain loss: 317.82617950439453\n",
      "Epoch: 11750\tTrain loss: 220.3762092590332\n",
      "Epoch: 11800\tTrain loss: 158.70863246917725\n",
      "Epoch: 11850\tTrain loss: 121.51000165939331\n",
      "Epoch: 11900\tTrain loss: 65.52036476135254\n",
      "Epoch: 11950\tTrain loss: 41.920527935028076\n",
      "Epoch: 12000\tTrain loss: 189.3326759338379\n",
      "Epoch: 12050\tTrain loss: 152.0988006591797\n",
      "Epoch: 12100\tTrain loss: 139.99685859680176\n",
      "Epoch: 12150\tTrain loss: 132.72776794433594\n",
      "Epoch: 12200\tTrain loss: 128.30781865119934\n",
      "Epoch: 12250\tTrain loss: 124.60186767578125\n",
      "Epoch: 12300\tTrain loss: 117.78097319602966\n",
      "Epoch: 12350\tTrain loss: 86.47572350502014\n",
      "Epoch: 12400\tTrain loss: 71.86589050292969\n",
      "Epoch: 12450\tTrain loss: 43.6413516998291\n",
      "Epoch: 12500\tTrain loss: 23.12069845199585\n",
      "Epoch: 12550\tTrain loss: 27.564208030700684\n",
      "Epoch: 12600\tTrain loss: 11.844842195510864\n",
      "Epoch: 12650\tTrain loss: 7.965360164642334\n",
      "Epoch: 12700\tTrain loss: 5.777380704879761\n",
      "Epoch: 12750\tTrain loss: 4.3662243485450745\n",
      "Epoch: 12800\tTrain loss: 3.3713646829128265\n",
      "Epoch: 12850\tTrain loss: 2.6307243704795837\n",
      "Epoch: 12900\tTrain loss: 2.05818635225296\n",
      "Epoch: 12950\tTrain loss: 1.6130888313055038\n",
      "Epoch: 13000\tTrain loss: 1.2688342928886414\n",
      "Epoch: 13050\tTrain loss: 0.9978391826152802\n",
      "Epoch: 13100\tTrain loss: 0.7892918884754181\n",
      "Epoch: 13150\tTrain loss: 0.6197834014892578\n",
      "Epoch: 13200\tTrain loss: 0.4882172867655754\n",
      "Epoch: 13250\tTrain loss: 0.3860776275396347\n",
      "Epoch: 13300\tTrain loss: 0.4157157465815544\n",
      "Epoch: 13350\tTrain loss: 0.2548437640070915\n",
      "Epoch: 13400\tTrain loss: 0.1938570737838745\n",
      "Epoch: 13450\tTrain loss: 0.15038463473320007\n",
      "Epoch: 13500\tTrain loss: 0.11654098704457283\n",
      "Epoch: 13550\tTrain loss: 0.08993232809007168\n",
      "Epoch: 13600\tTrain loss: 0.06920474767684937\n",
      "Epoch: 13650\tTrain loss: 0.05747964605689049\n",
      "Epoch: 13700\tTrain loss: 0.16142193973064423\n",
      "Epoch: 13750\tTrain loss: 0.033742187079042196\n",
      "Epoch: 13800\tTrain loss: 0.026770814321935177\n",
      "Epoch: 13850\tTrain loss: 0.4051080346107483\n",
      "Epoch: 13900\tTrain loss: 0.0563883176073432\n",
      "Epoch: 13950\tTrain loss: 0.060947731137275696\n",
      "Epoch: 14000\tTrain loss: 0.01200925512239337\n",
      "Epoch: 14050\tTrain loss: 0.0052455646218732\n",
      "Epoch: 14100\tTrain loss: 0.11255416925996542\n",
      "Epoch: 14150\tTrain loss: 0.005468928080517799\n",
      "Epoch: 14200\tTrain loss: 0.04925903491675854\n",
      "Epoch: 14250\tTrain loss: 0.21758125722408295\n",
      "Epoch: 14300\tTrain loss: 0.16730763763189316\n",
      "Epoch: 14350\tTrain loss: 0.0028955520829185843\n",
      "Epoch: 14400\tTrain loss: 0.0004670631024055183\n",
      "Epoch: 14450\tTrain loss: 0.0002653373230714351\n",
      "Epoch: 14500\tTrain loss: 0.0001375395841023419\n",
      "Epoch: 14550\tTrain loss: 7.918349729152396e-05\n",
      "Epoch: 14600\tTrain loss: 4.312330929678865e-05\n",
      "Epoch: 14650\tTrain loss: 3.775602908717701e-05\n",
      "Epoch: 14700\tTrain loss: 0.0005567845419136574\n",
      "Epoch: 14750\tTrain loss: 0.4936460107564926\n",
      "Epoch: 14800\tTrain loss: 716.5793914794922\n",
      "Epoch: 14850\tTrain loss: 581.9642486572266\n",
      "Epoch: 14900\tTrain loss: 353.18756103515625\n",
      "Epoch: 14950\tTrain loss: 263.2010040283203\n",
      "Epoch: 15000\tTrain loss: 227.37049865722656\n",
      "Epoch: 15050\tTrain loss: 212.51609802246094\n",
      "Epoch: 15100\tTrain loss: 202.09082794189453\n",
      "Epoch: 15150\tTrain loss: 193.46114349365234\n",
      "Epoch: 15200\tTrain loss: 182.41794204711914\n",
      "Epoch: 15250\tTrain loss: 170.2257537841797\n",
      "Epoch: 15300\tTrain loss: 159.03884506225586\n",
      "Epoch: 15350\tTrain loss: 148.75667572021484\n",
      "Epoch: 15400\tTrain loss: 136.08535766601562\n",
      "Epoch: 15450\tTrain loss: 123.02519607543945\n",
      "Epoch: 15500\tTrain loss: 107.10664939880371\n",
      "Epoch: 15550\tTrain loss: 86.59549713134766\n",
      "Epoch: 15600\tTrain loss: 76.91787719726562\n",
      "Epoch: 15650\tTrain loss: 68.75958061218262\n",
      "Epoch: 15700\tTrain loss: 52.569313049316406\n",
      "Epoch: 15750\tTrain loss: 47.93180799484253\n",
      "Epoch: 15800\tTrain loss: 45.30055522918701\n",
      "Epoch: 15850\tTrain loss: 41.453126430511475\n",
      "Epoch: 15900\tTrain loss: 38.485897064208984\n",
      "Epoch: 15950\tTrain loss: 35.69500398635864\n",
      "Epoch: 16000\tTrain loss: 33.117178440093994\n",
      "Epoch: 16050\tTrain loss: 30.798084259033203\n",
      "Epoch: 16100\tTrain loss: 28.675790786743164\n",
      "Epoch: 16150\tTrain loss: 26.617527961730957\n",
      "Epoch: 16200\tTrain loss: 24.73940360546112\n",
      "Epoch: 16250\tTrain loss: 24.986114501953125\n",
      "Epoch: 16300\tTrain loss: 21.364255905151367\n",
      "Epoch: 16350\tTrain loss: 19.780369997024536\n",
      "Epoch: 16400\tTrain loss: 18.385910034179688\n",
      "Epoch: 16450\tTrain loss: 21.602015495300293\n",
      "Epoch: 16500\tTrain loss: 15.961493492126465\n",
      "Epoch: 16550\tTrain loss: 14.540660381317139\n",
      "Epoch: 16600\tTrain loss: 13.327850818634033\n",
      "Epoch: 16650\tTrain loss: 12.222428798675537\n",
      "Epoch: 16700\tTrain loss: 11.199892044067383\n",
      "Epoch: 16750\tTrain loss: 10.235419243574142\n",
      "Epoch: 16800\tTrain loss: 9.365811824798584\n",
      "Epoch: 16850\tTrain loss: 8.529615953564644\n",
      "Epoch: 16900\tTrain loss: 7.764129638671875\n",
      "Epoch: 16950\tTrain loss: 7.0401143580675125\n",
      "Epoch: 17000\tTrain loss: 6.365204863250256\n",
      "Epoch: 17050\tTrain loss: 5.752033710479736\n",
      "Epoch: 17100\tTrain loss: 5.215751469135284\n",
      "Epoch: 17150\tTrain loss: 4.668348602950573\n",
      "Epoch: 17200\tTrain loss: 4.327426984906197\n",
      "Epoch: 17250\tTrain loss: 3.7074209451675415\n",
      "Epoch: 17300\tTrain loss: 3.2857018262147903\n",
      "Epoch: 17350\tTrain loss: 2.913112759590149\n",
      "Epoch: 17400\tTrain loss: 2.5696594826877117\n",
      "Epoch: 17450\tTrain loss: 2.258351281285286\n",
      "Epoch: 17500\tTrain loss: 1.984941303730011\n",
      "Epoch: 17550\tTrain loss: 1.735679566860199\n",
      "Epoch: 17600\tTrain loss: 1.510618805885315\n",
      "Epoch: 17650\tTrain loss: 1.3118658661842346\n",
      "Epoch: 17700\tTrain loss: 1.1330681033432484\n",
      "Epoch: 17750\tTrain loss: 1.2370011806488037\n",
      "Epoch: 17800\tTrain loss: 146.34074413776398\n",
      "Epoch: 17850\tTrain loss: 42.59659653902054\n",
      "Epoch: 17900\tTrain loss: 159.10475540161133\n",
      "Epoch: 17950\tTrain loss: 25.051628828048706\n",
      "Epoch: 18000\tTrain loss: 425.1555824279785\n",
      "Epoch: 18050\tTrain loss: 154.97296524047852\n",
      "Epoch: 18100\tTrain loss: 109.2001461982727\n",
      "Epoch: 18150\tTrain loss: 93.98701238632202\n",
      "Epoch: 18200\tTrain loss: 79.99475479125977\n",
      "Epoch: 18250\tTrain loss: 59.217295944690704\n",
      "Epoch: 18300\tTrain loss: 29.184081554412842\n",
      "Epoch: 18350\tTrain loss: 21.444551467895508\n",
      "Epoch: 18400\tTrain loss: 16.421315789222717\n",
      "Epoch: 18450\tTrain loss: 12.856974124908447\n",
      "Epoch: 18500\tTrain loss: 10.164212465286255\n",
      "Epoch: 18550\tTrain loss: 8.109491348266602\n",
      "Epoch: 18600\tTrain loss: 6.488240599632263\n",
      "Epoch: 18650\tTrain loss: 5.193302631378174\n",
      "Epoch: 18700\tTrain loss: 4.1665685176849365\n",
      "Epoch: 18750\tTrain loss: 3.3281224370002747\n",
      "Epoch: 18800\tTrain loss: 2.660750687122345\n",
      "Epoch: 18850\tTrain loss: 2.116998016834259\n",
      "Epoch: 18900\tTrain loss: 1.6837315261363983\n",
      "Epoch: 18950\tTrain loss: 1.3328966200351715\n",
      "Epoch: 19000\tTrain loss: 1.0677369236946106\n",
      "Epoch: 19050\tTrain loss: 9.412839889526367\n",
      "Epoch: 19100\tTrain loss: 398.2316131591797\n",
      "Epoch: 19150\tTrain loss: 208.54687881469727\n",
      "Epoch: 19200\tTrain loss: 4105.370178222656\n",
      "Epoch: 19250\tTrain loss: 226.74575805664062\n",
      "Epoch: 19300\tTrain loss: 211.59325408935547\n",
      "Epoch: 19350\tTrain loss: 201.42116737365723\n",
      "Epoch: 19400\tTrain loss: 192.8979034423828\n",
      "Epoch: 19450\tTrain loss: 184.8763246536255\n",
      "Epoch: 19500\tTrain loss: 176.6649169921875\n",
      "Epoch: 19550\tTrain loss: 167.6347770690918\n",
      "Epoch: 19600\tTrain loss: 176.03003311157227\n",
      "Epoch: 19650\tTrain loss: 147.17934799194336\n",
      "Epoch: 19700\tTrain loss: 130.26636791229248\n",
      "Epoch: 19750\tTrain loss: 117.03213500976562\n",
      "Epoch: 19800\tTrain loss: 107.15324592590332\n",
      "Epoch: 19850\tTrain loss: 98.9840955734253\n",
      "Epoch: 19900\tTrain loss: 91.90721893310547\n",
      "Epoch: 19950\tTrain loss: 85.67760753631592\n",
      "Epoch: 20000\tTrain loss: 80.06608009338379\n",
      "Epoch: 20050\tTrain loss: 74.93597316741943\n",
      "Epoch: 20100\tTrain loss: 70.18374156951904\n",
      "Epoch: 20150\tTrain loss: 65.74578428268433\n",
      "Epoch: 20200\tTrain loss: 61.5826358795166\n",
      "Epoch: 20250\tTrain loss: 57.660807609558105\n",
      "Epoch: 20300\tTrain loss: 53.92401742935181\n",
      "Epoch: 20350\tTrain loss: 50.40695858001709\n",
      "Epoch: 20400\tTrain loss: 46.64846706390381\n",
      "Epoch: 20450\tTrain loss: 35.818583965301514\n",
      "Epoch: 20500\tTrain loss: 29.679226875305176\n",
      "Epoch: 20550\tTrain loss: 22.66655731201172\n",
      "Epoch: 20600\tTrain loss: 11.369040489196777\n",
      "Epoch: 20650\tTrain loss: 6.197580099105835\n",
      "Epoch: 20700\tTrain loss: 4.547305226325989\n",
      "Epoch: 20750\tTrain loss: 3.662111818790436\n",
      "Epoch: 20800\tTrain loss: 3.0438594818115234\n",
      "Epoch: 20850\tTrain loss: 2.558068871498108\n",
      "Epoch: 20900\tTrain loss: 2.1598336696624756\n",
      "Epoch: 20950\tTrain loss: 1.824144959449768\n",
      "Epoch: 21000\tTrain loss: 1.542170912027359\n",
      "Epoch: 21050\tTrain loss: 1.303909957408905\n",
      "Epoch: 21100\tTrain loss: 1.0995463728904724\n",
      "Epoch: 21150\tTrain loss: 0.9276442229747772\n",
      "Epoch: 21200\tTrain loss: 0.780623733997345\n",
      "Epoch: 21250\tTrain loss: 4449.958190917969\n",
      "Epoch: 21300\tTrain loss: 558.6018295288086\n",
      "Epoch: 21350\tTrain loss: 476.71240234375\n",
      "Epoch: 21400\tTrain loss: 310.57606506347656\n",
      "Epoch: 21450\tTrain loss: 274.034122467041\n",
      "Epoch: 21500\tTrain loss: 253.53463745117188\n",
      "Epoch: 21550\tTrain loss: 242.5033950805664\n",
      "Epoch: 21600\tTrain loss: 219.20202255249023\n",
      "Epoch: 21650\tTrain loss: 203.06839752197266\n",
      "Epoch: 21700\tTrain loss: 604.099609375\n",
      "Epoch: 21750\tTrain loss: 242.7559356689453\n",
      "Epoch: 21800\tTrain loss: 185.03786087036133\n",
      "Epoch: 21850\tTrain loss: 169.73570251464844\n",
      "Epoch: 21900\tTrain loss: 161.23223114013672\n",
      "Epoch: 21950\tTrain loss: 495.1626739501953\n",
      "Epoch: 22000\tTrain loss: 146.41323280334473\n",
      "Epoch: 22050\tTrain loss: 135.33756828308105\n",
      "Epoch: 22100\tTrain loss: 130.31329345703125\n",
      "Epoch: 22150\tTrain loss: 117.75546264648438\n",
      "Epoch: 22200\tTrain loss: 117.60692596435547\n",
      "Epoch: 22250\tTrain loss: 103.45860481262207\n",
      "Epoch: 22300\tTrain loss: 94.17266273498535\n",
      "Epoch: 22350\tTrain loss: 94.30166816711426\n",
      "Epoch: 22400\tTrain loss: 75.59418106079102\n",
      "Epoch: 22450\tTrain loss: 67.44075393676758\n",
      "Epoch: 22500\tTrain loss: 61.16866493225098\n",
      "Epoch: 22550\tTrain loss: 59.89021301269531\n",
      "Epoch: 22600\tTrain loss: 60.28997778892517\n",
      "Epoch: 22650\tTrain loss: 50.117652893066406\n",
      "Epoch: 22700\tTrain loss: 43.60356795787811\n",
      "Epoch: 22750\tTrain loss: 39.7906379699707\n",
      "Epoch: 22800\tTrain loss: 29.170087814331055\n",
      "Epoch: 22850\tTrain loss: 25.294448852539062\n",
      "Epoch: 22900\tTrain loss: 22.804158210754395\n",
      "Epoch: 22950\tTrain loss: 22.189208984375\n",
      "Epoch: 23000\tTrain loss: 19.224381148815155\n",
      "Epoch: 23050\tTrain loss: 17.442073613405228\n",
      "Epoch: 23100\tTrain loss: 32.58321189880371\n",
      "Epoch: 23150\tTrain loss: 575.9050521850586\n",
      "Epoch: 23200\tTrain loss: 423.36822509765625\n",
      "Epoch: 23250\tTrain loss: 415.9826202392578\n",
      "Epoch: 23300\tTrain loss: 378.39014434814453\n",
      "Epoch: 23350\tTrain loss: 339.1203384399414\n",
      "Epoch: 23400\tTrain loss: 301.26342010498047\n",
      "Epoch: 23450\tTrain loss: 271.08129119873047\n",
      "Epoch: 23500\tTrain loss: 245.17845153808594\n",
      "Epoch: 23550\tTrain loss: 226.5364532470703\n",
      "Epoch: 23600\tTrain loss: 202.56998825073242\n",
      "Epoch: 23650\tTrain loss: 200.08185577392578\n",
      "Epoch: 23700\tTrain loss: 156.31674194335938\n",
      "Epoch: 23750\tTrain loss: 130.51079940795898\n",
      "Epoch: 23800\tTrain loss: 106.09243583679199\n",
      "Epoch: 23850\tTrain loss: 83.46982479095459\n",
      "Epoch: 23900\tTrain loss: 60.087947845458984\n",
      "Epoch: 23950\tTrain loss: 41.94502830505371\n",
      "Epoch: 24000\tTrain loss: 34.154340744018555\n",
      "Epoch: 24050\tTrain loss: 29.627832412719727\n",
      "Epoch: 24100\tTrain loss: 26.27902889251709\n",
      "Epoch: 24150\tTrain loss: 23.50364452600479\n",
      "Epoch: 24200\tTrain loss: 21.12521341443062\n",
      "Epoch: 24250\tTrain loss: 19.078547477722168\n",
      "Epoch: 24300\tTrain loss: 17.238017439842224\n",
      "Epoch: 24350\tTrain loss: 15.62546682357788\n",
      "Epoch: 24400\tTrain loss: 14.169650554656982\n",
      "Epoch: 24450\tTrain loss: 12.857809066772461\n",
      "Epoch: 24500\tTrain loss: 11.667908132076263\n",
      "Epoch: 24550\tTrain loss: 10.58962631225586\n",
      "Epoch: 24600\tTrain loss: 9.631860733032227\n",
      "Epoch: 24650\tTrain loss: 8.731173694133759\n",
      "Epoch: 24700\tTrain loss: 7.915361225605011\n",
      "Epoch: 24750\tTrain loss: 7.173840284347534\n",
      "Epoch: 24800\tTrain loss: 6.477588653564453\n",
      "Epoch: 24850\tTrain loss: 6.074066400527954\n",
      "Epoch: 24900\tTrain loss: 10.76701545715332\n",
      "Epoch: 24950\tTrain loss: 7.2283759117126465\n",
      "Epoch: 25000\tTrain loss: 5.833922922611237\n",
      "Epoch: 25050\tTrain loss: 4.841773152351379\n",
      "Epoch: 25100\tTrain loss: 4.051071882247925\n",
      "Epoch: 25150\tTrain loss: 3.3862959146499634\n",
      "Epoch: 25200\tTrain loss: 2.82613742351532\n",
      "Epoch: 25250\tTrain loss: 2.349425882101059\n",
      "Epoch: 25300\tTrain loss: 1.9486509561538696\n",
      "Epoch: 25350\tTrain loss: 1.6069908738136292\n",
      "Epoch: 25400\tTrain loss: 1.3258633017539978\n",
      "Epoch: 25450\tTrain loss: 1.086050570011139\n",
      "Epoch: 25500\tTrain loss: 0.8868842720985413\n",
      "Epoch: 25550\tTrain loss: 0.720850944519043\n",
      "Epoch: 25600\tTrain loss: 0.5826446637511253\n",
      "Epoch: 25650\tTrain loss: 0.46910442411899567\n",
      "Epoch: 25700\tTrain loss: 0.37485720217227936\n",
      "Epoch: 25750\tTrain loss: 0.2971321791410446\n",
      "Epoch: 25800\tTrain loss: 0.23390140384435654\n",
      "Epoch: 25850\tTrain loss: 0.18306129425764084\n",
      "Epoch: 25900\tTrain loss: 0.14159630797803402\n",
      "Epoch: 25950\tTrain loss: 0.10911798477172852\n",
      "Epoch: 26000\tTrain loss: 0.08308267872780561\n",
      "Epoch: 26050\tTrain loss: 0.06268169917166233\n",
      "Epoch: 26100\tTrain loss: 0.04672888829372823\n",
      "Epoch: 26150\tTrain loss: 0.034700771793723106\n",
      "Epoch: 26200\tTrain loss: 0.025352983211632818\n",
      "Epoch: 26250\tTrain loss: 0.018342448864132166\n",
      "Epoch: 26300\tTrain loss: 0.013058265787549317\n",
      "Epoch: 26350\tTrain loss: 0.010448015993461013\n",
      "Epoch: 26400\tTrain loss: 0.006451672175899148\n",
      "Epoch: 26450\tTrain loss: 0.004831318743526936\n",
      "Epoch: 26500\tTrain loss: 0.0030113012180663645\n",
      "Epoch: 26550\tTrain loss: 0.0020121397719776724\n",
      "Epoch: 26600\tTrain loss: 0.0033926067990250885\n",
      "Epoch: 26650\tTrain loss: 0.4149600883247331\n",
      "Epoch: 26700\tTrain loss: 0.5389600321650505\n",
      "Epoch: 26750\tTrain loss: 823.0500103235245\n",
      "Epoch: 26800\tTrain loss: 26.071707248687744\n",
      "Epoch: 26850\tTrain loss: 3.6547634601593018\n",
      "Epoch: 26900\tTrain loss: 1.995420217514038\n",
      "Epoch: 26950\tTrain loss: 1.2755330502986908\n",
      "Epoch: 27000\tTrain loss: 0.8365610837936401\n",
      "Epoch: 27050\tTrain loss: 0.5272856336086988\n",
      "Epoch: 27100\tTrain loss: 0.30470841750502586\n",
      "Epoch: 27150\tTrain loss: 0.16111668944358826\n",
      "Epoch: 27200\tTrain loss: 0.08017997443675995\n",
      "Epoch: 27250\tTrain loss: 0.04087532497942448\n",
      "Epoch: 27300\tTrain loss: 0.09708944242447615\n",
      "Epoch: 27350\tTrain loss: 0.01890200423076749\n",
      "Epoch: 27400\tTrain loss: 0.07029936462640762\n",
      "Epoch: 27450\tTrain loss: 0.009145541582256556\n",
      "Epoch: 27500\tTrain loss: 0.00460552831646055\n",
      "Epoch: 27550\tTrain loss: 0.0027400057879276574\n",
      "Epoch: 27600\tTrain loss: 0.0017691187240416184\n",
      "Epoch: 27650\tTrain loss: 0.0012256907648406923\n",
      "Epoch: 27700\tTrain loss: 0.0008144734456436709\n",
      "Epoch: 27750\tTrain loss: 0.0005548816479858942\n",
      "Epoch: 27800\tTrain loss: 0.00037712827543145977\n",
      "Epoch: 27850\tTrain loss: 0.00042238380410708487\n",
      "Epoch: 27900\tTrain loss: 0.015257991966791451\n",
      "Epoch: 27950\tTrain loss: 0.39936235174536705\n",
      "Epoch: 28000\tTrain loss: 0.005932340631261468\n",
      "Epoch: 28050\tTrain loss: 0.000998209579847753\n",
      "Epoch: 28100\tTrain loss: 0.000249494900344871\n",
      "Epoch: 28150\tTrain loss: 8.411405724473298e-05\n",
      "Epoch: 28200\tTrain loss: 3.4368571959930705e-05\n",
      "Epoch: 28250\tTrain loss: 1.5899212712611188e-05\n",
      "Epoch: 28300\tTrain loss: 7.801213328662016e-06\n",
      "Epoch: 28350\tTrain loss: 4.007108600490028e-06\n",
      "Epoch: 28400\tTrain loss: 2.1329356911792274e-06\n",
      "Epoch: 28450\tTrain loss: 1.1773624919442227e-06\n",
      "Epoch: 28500\tTrain loss: 3.2903885482937767e-06\n",
      "Epoch: 28550\tTrain loss: 1.1198740423878917e-06\n",
      "Epoch: 28600\tTrain loss: 0.0005384175019571558\n",
      "Epoch: 28650\tTrain loss: 0.09488129056990147\n",
      "Epoch: 28700\tTrain loss: 0.0021616220474243164\n",
      "Epoch: 28750\tTrain loss: 0.00019472832718747668\n",
      "Epoch: 28800\tTrain loss: 0.00013360112006921554\n",
      "Epoch: 28850\tTrain loss: 6.09283515624702\n",
      "Epoch: 28900\tTrain loss: 0.0039018986863084137\n",
      "Epoch: 28950\tTrain loss: 0.0006860971479909495\n",
      "Epoch: 29000\tTrain loss: 1.3011816143989563\n",
      "Epoch: 29050\tTrain loss: 0.001953471568413079\n",
      "Epoch: 29100\tTrain loss: 6.285305062192492e-05\n",
      "Epoch: 29150\tTrain loss: 1.5746729559396044e-05\n",
      "Epoch: 29200\tTrain loss: 0.00014319931779027684\n",
      "Epoch: 29250\tTrain loss: 9.405188620803528e-06\n",
      "Epoch: 29300\tTrain loss: 4.689933561508042e-07\n",
      "Epoch: 29350\tTrain loss: 3.6813992103645887e-06\n",
      "Epoch: 29400\tTrain loss: 129.17461967468262\n",
      "Epoch: 29450\tTrain loss: 2.8750967979431152\n",
      "Epoch: 29500\tTrain loss: 1.0738642811775208\n",
      "Epoch: 29550\tTrain loss: 0.30371368676424026\n",
      "Epoch: 29600\tTrain loss: 0.10674172081053257\n",
      "Epoch: 29650\tTrain loss: 0.04721840750426054\n",
      "Epoch: 29700\tTrain loss: 0.023708301596343517\n",
      "Epoch: 29750\tTrain loss: 0.012874308973550797\n",
      "Epoch: 29800\tTrain loss: 0.007392888655886054\n",
      "Epoch: 29850\tTrain loss: 0.004413432558067143\n",
      "Epoch: 29900\tTrain loss: 0.00269368349108845\n",
      "Epoch: 29950\tTrain loss: 0.0016693917132215574\n",
      "Epoch: 30000\tTrain loss: 0.001040016024489887\n",
      "Epoch: 30050\tTrain loss: 0.0006456431801780127\n",
      "Epoch: 30100\tTrain loss: 0.0003994569292444794\n",
      "Epoch: 30150\tTrain loss: 0.0002463401651766617\n",
      "Epoch: 30200\tTrain loss: 0.0001477620317018591\n",
      "Epoch: 30250\tTrain loss: 8.772995806793915e-05\n",
      "Epoch: 30300\tTrain loss: 5.1214743052696576e-05\n",
      "Epoch: 30350\tTrain loss: 0.17360002448185696\n",
      "Epoch: 30400\tTrain loss: 0.017097673378884792\n",
      "Epoch: 30450\tTrain loss: 0.0010463543585501611\n",
      "Epoch: 30500\tTrain loss: 0.0001518002900411375\n",
      "Epoch: 30550\tTrain loss: 3.396042484382633e-05\n",
      "Epoch: 30600\tTrain loss: 1.0615887276799185e-05\n",
      "Epoch: 30650\tTrain loss: 3.980586541274533e-06\n",
      "Epoch: 30700\tTrain loss: 1.670730597425063e-06\n",
      "Epoch: 30750\tTrain loss: 7.89559770453252e-07\n",
      "Epoch: 30800\tTrain loss: 4.1310882892275913e-07\n",
      "Epoch: 30850\tTrain loss: 2.422214784303378e-07\n",
      "Epoch: 30900\tTrain loss: 1.5870771363779568e-07\n",
      "Epoch: 30950\tTrain loss: 1.2852251707329287e-07\n",
      "Epoch: 31000\tTrain loss: 1.0857668897301664e-07\n",
      "Epoch: 31050\tTrain loss: 9.238720188875504e-08\n",
      "Epoch: 31100\tTrain loss: 1.7181504414764959e-06\n",
      "Epoch: 31150\tTrain loss: 0.0009259683920390671\n",
      "Epoch: 31200\tTrain loss: 8.51774984766962e-05\n",
      "Epoch: 31250\tTrain loss: 0.6265804227441549\n",
      "Epoch: 31300\tTrain loss: 2.4465914368629456\n",
      "Epoch: 31350\tTrain loss: 0.016896291635930538\n",
      "Epoch: 31400\tTrain loss: 0.004779445822350681\n",
      "Epoch: 31450\tTrain loss: 0.001932344282977283\n",
      "Epoch: 31500\tTrain loss: 0.0008259431924670935\n",
      "Epoch: 31550\tTrain loss: 0.00034072591370204464\n",
      "Epoch: 31600\tTrain loss: 0.00013373939145822078\n",
      "Epoch: 31650\tTrain loss: 5.270077608088286e-05\n",
      "Epoch: 31700\tTrain loss: 2.1686337731807726e-05\n",
      "Epoch: 31750\tTrain loss: 0.00011469339915493038\n",
      "Epoch: 31800\tTrain loss: 1.6475288020956214e-05\n",
      "Epoch: 31850\tTrain loss: 0.5663913190364838\n",
      "Epoch: 31900\tTrain loss: 0.0007699811030761339\n",
      "Epoch: 31950\tTrain loss: 0.0001353871266474016\n",
      "Epoch: 32000\tTrain loss: 3.6650182664743625e-05\n",
      "Epoch: 32050\tTrain loss: 1.1165409205204924e-05\n",
      "Epoch: 32100\tTrain loss: 3.489443308524187e-06\n",
      "Epoch: 32150\tTrain loss: 1.1379830588964523e-06\n",
      "Epoch: 32200\tTrain loss: 4.0416811231125394e-07\n",
      "Epoch: 32250\tTrain loss: 1.6517005629879122e-07\n",
      "Epoch: 32300\tTrain loss: 8.645260418660428e-08\n",
      "Epoch: 32350\tTrain loss: 6.139485408951373e-08\n",
      "Epoch: 32400\tTrain loss: 4.835633848898624e-08\n",
      "Epoch: 32450\tTrain loss: 4.292362110192016e-08\n",
      "Epoch: 32500\tTrain loss: 3.8711975847149915e-08\n",
      "Epoch: 32550\tTrain loss: 3.337239284917359e-08\n",
      "Epoch: 32600\tTrain loss: 3.179949148446326e-08\n",
      "Epoch: 32650\tTrain loss: 6.010135056300214e-08\n",
      "Epoch: 32700\tTrain loss: 6.238216315068712e-05\n",
      "Epoch: 32750\tTrain loss: 0.0003340667753946036\n",
      "Epoch: 32800\tTrain loss: 0.09394439321476966\n",
      "Epoch: 32850\tTrain loss: 0.05765483621507883\n",
      "Epoch: 32900\tTrain loss: 0.00037664287083316594\n",
      "Epoch: 32950\tTrain loss: 0.0017291568801738322\n",
      "Epoch: 33000\tTrain loss: 0.00021705621475121006\n",
      "Epoch: 33050\tTrain loss: 1.1787905620508354e-06\n",
      "Epoch: 33100\tTrain loss: 0.0026535714277997613\n",
      "Epoch: 33150\tTrain loss: 0.01172578742261976\n",
      "Epoch: 33200\tTrain loss: 0.0019528537486621644\n",
      "Epoch: 33250\tTrain loss: 0.21117828227579594\n",
      "Epoch: 33300\tTrain loss: 0.1795134418644011\n",
      "Epoch: 33350\tTrain loss: 0.004373437695903704\n",
      "Epoch: 33400\tTrain loss: 0.00132072742053424\n",
      "Epoch: 33450\tTrain loss: 3.992031565758225e-06\n",
      "Epoch: 33500\tTrain loss: 6.923141597781068e-07\n",
      "Epoch: 33550\tTrain loss: 1.331636045875939e-07\n",
      "Epoch: 33600\tTrain loss: 5.944942405022857e-08\n",
      "Epoch: 33650\tTrain loss: 4.176981915904321e-08\n",
      "Epoch: 33700\tTrain loss: 3.4991858743893545e-08\n",
      "Epoch: 33750\tTrain loss: 3.9063808188188887e-08\n",
      "Epoch: 33800\tTrain loss: 3.7842741384963574e-08\n",
      "Epoch: 33850\tTrain loss: 2.2636313268264985e-08\n",
      "Epoch: 33900\tTrain loss: 6.982849676262504e-08\n",
      "Epoch: 33950\tTrain loss: 0.003224683227017522\n",
      "Epoch: 34000\tTrain loss: 85.17147874832153\n",
      "Epoch: 34050\tTrain loss: 0.21630656719207764\n",
      "Epoch: 34100\tTrain loss: 0.01853000372648239\n",
      "Epoch: 34150\tTrain loss: 0.17041383683681488\n",
      "Epoch: 34200\tTrain loss: 0.0029682738822884858\n",
      "Epoch: 34250\tTrain loss: 0.0006893000099807978\n",
      "Epoch: 34300\tTrain loss: 0.0003242881502956152\n",
      "Epoch: 34350\tTrain loss: 0.00017120801931014284\n",
      "Epoch: 34400\tTrain loss: 9.294855044572614e-05\n",
      "Epoch: 34450\tTrain loss: 4.9938020111994774e-05\n",
      "Epoch: 34500\tTrain loss: 2.678097280295333e-05\n",
      "Epoch: 34550\tTrain loss: 1.4008778634888586e-05\n",
      "Epoch: 34600\tTrain loss: 7.288996130228043e-06\n",
      "Epoch: 34650\tTrain loss: 3.671863396448316e-06\n",
      "Epoch: 34700\tTrain loss: 1.8064346818391641e-06\n",
      "Epoch: 34750\tTrain loss: 9.193705743371083e-07\n",
      "Epoch: 34800\tTrain loss: 4.588522948267837e-07\n",
      "Epoch: 34850\tTrain loss: 2.484561807403196e-07\n",
      "Epoch: 34900\tTrain loss: 1.4594859365502089e-07\n",
      "Epoch: 34950\tTrain loss: 8.907583293193966e-08\n",
      "Epoch: 35000\tTrain loss: 1.2033205187833573e-07\n",
      "Epoch: 35050\tTrain loss: 7.255328706001762e-05\n",
      "Epoch: 35100\tTrain loss: 0.0013637945521622896\n",
      "Epoch: 35150\tTrain loss: 0.0374821238219738\n",
      "Epoch: 35200\tTrain loss: 0.03642160352319479\n",
      "Epoch: 35250\tTrain loss: 0.019681508652865887\n",
      "Epoch: 35300\tTrain loss: 0.0033004949218593538\n",
      "Epoch: 35350\tTrain loss: 7.096672197803855e-05\n",
      "Epoch: 35400\tTrain loss: 0.0008331613789778203\n",
      "Epoch: 35450\tTrain loss: 196.61005401611328\n",
      "Epoch: 35500\tTrain loss: 1272.0763549804688\n",
      "Epoch: 35550\tTrain loss: 514.5158882141113\n",
      "Epoch: 35600\tTrain loss: 77.14932250976562\n",
      "Epoch: 35650\tTrain loss: 31.79120922088623\n",
      "Epoch: 35700\tTrain loss: 17.47554302215576\n",
      "Epoch: 35750\tTrain loss: 10.464999198913574\n",
      "Epoch: 35800\tTrain loss: 7.808460712432861\n",
      "Epoch: 35850\tTrain loss: 6.125649690628052\n",
      "Epoch: 35900\tTrain loss: 4.939592719078064\n",
      "Epoch: 35950\tTrain loss: 4.63267707824707\n",
      "Epoch: 36000\tTrain loss: 4.50157618522644\n",
      "Epoch: 36050\tTrain loss: 2.8230202198028564\n",
      "Epoch: 36100\tTrain loss: 2.352582097053528\n",
      "Epoch: 36150\tTrain loss: 1.959916889667511\n",
      "Epoch: 36200\tTrain loss: 1.6303462386131287\n",
      "Epoch: 36250\tTrain loss: 1.3506699204444885\n",
      "Epoch: 36300\tTrain loss: 1.1178542971611023\n",
      "Epoch: 36350\tTrain loss: 0.9274504482746124\n",
      "Epoch: 36400\tTrain loss: 0.7655537724494934\n",
      "Epoch: 36450\tTrain loss: 0.6371332705020905\n",
      "Epoch: 36500\tTrain loss: 1.007313221693039\n",
      "Epoch: 36550\tTrain loss: 0.45498697459697723\n",
      "Epoch: 36600\tTrain loss: 0.3818155601620674\n",
      "Epoch: 36650\tTrain loss: 0.32531876862049103\n",
      "Epoch: 36700\tTrain loss: 0.2787822484970093\n",
      "Epoch: 36750\tTrain loss: 0.23972611129283905\n",
      "Epoch: 36800\tTrain loss: 0.20677822455763817\n",
      "Epoch: 36850\tTrain loss: 0.17909693717956543\n",
      "Epoch: 36900\tTrain loss: 0.15569493919610977\n",
      "Epoch: 36950\tTrain loss: 0.20728051289916039\n",
      "Epoch: 37000\tTrain loss: 0.30644915252923965\n",
      "Epoch: 37050\tTrain loss: 0.10727168247103691\n",
      "Epoch: 37100\tTrain loss: 0.0946134477853775\n",
      "Epoch: 37150\tTrain loss: 0.08873434364795685\n",
      "Epoch: 37200\tTrain loss: 0.1176380068063736\n",
      "Epoch: 37250\tTrain loss: 0.07662057317793369\n",
      "Epoch: 37300\tTrain loss: 0.8321173787117004\n",
      "Epoch: 37350\tTrain loss: 0.07661805115640163\n",
      "Epoch: 37400\tTrain loss: 0.05170711036771536\n",
      "Epoch: 37450\tTrain loss: 0.0446929382160306\n",
      "Epoch: 37500\tTrain loss: 0.0390236871317029\n",
      "Epoch: 37550\tTrain loss: 0.03421422187238932\n",
      "Epoch: 37600\tTrain loss: 0.030187806580215693\n",
      "Epoch: 37650\tTrain loss: 0.026628327555954456\n",
      "Epoch: 37700\tTrain loss: 0.023570061661303043\n",
      "Epoch: 37750\tTrain loss: 0.020898489048704505\n",
      "Epoch: 37800\tTrain loss: 0.02276230789721012\n",
      "Epoch: 37850\tTrain loss: 0.04279637150466442\n",
      "Epoch: 37900\tTrain loss: 0.046612322330474854\n",
      "Epoch: 37950\tTrain loss: 0.03253116086125374\n",
      "Epoch: 38000\tTrain loss: 0.019673491828143597\n",
      "Epoch: 38050\tTrain loss: 0.0148182837292552\n",
      "Epoch: 38100\tTrain loss: 0.01194195612333715\n",
      "Epoch: 38150\tTrain loss: 0.011606368352659047\n",
      "Epoch: 38200\tTrain loss: 0.011898106429725885\n",
      "Epoch: 38250\tTrain loss: 0.010321565205231309\n",
      "Epoch: 38300\tTrain loss: 0.006078626378439367\n",
      "Epoch: 38350\tTrain loss: 0.14166230615228415\n",
      "Epoch: 38400\tTrain loss: 0.13643549755215645\n",
      "Epoch: 38450\tTrain loss: 0.010885565541684628\n",
      "Epoch: 38500\tTrain loss: 0.07589313015341759\n",
      "Epoch: 38550\tTrain loss: 0.010085753863677382\n",
      "Epoch: 38600\tTrain loss: 0.0020833027665503323\n",
      "Epoch: 38650\tTrain loss: 0.0013602768594864756\n",
      "Epoch: 38700\tTrain loss: 0.00100040651159361\n",
      "Epoch: 38750\tTrain loss: 0.0007324906546273269\n",
      "Epoch: 38800\tTrain loss: 0.000551677992916666\n",
      "Epoch: 38850\tTrain loss: 0.0013857629492122214\n",
      "Epoch: 38900\tTrain loss: 0.16219639778137207\n",
      "Epoch: 38950\tTrain loss: 0.2572553185746074\n",
      "Epoch: 39000\tTrain loss: 1000.0385284423828\n",
      "Epoch: 39050\tTrain loss: 212.03982162475586\n",
      "Epoch: 39100\tTrain loss: 235.3478126525879\n",
      "Epoch: 39150\tTrain loss: 165.61356353759766\n",
      "Epoch: 39200\tTrain loss: 136.0636978149414\n",
      "Epoch: 39250\tTrain loss: 58.99735164642334\n",
      "Epoch: 39300\tTrain loss: 39.86264419555664\n",
      "Epoch: 39350\tTrain loss: 31.171120643615723\n",
      "Epoch: 39400\tTrain loss: 25.372223377227783\n",
      "Epoch: 39450\tTrain loss: 20.928953170776367\n",
      "Epoch: 39500\tTrain loss: 17.32857656478882\n",
      "Epoch: 39550\tTrain loss: 14.357101440429688\n",
      "Epoch: 39600\tTrain loss: 11.924686968326569\n",
      "Epoch: 39650\tTrain loss: 9.955605208873749\n",
      "Epoch: 39700\tTrain loss: 8.8317391872406\n",
      "Epoch: 39750\tTrain loss: 6.827299356460571\n",
      "Epoch: 39800\tTrain loss: 5.623198360204697\n",
      "Epoch: 39850\tTrain loss: 5.8660807609558105\n",
      "Epoch: 39900\tTrain loss: 3.6800639629364014\n",
      "Epoch: 39950\tTrain loss: 2.916025996208191\n",
      "Epoch: 40000\tTrain loss: 2.3050082623958588\n",
      "Epoch: 40050\tTrain loss: 1.8330870121717453\n",
      "Epoch: 40100\tTrain loss: 1.4704913049936295\n",
      "Epoch: 40150\tTrain loss: 1.1980279982089996\n",
      "Epoch: 40200\tTrain loss: 0.9834315627813339\n",
      "Epoch: 40250\tTrain loss: 1.6502252221107483\n",
      "Epoch: 40300\tTrain loss: 0.6813933849334717\n",
      "Epoch: 40350\tTrain loss: 0.9534211754798889\n",
      "Epoch: 40400\tTrain loss: 0.650274932384491\n",
      "Epoch: 40450\tTrain loss: 0.42592138051986694\n",
      "Epoch: 40500\tTrain loss: 0.3416600972414017\n",
      "Epoch: 40550\tTrain loss: 0.42735935747623444\n",
      "Epoch: 40600\tTrain loss: 0.3041265048086643\n",
      "Epoch: 40650\tTrain loss: 0.21248393319547176\n",
      "Epoch: 40700\tTrain loss: 0.16635210812091827\n",
      "Epoch: 40750\tTrain loss: 0.13699938310310245\n",
      "Epoch: 40800\tTrain loss: 0.11271180585026741\n",
      "Epoch: 40850\tTrain loss: 0.09935647249221802\n",
      "Epoch: 40900\tTrain loss: 0.08842466631904244\n",
      "Epoch: 40950\tTrain loss: 6.187213525176048\n",
      "Epoch: 41000\tTrain loss: 0.9032825529575348\n",
      "Epoch: 41050\tTrain loss: 0.14753219485282898\n",
      "Epoch: 41100\tTrain loss: 0.03720133099704981\n",
      "Epoch: 41150\tTrain loss: 0.024972179904580116\n",
      "Epoch: 41200\tTrain loss: 0.03370136581361294\n",
      "Epoch: 41250\tTrain loss: 0.09726154431700706\n",
      "Epoch: 41300\tTrain loss: 0.1284084040671587\n",
      "Epoch: 41350\tTrain loss: 0.012045981828123331\n",
      "Epoch: 41400\tTrain loss: 0.005021220771595836\n",
      "Epoch: 41450\tTrain loss: 0.0028916378505527973\n",
      "Epoch: 41500\tTrain loss: 0.001907452620798722\n",
      "Epoch: 41550\tTrain loss: 0.00132558069890365\n",
      "Epoch: 41600\tTrain loss: 0.0009361500997329131\n",
      "Epoch: 41650\tTrain loss: 0.0006658526544924825\n",
      "Epoch: 41700\tTrain loss: 0.0004735781840281561\n",
      "Epoch: 41750\tTrain loss: 0.00037412453093566\n",
      "Epoch: 41800\tTrain loss: 0.00024475945974700153\n",
      "Epoch: 41850\tTrain loss: 0.00018501909016777063\n",
      "Epoch: 41900\tTrain loss: 0.00012592909115483053\n",
      "Epoch: 41950\tTrain loss: 0.000210371816137922\n",
      "Epoch: 42000\tTrain loss: 0.9839364178478718\n",
      "Epoch: 42050\tTrain loss: 0.045844811596907675\n",
      "Epoch: 42100\tTrain loss: 0.0016499304329045117\n",
      "Epoch: 42150\tTrain loss: 0.00013405428762780502\n",
      "Epoch: 42200\tTrain loss: 4.070157547175768e-05\n",
      "Epoch: 42250\tTrain loss: 1.775102373358095e-05\n",
      "Epoch: 42300\tTrain loss: 8.507098868903995e-06\n",
      "Epoch: 42350\tTrain loss: 4.304453909753647e-06\n",
      "Epoch: 42400\tTrain loss: 2.262586122014909e-06\n",
      "Epoch: 42450\tTrain loss: 1.2341938315785228e-06\n",
      "Epoch: 42500\tTrain loss: 7.305811671187712e-07\n",
      "Epoch: 42550\tTrain loss: 4.639331621092424e-07\n",
      "Epoch: 42600\tTrain loss: 0.000999078998574987\n",
      "Epoch: 42650\tTrain loss: 0.05600856663659215\n",
      "Epoch: 42700\tTrain loss: 2.170406937599182\n",
      "Epoch: 42750\tTrain loss: 0.17732958495616913\n",
      "Epoch: 42800\tTrain loss: 0.05163895897567272\n",
      "Epoch: 42850\tTrain loss: 0.02051763702183962\n",
      "Epoch: 42900\tTrain loss: 0.009265168569982052\n",
      "Epoch: 42950\tTrain loss: 0.005599628202617168\n",
      "Epoch: 43000\tTrain loss: 0.006064089713618159\n",
      "Epoch: 43050\tTrain loss: 0.004457951232325286\n",
      "Epoch: 43100\tTrain loss: 0.0011981463467236608\n",
      "Epoch: 43150\tTrain loss: 0.01924023637548089\n",
      "Epoch: 43200\tTrain loss: 0.019487016135826707\n",
      "Epoch: 43250\tTrain loss: 0.07192565500736237\n",
      "Epoch: 43300\tTrain loss: 0.0033525406615808606\n",
      "Epoch: 43350\tTrain loss: 0.003522913553752005\n",
      "Epoch: 43400\tTrain loss: 0.0005669796373695135\n",
      "Epoch: 43450\tTrain loss: 0.0014533829235006124\n",
      "Epoch: 43500\tTrain loss: 0.007395436055958271\n",
      "Epoch: 43550\tTrain loss: 0.0006140363257145509\n",
      "Epoch: 43600\tTrain loss: 0.02646301733329892\n",
      "Epoch: 43650\tTrain loss: 0.019463351170998067\n",
      "Epoch: 43700\tTrain loss: 0.00032043844839790836\n",
      "Epoch: 43750\tTrain loss: 0.000511667676619254\n",
      "Epoch: 43800\tTrain loss: 0.03323172405362129\n",
      "Epoch: 43850\tTrain loss: 0.37866032868623734\n",
      "Epoch: 43900\tTrain loss: 0.6105178445577621\n",
      "Epoch: 43950\tTrain loss: 0.015867849346250296\n",
      "Epoch: 44000\tTrain loss: 0.0011881680693477392\n",
      "Epoch: 44050\tTrain loss: 0.0002122588484780863\n",
      "Epoch: 44100\tTrain loss: 0.0005113539809826761\n",
      "Epoch: 44150\tTrain loss: 3.391131815533299e-05\n",
      "Epoch: 44200\tTrain loss: 0.002034753590123728\n",
      "Epoch: 44250\tTrain loss: 0.00025997727061621845\n",
      "Epoch: 44300\tTrain loss: 0.00048309191333828494\n",
      "Epoch: 44350\tTrain loss: 0.08069843286648393\n",
      "Epoch: 44400\tTrain loss: 143.60966110229492\n",
      "Epoch: 44450\tTrain loss: 11.722139120101929\n",
      "Epoch: 44500\tTrain loss: 4.144914507865906\n",
      "Epoch: 44550\tTrain loss: 1.8468916416168213\n",
      "Epoch: 44600\tTrain loss: 1.0605534762144089\n",
      "Epoch: 44650\tTrain loss: 0.6963744461536407\n",
      "Epoch: 44700\tTrain loss: 0.4827475845813751\n",
      "Epoch: 44750\tTrain loss: 0.3422929160296917\n",
      "Epoch: 44800\tTrain loss: 0.2451721727848053\n",
      "Epoch: 44850\tTrain loss: 0.17607249319553375\n",
      "Epoch: 44900\tTrain loss: 0.12696227990090847\n",
      "Epoch: 44950\tTrain loss: 0.09096307959407568\n",
      "Epoch: 45000\tTrain loss: 0.06468416890129447\n",
      "Epoch: 45050\tTrain loss: 0.04600942134857178\n",
      "Epoch: 45100\tTrain loss: 0.032463314011693\n",
      "Epoch: 45150\tTrain loss: 0.022724203066900373\n",
      "Epoch: 45200\tTrain loss: 0.01584330282639712\n",
      "Epoch: 45250\tTrain loss: 0.011010775720933452\n",
      "Epoch: 45300\tTrain loss: 0.0075712821271736175\n",
      "Epoch: 45350\tTrain loss: 0.0052028162172064185\n",
      "Epoch: 45400\tTrain loss: 0.0035472390009090304\n",
      "Epoch: 45450\tTrain loss: 0.002413731242995709\n",
      "Epoch: 45500\tTrain loss: 0.0016323766176356003\n",
      "Epoch: 45550\tTrain loss: 0.0011097431124653667\n",
      "Epoch: 45600\tTrain loss: 0.0007352250686381012\n",
      "Epoch: 45650\tTrain loss: 0.00048727966168371495\n",
      "Epoch: 45700\tTrain loss: 0.0003209444203093881\n",
      "Epoch: 45750\tTrain loss: 0.0002078789384540869\n",
      "Epoch: 45800\tTrain loss: 0.00013422522465589282\n",
      "Epoch: 45850\tTrain loss: 0.00010093863420479465\n",
      "Epoch: 45900\tTrain loss: 0.20267732557840645\n",
      "Epoch: 45950\tTrain loss: 0.30195070430636406\n",
      "Epoch: 46000\tTrain loss: 0.020559524651616812\n",
      "Epoch: 46050\tTrain loss: 0.00030401347612496465\n",
      "Epoch: 46100\tTrain loss: 0.0009182476933347061\n",
      "Epoch: 46150\tTrain loss: 4.8680201871320605e-05\n",
      "Epoch: 46200\tTrain loss: 2.2365168206306407e-05\n",
      "Epoch: 46250\tTrain loss: 7.423376473525423e-05\n",
      "Epoch: 46300\tTrain loss: 0.0037947277542116353\n",
      "Epoch: 46350\tTrain loss: 0.24896657839417458\n",
      "Epoch: 46400\tTrain loss: 0.027483400888741016\n",
      "Epoch: 46450\tTrain loss: 0.003077006433159113\n",
      "Epoch: 46500\tTrain loss: 0.01553002482978627\n",
      "Epoch: 46550\tTrain loss: 0.7313916236162186\n",
      "Epoch: 46600\tTrain loss: 0.012523772427812219\n",
      "Epoch: 46650\tTrain loss: 0.006897049956023693\n",
      "Epoch: 46700\tTrain loss: 0.0002638776641106233\n",
      "Epoch: 46750\tTrain loss: 8.15687180875102e-05\n",
      "Epoch: 46800\tTrain loss: 0.0019786410848610103\n",
      "Epoch: 46850\tTrain loss: 0.0009025763429235667\n",
      "Epoch: 46900\tTrain loss: 0.010242313612252474\n",
      "Epoch: 46950\tTrain loss: 0.009648278122767806\n",
      "Epoch: 47000\tTrain loss: 0.23923806101083755\n",
      "Epoch: 47050\tTrain loss: 0.0005841981619596481\n",
      "Epoch: 47100\tTrain loss: 7.512988486269023e-05\n",
      "Epoch: 47150\tTrain loss: 1.3596844155472354e-05\n",
      "Epoch: 47200\tTrain loss: 2.284185029566288e-05\n",
      "Epoch: 47250\tTrain loss: 5.8559804756441736e-06\n",
      "Epoch: 47300\tTrain loss: 0.00011294264936623222\n",
      "Epoch: 47350\tTrain loss: 0.0001268866503778554\n",
      "Epoch: 47400\tTrain loss: 3.361407038937614e-06\n",
      "Epoch: 47450\tTrain loss: 0.00010221834418189246\n",
      "Epoch: 47500\tTrain loss: 0.0006318406558420975\n",
      "Epoch: 47550\tTrain loss: 0.1427578255534172\n",
      "Epoch: 47600\tTrain loss: 0.4854422025382519\n",
      "Epoch: 47650\tTrain loss: 0.08671186491847038\n",
      "Epoch: 47700\tTrain loss: 0.055322274565696716\n",
      "Epoch: 47750\tTrain loss: 1.2562783062458038\n",
      "Epoch: 47800\tTrain loss: 0.025308568961918354\n",
      "Epoch: 47850\tTrain loss: 6.924133413122036e-05\n",
      "Epoch: 47900\tTrain loss: 1.0091262538480805e-05\n",
      "Epoch: 47950\tTrain loss: 2.372849507992214e-06\n",
      "Epoch: 48000\tTrain loss: 5.726909506620359e-07\n",
      "Epoch: 48050\tTrain loss: 2.137126600132433e-07\n",
      "Epoch: 48100\tTrain loss: 1.1490968176985916e-07\n",
      "Epoch: 48150\tTrain loss: 1.0143137885165743e-07\n",
      "Epoch: 48200\tTrain loss: 6.863847268334666e-08\n",
      "Epoch: 48250\tTrain loss: 5.572144345933339e-06\n",
      "Epoch: 48300\tTrain loss: 0.00024173837300622836\n",
      "Epoch: 48350\tTrain loss: 0.0005479715982801281\n",
      "Epoch: 48400\tTrain loss: 0.0055996955052251\n",
      "Epoch: 48450\tTrain loss: 1.4882175922393799\n",
      "Epoch: 48500\tTrain loss: 0.3913358375430107\n",
      "Epoch: 48550\tTrain loss: 0.01770417857915163\n",
      "Epoch: 48600\tTrain loss: 0.002744738245382905\n",
      "Epoch: 48650\tTrain loss: 0.00047570839524269104\n",
      "Epoch: 48700\tTrain loss: 9.515934289083816e-05\n",
      "Epoch: 48750\tTrain loss: 2.161197153327521e-05\n",
      "Epoch: 48800\tTrain loss: 5.588695898950391e-06\n",
      "Epoch: 48850\tTrain loss: 1.5652996978587908e-06\n",
      "Epoch: 48900\tTrain loss: 4.805986577594012e-07\n",
      "Epoch: 48950\tTrain loss: 1.8155098047145657e-07\n",
      "Epoch: 49000\tTrain loss: 1.4879430665359905e-07\n",
      "Epoch: 49050\tTrain loss: 2.5142605686312436e-07\n",
      "Epoch: 49100\tTrain loss: 2.205257999321475e-06\n",
      "Epoch: 49150\tTrain loss: 0.0011680361944854667\n",
      "Epoch: 49200\tTrain loss: 0.29924894217401743\n",
      "Epoch: 49250\tTrain loss: 32.25530253350735\n",
      "Epoch: 49300\tTrain loss: 874.1978378295898\n",
      "Epoch: 49350\tTrain loss: 48.47994899749756\n",
      "Epoch: 49400\tTrain loss: 24.554842948913574\n",
      "Epoch: 49450\tTrain loss: 18.549494981765747\n",
      "Epoch: 49500\tTrain loss: 14.557211995124817\n",
      "Epoch: 49550\tTrain loss: 9.962690591812134\n",
      "Epoch: 49600\tTrain loss: 21.84936237335205\n",
      "Epoch: 49650\tTrain loss: 10.24937915802002\n",
      "Epoch: 49700\tTrain loss: 7.296175718307495\n",
      "Epoch: 49750\tTrain loss: 5.477751135826111\n",
      "Epoch: 49800\tTrain loss: 4.222715318202972\n",
      "Epoch: 49850\tTrain loss: 3.3245319724082947\n",
      "Epoch: 49900\tTrain loss: 2.663411021232605\n",
      "Epoch: 49950\tTrain loss: 2.1738537549972534\n",
      "Training CFRNN\n",
      "Epoch: 0\tTrain loss: 1534304.1875\n",
      "Epoch: 50\tTrain loss: 1485784.625\n",
      "Epoch: 100\tTrain loss: 1439450.625\n",
      "Epoch: 150\tTrain loss: 1396021.3125\n",
      "Epoch: 200\tTrain loss: 1354508.390625\n",
      "Epoch: 250\tTrain loss: 1314326.9375\n",
      "Epoch: 300\tTrain loss: 1275576.3125\n",
      "Epoch: 350\tTrain loss: 1238022.375\n",
      "Epoch: 400\tTrain loss: 1201561.2734375\n",
      "Epoch: 450\tTrain loss: 1166352.4375\n",
      "Epoch: 500\tTrain loss: 1132113.6875\n",
      "Epoch: 550\tTrain loss: 1098884.28125\n",
      "Epoch: 600\tTrain loss: 1066639.65625\n",
      "Epoch: 650\tTrain loss: 1035551.21875\n",
      "Epoch: 700\tTrain loss: 1005325.21875\n",
      "Epoch: 750\tTrain loss: 976016.1875\n",
      "Epoch: 800\tTrain loss: 947635.0\n",
      "Epoch: 850\tTrain loss: 920179.5\n",
      "Epoch: 900\tTrain loss: 893510.3125\n",
      "Epoch: 950\tTrain loss: 867602.421875\n",
      "Epoch: 1000\tTrain loss: 842675.1875\n",
      "Epoch: 1050\tTrain loss: 818398.125\n",
      "Epoch: 1100\tTrain loss: 794914.625\n",
      "Epoch: 1150\tTrain loss: 772273.28125\n",
      "Epoch: 1200\tTrain loss: 750416.0625\n",
      "Epoch: 1250\tTrain loss: 729369.515625\n",
      "Epoch: 1300\tTrain loss: 708913.671875\n",
      "Epoch: 1350\tTrain loss: 689275.8125\n",
      "Epoch: 1400\tTrain loss: 670269.453125\n",
      "Epoch: 1450\tTrain loss: 652029.3671875\n",
      "Epoch: 1500\tTrain loss: 634422.15625\n",
      "Epoch: 1550\tTrain loss: 617528.0625\n",
      "Epoch: 1600\tTrain loss: 601295.4921875\n",
      "Epoch: 1650\tTrain loss: 585616.546875\n",
      "Epoch: 1700\tTrain loss: 570537.671875\n",
      "Epoch: 1750\tTrain loss: 556099.09375\n",
      "Epoch: 1800\tTrain loss: 542239.953125\n",
      "Epoch: 1850\tTrain loss: 528965.10546875\n",
      "Epoch: 1900\tTrain loss: 516260.125\n",
      "Epoch: 1950\tTrain loss: 504156.71875\n",
      "Epoch: 2000\tTrain loss: 492605.90625\n",
      "Epoch: 2050\tTrain loss: 481595.953125\n",
      "Epoch: 2100\tTrain loss: 471085.296875\n",
      "Epoch: 2150\tTrain loss: 461067.421875\n",
      "Epoch: 2200\tTrain loss: 451587.0\n",
      "Epoch: 2250\tTrain loss: 442633.390625\n",
      "Epoch: 2300\tTrain loss: 434115.41796875\n",
      "Epoch: 2350\tTrain loss: 426105.640625\n",
      "Epoch: 2400\tTrain loss: 418494.25\n",
      "Epoch: 2450\tTrain loss: 411290.59375\n",
      "Epoch: 2500\tTrain loss: 404568.4296875\n",
      "Epoch: 2550\tTrain loss: 398143.5703125\n",
      "Epoch: 2600\tTrain loss: 392196.34375\n",
      "Epoch: 2650\tTrain loss: 386560.078125\n",
      "Epoch: 2700\tTrain loss: 381370.41796875\n",
      "Epoch: 2750\tTrain loss: 376398.375\n",
      "Epoch: 2800\tTrain loss: 371894.40625\n",
      "Epoch: 2850\tTrain loss: 367606.3046875\n",
      "Epoch: 2900\tTrain loss: 363663.203125\n",
      "Epoch: 2950\tTrain loss: 309574.3984375\n",
      "Epoch: 3000\tTrain loss: 269097.59375\n",
      "Epoch: 3050\tTrain loss: 258118.390625\n",
      "Epoch: 3100\tTrain loss: 247832.8992919922\n",
      "Epoch: 3150\tTrain loss: 238223.03125\n",
      "Epoch: 3200\tTrain loss: 228969.9453125\n",
      "Epoch: 3250\tTrain loss: 219455.4140625\n",
      "Epoch: 3300\tTrain loss: 210232.09375\n",
      "Epoch: 3350\tTrain loss: 201454.5546875\n",
      "Epoch: 3400\tTrain loss: 193004.64453125\n",
      "Epoch: 3450\tTrain loss: 184955.72358703613\n",
      "Epoch: 3500\tTrain loss: 177290.19384765625\n",
      "Epoch: 3550\tTrain loss: 169910.59057617188\n",
      "Epoch: 3600\tTrain loss: 162830.0078125\n",
      "Epoch: 3650\tTrain loss: 156049.59375\n",
      "Epoch: 3700\tTrain loss: 149615.55078125\n",
      "Epoch: 3750\tTrain loss: 143370.71484375\n",
      "Epoch: 3800\tTrain loss: 137418.80859375\n",
      "Epoch: 3850\tTrain loss: 131808.27639770508\n",
      "Epoch: 3900\tTrain loss: 126367.033203125\n",
      "Epoch: 3950\tTrain loss: 121185.22552490234\n",
      "Epoch: 4000\tTrain loss: 116295.986328125\n",
      "Epoch: 4050\tTrain loss: 111620.12109375\n",
      "Epoch: 4100\tTrain loss: 106885.52716064453\n",
      "Epoch: 4150\tTrain loss: 102134.7421875\n",
      "Epoch: 4200\tTrain loss: 97515.8819732666\n",
      "Epoch: 4250\tTrain loss: 93062.798828125\n",
      "Epoch: 4300\tTrain loss: 88740.39849853516\n",
      "Epoch: 4350\tTrain loss: 84488.337890625\n",
      "Epoch: 4400\tTrain loss: 80417.36666870117\n",
      "Epoch: 4450\tTrain loss: 76503.2392578125\n",
      "Epoch: 4500\tTrain loss: 72719.4716796875\n",
      "Epoch: 4550\tTrain loss: 69046.57345581055\n",
      "Epoch: 4600\tTrain loss: 65571.9912109375\n",
      "Epoch: 4650\tTrain loss: 62140.97296142578\n",
      "Epoch: 4700\tTrain loss: 58961.0185546875\n",
      "Epoch: 4750\tTrain loss: 55805.563415527344\n",
      "Epoch: 4800\tTrain loss: 52805.76318359375\n",
      "Epoch: 4850\tTrain loss: 49938.52978515625\n",
      "Epoch: 4900\tTrain loss: 47204.97189331055\n",
      "Epoch: 4950\tTrain loss: 44577.09619140625\n",
      "Epoch: 5000\tTrain loss: 42071.65673828125\n",
      "Epoch: 5050\tTrain loss: 39754.04833984375\n",
      "Epoch: 5100\tTrain loss: 37446.05894470215\n",
      "Epoch: 5150\tTrain loss: 35295.732177734375\n",
      "Epoch: 5200\tTrain loss: 33264.346435546875\n",
      "Epoch: 5250\tTrain loss: 31333.73583984375\n",
      "Epoch: 5300\tTrain loss: 29504.841247558594\n",
      "Epoch: 5350\tTrain loss: 27792.803344726562\n",
      "Epoch: 5400\tTrain loss: 26199.302978515625\n",
      "Epoch: 5450\tTrain loss: 24675.58235168457\n",
      "Epoch: 5500\tTrain loss: 23278.392211914062\n",
      "Epoch: 5550\tTrain loss: 21939.060485839844\n",
      "Epoch: 5600\tTrain loss: 20711.435485839844\n",
      "Epoch: 5650\tTrain loss: 19564.72654724121\n",
      "Epoch: 5700\tTrain loss: 18518.016876220703\n",
      "Epoch: 5750\tTrain loss: 17541.828582763672\n",
      "Epoch: 5800\tTrain loss: 16631.163360595703\n",
      "Epoch: 5850\tTrain loss: 15866.197662353516\n",
      "Epoch: 5900\tTrain loss: 15078.84262084961\n",
      "Epoch: 5950\tTrain loss: 14373.971618652344\n",
      "Epoch: 6000\tTrain loss: 13678.832336425781\n",
      "Epoch: 6050\tTrain loss: 12785.762420654297\n",
      "Epoch: 6100\tTrain loss: 12024.597229003906\n",
      "Epoch: 6150\tTrain loss: 11293.428802490234\n",
      "Epoch: 6200\tTrain loss: 10591.54116821289\n",
      "Epoch: 6250\tTrain loss: 9932.49380493164\n",
      "Epoch: 6300\tTrain loss: 9276.200057983398\n",
      "Epoch: 6350\tTrain loss: 8671.404388427734\n",
      "Epoch: 6400\tTrain loss: 8082.71614074707\n",
      "Epoch: 6450\tTrain loss: 7539.751739501953\n",
      "Epoch: 6500\tTrain loss: 7004.549133300781\n",
      "Epoch: 6550\tTrain loss: 6529.663421630859\n",
      "Epoch: 6600\tTrain loss: 6047.829559326172\n",
      "Epoch: 6650\tTrain loss: 5658.7352294921875\n",
      "Epoch: 6700\tTrain loss: 5205.2098388671875\n",
      "Epoch: 6750\tTrain loss: 4820.732147216797\n",
      "Epoch: 6800\tTrain loss: 4466.745819091797\n",
      "Epoch: 6850\tTrain loss: 4133.224212646484\n",
      "Epoch: 6900\tTrain loss: 3844.4712829589844\n",
      "Epoch: 6950\tTrain loss: 3546.188262939453\n",
      "Epoch: 7000\tTrain loss: 3284.0562438964844\n",
      "Epoch: 7050\tTrain loss: 3047.2911376953125\n",
      "Epoch: 7100\tTrain loss: 2836.9190063476562\n",
      "Epoch: 7150\tTrain loss: 2630.8326110839844\n",
      "Epoch: 7200\tTrain loss: 2446.679656982422\n",
      "Epoch: 7250\tTrain loss: 2283.024642944336\n",
      "Epoch: 7300\tTrain loss: 2135.944549560547\n",
      "Epoch: 7350\tTrain loss: 2002.4903106689453\n",
      "Epoch: 7400\tTrain loss: 1916.2423706054688\n",
      "Epoch: 7450\tTrain loss: 1774.001708984375\n",
      "Epoch: 7500\tTrain loss: 1676.8548278808594\n",
      "Epoch: 7550\tTrain loss: 1588.5274963378906\n",
      "Epoch: 7600\tTrain loss: 1509.9408874511719\n",
      "Epoch: 7650\tTrain loss: 1439.4143142700195\n",
      "Epoch: 7700\tTrain loss: 1375.7018127441406\n",
      "Epoch: 7750\tTrain loss: 1320.3768005371094\n",
      "Epoch: 7800\tTrain loss: 1272.417709350586\n",
      "Epoch: 7850\tTrain loss: 1219.7679748535156\n",
      "Epoch: 7900\tTrain loss: 1176.507568359375\n",
      "Epoch: 7950\tTrain loss: 1137.4851684570312\n",
      "Epoch: 8000\tTrain loss: 1100.6034088134766\n",
      "Epoch: 8050\tTrain loss: 1067.0775680541992\n",
      "Epoch: 8100\tTrain loss: 1039.6666717529297\n",
      "Epoch: 8150\tTrain loss: 1006.5378723144531\n",
      "Epoch: 8200\tTrain loss: 1072.9719696044922\n",
      "Epoch: 8250\tTrain loss: 953.4619445800781\n",
      "Epoch: 8300\tTrain loss: 927.3548202514648\n",
      "Epoch: 8350\tTrain loss: 902.6116638183594\n",
      "Epoch: 8400\tTrain loss: 878.7876892089844\n",
      "Epoch: 8450\tTrain loss: 855.7685317993164\n",
      "Epoch: 8500\tTrain loss: 833.3976440429688\n",
      "Epoch: 8550\tTrain loss: 811.6477127075195\n",
      "Epoch: 8600\tTrain loss: 790.3807678222656\n",
      "Epoch: 8650\tTrain loss: 769.328727722168\n",
      "Epoch: 8700\tTrain loss: 749.5276184082031\n",
      "Epoch: 8750\tTrain loss: 728.7516632080078\n",
      "Epoch: 8800\tTrain loss: 708.6806106567383\n",
      "Epoch: 8850\tTrain loss: 689.1127319335938\n",
      "Epoch: 8900\tTrain loss: 669.1934814453125\n",
      "Epoch: 8950\tTrain loss: 648.9413909912109\n",
      "Epoch: 9000\tTrain loss: 628.3279342651367\n",
      "Epoch: 9050\tTrain loss: 607.2077331542969\n",
      "Epoch: 9100\tTrain loss: 592.7945404052734\n",
      "Epoch: 9150\tTrain loss: 568.2476501464844\n",
      "Epoch: 9200\tTrain loss: 547.7584533691406\n",
      "Epoch: 9250\tTrain loss: 527.5936393737793\n",
      "Epoch: 9300\tTrain loss: 508.1373596191406\n",
      "Epoch: 9350\tTrain loss: 489.20094299316406\n",
      "Epoch: 9400\tTrain loss: 470.702880859375\n",
      "Epoch: 9450\tTrain loss: 452.64678955078125\n",
      "Epoch: 9500\tTrain loss: 435.0206184387207\n",
      "Epoch: 9550\tTrain loss: 418.3561820983887\n",
      "Epoch: 9600\tTrain loss: 402.0196838378906\n",
      "Epoch: 9650\tTrain loss: 386.61956787109375\n",
      "Epoch: 9700\tTrain loss: 11684.902221679688\n",
      "Epoch: 9750\tTrain loss: 363.5513229370117\n",
      "Epoch: 9800\tTrain loss: 345.6154327392578\n",
      "Epoch: 9850\tTrain loss: 329.96221923828125\n",
      "Epoch: 9900\tTrain loss: 315.2890853881836\n",
      "Epoch: 9950\tTrain loss: 301.60723876953125\n",
      "Epoch: 10000\tTrain loss: 288.04947090148926\n",
      "Epoch: 10050\tTrain loss: 276.4300537109375\n",
      "Epoch: 10100\tTrain loss: 262.94986724853516\n",
      "Epoch: 10150\tTrain loss: 250.747314453125\n",
      "Epoch: 10200\tTrain loss: 238.7724666595459\n",
      "Epoch: 10250\tTrain loss: 18931.67431640625\n",
      "Epoch: 10300\tTrain loss: 12695.371704101562\n",
      "Epoch: 10350\tTrain loss: 11161.55419921875\n",
      "Epoch: 10400\tTrain loss: 10350.377960205078\n",
      "Epoch: 10450\tTrain loss: 9869.060302734375\n",
      "Epoch: 10500\tTrain loss: 9579.96337890625\n",
      "Epoch: 10550\tTrain loss: 9354.693115234375\n",
      "Epoch: 10600\tTrain loss: 9223.5673828125\n",
      "Epoch: 10650\tTrain loss: 9110.460205078125\n",
      "Epoch: 10700\tTrain loss: 1748.2992248535156\n",
      "Epoch: 10750\tTrain loss: 1363.0791778564453\n",
      "Epoch: 10800\tTrain loss: 1127.6812362670898\n",
      "Epoch: 10850\tTrain loss: 964.6383743286133\n",
      "Epoch: 10900\tTrain loss: 831.3222675323486\n",
      "Epoch: 10950\tTrain loss: 721.2377433776855\n",
      "Epoch: 11000\tTrain loss: 630.4797401428223\n",
      "Epoch: 11050\tTrain loss: 554.1728744506836\n",
      "Epoch: 11100\tTrain loss: 491.94681549072266\n",
      "Epoch: 11150\tTrain loss: 440.9259033203125\n",
      "Epoch: 11200\tTrain loss: 399.77880859375\n",
      "Epoch: 11250\tTrain loss: 366.4896926879883\n",
      "Epoch: 11300\tTrain loss: 342.0281982421875\n",
      "Epoch: 11350\tTrain loss: 317.7579803466797\n",
      "Epoch: 11400\tTrain loss: 299.2201232910156\n",
      "Epoch: 11450\tTrain loss: 283.9323616027832\n",
      "Epoch: 11500\tTrain loss: 270.9805221557617\n",
      "Epoch: 11550\tTrain loss: 259.9212875366211\n",
      "Epoch: 11600\tTrain loss: 250.3074188232422\n",
      "Epoch: 11650\tTrain loss: 241.63629531860352\n",
      "Epoch: 11700\tTrain loss: 233.87183380126953\n",
      "Epoch: 11750\tTrain loss: 226.6760368347168\n",
      "Epoch: 11800\tTrain loss: 219.75201606750488\n",
      "Epoch: 11850\tTrain loss: 213.31855010986328\n",
      "Epoch: 11900\tTrain loss: 206.9041519165039\n",
      "Epoch: 11950\tTrain loss: 200.6816864013672\n",
      "Epoch: 12000\tTrain loss: 194.61749267578125\n",
      "Epoch: 12050\tTrain loss: 188.52822875976562\n",
      "Epoch: 12100\tTrain loss: 182.58123016357422\n",
      "Epoch: 12150\tTrain loss: 176.71036529541016\n",
      "Epoch: 12200\tTrain loss: 170.8508758544922\n",
      "Epoch: 12250\tTrain loss: 165.0324935913086\n",
      "Epoch: 12300\tTrain loss: 159.20115661621094\n",
      "Epoch: 12350\tTrain loss: 153.46796417236328\n",
      "Epoch: 12400\tTrain loss: 148.09334564208984\n",
      "Epoch: 12450\tTrain loss: 3061.060546875\n",
      "Epoch: 12500\tTrain loss: 249.43418884277344\n",
      "Epoch: 12550\tTrain loss: 225.80751037597656\n",
      "Epoch: 12600\tTrain loss: 207.74154663085938\n",
      "Epoch: 12650\tTrain loss: 192.47389221191406\n",
      "Epoch: 12700\tTrain loss: 178.79447174072266\n",
      "Epoch: 12750\tTrain loss: 166.2302017211914\n",
      "Epoch: 12800\tTrain loss: 154.53688430786133\n",
      "Epoch: 12850\tTrain loss: 142.89920806884766\n",
      "Epoch: 12900\tTrain loss: 127.6401252746582\n",
      "Epoch: 12950\tTrain loss: 137.4444923400879\n",
      "Epoch: 13000\tTrain loss: 123.1685791015625\n",
      "Epoch: 13050\tTrain loss: 114.58835983276367\n",
      "Epoch: 13100\tTrain loss: 108.51954650878906\n",
      "Epoch: 13150\tTrain loss: 102.76680374145508\n",
      "Epoch: 13200\tTrain loss: 97.07382583618164\n",
      "Epoch: 13250\tTrain loss: 91.16796875\n",
      "Epoch: 13300\tTrain loss: 85.1581802368164\n",
      "Epoch: 13350\tTrain loss: 79.50911903381348\n",
      "Epoch: 13400\tTrain loss: 76.24031066894531\n",
      "Epoch: 13450\tTrain loss: 70.9134635925293\n",
      "Epoch: 13500\tTrain loss: 66.08523941040039\n",
      "Epoch: 13550\tTrain loss: 61.629350662231445\n",
      "Epoch: 13600\tTrain loss: 62.87784194946289\n",
      "Epoch: 13650\tTrain loss: 57.187618255615234\n",
      "Epoch: 13700\tTrain loss: 52.83517265319824\n",
      "Epoch: 13750\tTrain loss: 296.5612335205078\n",
      "Epoch: 13800\tTrain loss: 51.17607879638672\n",
      "Epoch: 13850\tTrain loss: 47.659427642822266\n",
      "Epoch: 13900\tTrain loss: 44.52497482299805\n",
      "Epoch: 13950\tTrain loss: 41.55543518066406\n",
      "Epoch: 14000\tTrain loss: 38.73257637023926\n",
      "Epoch: 14050\tTrain loss: 36.047075271606445\n",
      "Epoch: 14100\tTrain loss: 33.50977420806885\n",
      "Epoch: 14150\tTrain loss: 45.50642204284668\n",
      "Epoch: 14200\tTrain loss: 28.858028411865234\n",
      "Epoch: 14250\tTrain loss: 26.661200523376465\n",
      "Epoch: 14300\tTrain loss: 24.593442916870117\n",
      "Epoch: 14350\tTrain loss: 22.636016845703125\n",
      "Epoch: 14400\tTrain loss: 20.778846740722656\n",
      "Epoch: 14450\tTrain loss: 20.06735849380493\n",
      "Epoch: 14500\tTrain loss: 25.74540901184082\n",
      "Epoch: 14550\tTrain loss: 18.2957124710083\n",
      "Epoch: 14600\tTrain loss: 16.790857315063477\n",
      "Epoch: 14650\tTrain loss: 15.440380334854126\n",
      "Epoch: 14700\tTrain loss: 14.197747230529785\n",
      "Epoch: 14750\tTrain loss: 13.053414821624756\n",
      "Epoch: 14800\tTrain loss: 12.037550926208496\n",
      "Epoch: 14850\tTrain loss: 11.307687282562256\n",
      "Epoch: 14900\tTrain loss: 10.180900573730469\n",
      "Epoch: 14950\tTrain loss: 9.904611587524414\n",
      "Epoch: 15000\tTrain loss: 8.711480379104614\n",
      "Epoch: 15050\tTrain loss: 7.980340838432312\n",
      "Epoch: 15100\tTrain loss: 7.308881402015686\n",
      "Epoch: 15150\tTrain loss: 6.892611026763916\n",
      "Epoch: 15200\tTrain loss: 6.729195833206177\n",
      "Epoch: 15250\tTrain loss: 6.534517765045166\n",
      "Epoch: 15300\tTrain loss: 5.237696051597595\n",
      "Epoch: 15350\tTrain loss: 4.895701885223389\n",
      "Epoch: 15400\tTrain loss: 4.348240256309509\n",
      "Epoch: 15450\tTrain loss: 4.090335726737976\n",
      "Epoch: 15500\tTrain loss: 3.7264662981033325\n",
      "Epoch: 15550\tTrain loss: 3.5292487144470215\n",
      "Epoch: 15600\tTrain loss: 4.793233335018158\n",
      "Epoch: 15650\tTrain loss: 3.210944175720215\n",
      "Epoch: 15700\tTrain loss: 2.6733967065811157\n",
      "Epoch: 15750\tTrain loss: 2.042813539505005\n",
      "Epoch: 15800\tTrain loss: 1.7999547570943832\n",
      "Epoch: 15850\tTrain loss: 1.5801098346710205\n",
      "Epoch: 15900\tTrain loss: 1.5017595365643501\n",
      "Epoch: 15950\tTrain loss: 22.881373643875122\n",
      "Epoch: 16000\tTrain loss: 1.2138160467147827\n",
      "Epoch: 16050\tTrain loss: 0.879630982875824\n",
      "Epoch: 16100\tTrain loss: 0.7561573684215546\n",
      "Epoch: 16150\tTrain loss: 0.6478838548064232\n",
      "Epoch: 16200\tTrain loss: 0.5516983568668365\n",
      "Epoch: 16250\tTrain loss: 0.46689459681510925\n",
      "Epoch: 16300\tTrain loss: 0.3914955612272024\n",
      "Epoch: 16350\tTrain loss: 0.3266594633460045\n",
      "Epoch: 16400\tTrain loss: 0.26982200238853693\n",
      "Epoch: 16450\tTrain loss: 0.24994266219437122\n",
      "Epoch: 16500\tTrain loss: 0.23464514315128326\n",
      "Epoch: 16550\tTrain loss: 5.770025610923767\n",
      "Epoch: 16600\tTrain loss: 0.2890201136469841\n",
      "Epoch: 16650\tTrain loss: 0.1058640256524086\n",
      "Epoch: 16700\tTrain loss: 0.0854444894939661\n",
      "Epoch: 16750\tTrain loss: 0.0687091255094856\n",
      "Epoch: 16800\tTrain loss: 0.054907562414882705\n",
      "Epoch: 16850\tTrain loss: 0.04354000592138618\n",
      "Epoch: 16900\tTrain loss: 0.034198720823042095\n",
      "Epoch: 16950\tTrain loss: 0.02670392778236419\n",
      "Epoch: 17000\tTrain loss: 0.020682215807028115\n",
      "Epoch: 17050\tTrain loss: 0.01560025109210983\n",
      "Epoch: 17100\tTrain loss: 0.016788957873359323\n",
      "Epoch: 17150\tTrain loss: 1.58591478317976\n",
      "Epoch: 17200\tTrain loss: 4.292262077331543\n",
      "Epoch: 17250\tTrain loss: 0.04908730648458004\n",
      "Epoch: 17300\tTrain loss: 1.1512043625116348\n",
      "Epoch: 17350\tTrain loss: 0.34212353453040123\n",
      "Epoch: 17400\tTrain loss: 0.02397141233086586\n",
      "Epoch: 17450\tTrain loss: 0.046813955530524254\n",
      "Epoch: 17500\tTrain loss: 0.02398670371621847\n",
      "Epoch: 17550\tTrain loss: 0.07158460840582848\n",
      "Epoch: 17600\tTrain loss: 3.1852738857269287\n",
      "Epoch: 17650\tTrain loss: 0.02032360341399908\n",
      "Epoch: 17700\tTrain loss: 0.0021082686726003885\n",
      "Epoch: 17750\tTrain loss: 0.0236556651070714\n",
      "Epoch: 17800\tTrain loss: 0.012002673000097275\n",
      "Epoch: 17850\tTrain loss: 0.1229640319943428\n",
      "Epoch: 17900\tTrain loss: 0.37931227684020996\n",
      "Epoch: 17950\tTrain loss: 0.01006429735571146\n",
      "Epoch: 18000\tTrain loss: 0.28602235019207\n",
      "Epoch: 18050\tTrain loss: 11.727691173553467\n",
      "Epoch: 18100\tTrain loss: 0.3091738931834698\n",
      "Epoch: 18150\tTrain loss: 0.0211788946762681\n",
      "Epoch: 18200\tTrain loss: 0.009724992793053389\n",
      "Epoch: 18250\tTrain loss: 0.005162892863154411\n",
      "Epoch: 18300\tTrain loss: 0.003080529044382274\n",
      "Epoch: 18350\tTrain loss: 0.001987587893381715\n",
      "Epoch: 18400\tTrain loss: 0.0013447047676891088\n",
      "Epoch: 18450\tTrain loss: 0.0009315050265286118\n",
      "Epoch: 18500\tTrain loss: 0.0006528024387080222\n",
      "Epoch: 18550\tTrain loss: 0.0004562579415505752\n",
      "Epoch: 18600\tTrain loss: 0.00031719706021249294\n",
      "Epoch: 18650\tTrain loss: 0.0002173407892769319\n",
      "Epoch: 18700\tTrain loss: 0.0001473611364417593\n",
      "Epoch: 18750\tTrain loss: 9.886714178719558e-05\n",
      "Epoch: 18800\tTrain loss: 6.499183291452937e-05\n",
      "Epoch: 18850\tTrain loss: 4.2032269448100124e-05\n",
      "Epoch: 18900\tTrain loss: 2.7075177058577538e-05\n",
      "Epoch: 18950\tTrain loss: 1.66549934874638e-05\n",
      "Epoch: 19000\tTrain loss: 2.2567989617527928e-05\n",
      "Epoch: 19050\tTrain loss: 0.06592098227702081\n",
      "Epoch: 19100\tTrain loss: 2.776229977607727\n",
      "Epoch: 19150\tTrain loss: 0.32975277304649353\n",
      "Epoch: 19200\tTrain loss: 0.23861200362443924\n",
      "Epoch: 19250\tTrain loss: 0.03482533828355372\n",
      "Epoch: 19300\tTrain loss: 0.004111688103876077\n",
      "Epoch: 19350\tTrain loss: 0.0010084986429319542\n",
      "Epoch: 19400\tTrain loss: 0.0005869680026080459\n",
      "Epoch: 19450\tTrain loss: 0.0003440060681896284\n",
      "Epoch: 19500\tTrain loss: 0.00019924729713238776\n",
      "Epoch: 19550\tTrain loss: 0.00012425834756868426\n",
      "Epoch: 19600\tTrain loss: 0.00016856769434525631\n",
      "Epoch: 19650\tTrain loss: 0.00013666586528415792\n",
      "Epoch: 19700\tTrain loss: 0.11574055254459381\n",
      "Epoch: 19750\tTrain loss: 51.636547565460205\n",
      "Epoch: 19800\tTrain loss: 0.09854547306895256\n",
      "Epoch: 19850\tTrain loss: 0.008204205427318811\n",
      "Epoch: 19900\tTrain loss: 0.003271068911999464\n",
      "Epoch: 19950\tTrain loss: 0.0015893774689175189\n",
      "Epoch: 20000\tTrain loss: 0.0008356466714758426\n",
      "Epoch: 20050\tTrain loss: 0.0004470495623536408\n",
      "Epoch: 20100\tTrain loss: 0.00023634023091290146\n",
      "Epoch: 20150\tTrain loss: 0.00012543292905320413\n",
      "Epoch: 20200\tTrain loss: 6.53963961667614e-05\n",
      "Epoch: 20250\tTrain loss: 3.368930811120663e-05\n",
      "Epoch: 20300\tTrain loss: 1.7372897218592698e-05\n",
      "Epoch: 20350\tTrain loss: 9.761287401488516e-06\n",
      "Epoch: 20400\tTrain loss: 0.00046544586803065613\n",
      "Epoch: 20450\tTrain loss: 0.027034873608499765\n",
      "Epoch: 20500\tTrain loss: 49.19466590881348\n",
      "Epoch: 20550\tTrain loss: 0.31165725737810135\n",
      "Epoch: 20600\tTrain loss: 0.0477309413254261\n",
      "Epoch: 20650\tTrain loss: 0.02378189191222191\n",
      "Epoch: 20700\tTrain loss: 0.012400958687067032\n",
      "Epoch: 20750\tTrain loss: 0.006642637716140598\n",
      "Epoch: 20800\tTrain loss: 0.0036348241847008467\n",
      "Epoch: 20850\tTrain loss: 0.001985918788705021\n",
      "Epoch: 20900\tTrain loss: 0.0010838581074494869\n",
      "Epoch: 20950\tTrain loss: 0.0005865086932317354\n",
      "Epoch: 21000\tTrain loss: 0.0003114329156232998\n",
      "Epoch: 21050\tTrain loss: 0.00016772231901995838\n",
      "Epoch: 21100\tTrain loss: 8.650557765577105e-05\n",
      "Epoch: 21150\tTrain loss: 4.4464561142376624e-05\n",
      "Epoch: 21200\tTrain loss: 2.270321510877693e-05\n",
      "Epoch: 21250\tTrain loss: 1.1447694141963893e-05\n",
      "Epoch: 21300\tTrain loss: 5.744819290498526e-06\n",
      "Epoch: 21350\tTrain loss: 3.0557948775822297e-06\n",
      "Epoch: 21400\tTrain loss: 1.4908637666621871e-06\n",
      "Epoch: 21450\tTrain loss: 7.561165170955064e-05\n",
      "Epoch: 21500\tTrain loss: 3.0056951800361276e-05\n",
      "Epoch: 21550\tTrain loss: 0.391981640830636\n",
      "Epoch: 21600\tTrain loss: 2.8618812561035156\n",
      "Epoch: 21650\tTrain loss: 0.023938184836879373\n",
      "Epoch: 21700\tTrain loss: 0.007168367737904191\n",
      "Epoch: 21750\tTrain loss: 0.0032773477723821998\n",
      "Epoch: 21800\tTrain loss: 0.0015458596753887832\n",
      "Epoch: 21850\tTrain loss: 0.0007457085303030908\n",
      "Epoch: 21900\tTrain loss: 0.0003566875366232125\n",
      "Epoch: 21950\tTrain loss: 0.00016882728141354164\n",
      "Epoch: 22000\tTrain loss: 7.852250200812705e-05\n",
      "Epoch: 22050\tTrain loss: 3.5824592487188056e-05\n",
      "Epoch: 22100\tTrain loss: 1.606462956260657e-05\n",
      "Epoch: 22150\tTrain loss: 7.199069159469218e-06\n",
      "Epoch: 22200\tTrain loss: 3.175675487909757e-06\n",
      "Epoch: 22250\tTrain loss: 1.546546513964131e-06\n",
      "Epoch: 22300\tTrain loss: 2.0666737214014574e-05\n",
      "Epoch: 22350\tTrain loss: 0.02271051425486803\n",
      "Epoch: 22400\tTrain loss: 2.3150352239608765\n",
      "Epoch: 22450\tTrain loss: 0.8997175693511963\n",
      "Epoch: 22500\tTrain loss: 0.07520430721342564\n",
      "Epoch: 22550\tTrain loss: 0.004719595890492201\n",
      "Epoch: 22600\tTrain loss: 0.0057244632771471515\n",
      "Epoch: 22650\tTrain loss: 0.011135876935441047\n",
      "Epoch: 22700\tTrain loss: 0.034829368349164724\n",
      "Epoch: 22750\tTrain loss: 0.06127259135246277\n",
      "Epoch: 22800\tTrain loss: 1.274588257074356\n",
      "Epoch: 22850\tTrain loss: 0.007541254512034357\n",
      "Epoch: 22900\tTrain loss: 0.0016843948978930712\n",
      "Epoch: 22950\tTrain loss: 0.0004337652135291137\n",
      "Epoch: 23000\tTrain loss: 0.00013682802273251582\n",
      "Epoch: 23050\tTrain loss: 5.991177977193729e-05\n",
      "Epoch: 23100\tTrain loss: 2.7631125249172328e-05\n",
      "Epoch: 23150\tTrain loss: 1.0739579920482356e-05\n",
      "Epoch: 23200\tTrain loss: 4.501368039200315e-06\n",
      "Epoch: 23250\tTrain loss: 1.8812741586771153e-06\n",
      "Epoch: 23300\tTrain loss: 1.163095134870673e-06\n",
      "Epoch: 23350\tTrain loss: 5.016601278384769e-06\n",
      "Epoch: 23400\tTrain loss: 7.234670374600682e-05\n",
      "Epoch: 23450\tTrain loss: 0.0012102918467462587\n",
      "Epoch: 23500\tTrain loss: 0.05888390319887549\n",
      "Epoch: 23550\tTrain loss: 2.342553496360779\n",
      "Epoch: 23600\tTrain loss: 6.955912709236145\n",
      "Epoch: 23650\tTrain loss: 0.006857814732939005\n",
      "Epoch: 23700\tTrain loss: 0.0025154014001600444\n",
      "Epoch: 23750\tTrain loss: 0.0011295612348476425\n",
      "Epoch: 23800\tTrain loss: 0.0005147635856701527\n",
      "Epoch: 23850\tTrain loss: 0.00023093966592568904\n",
      "Epoch: 23900\tTrain loss: 0.00010041031418950297\n",
      "Epoch: 23950\tTrain loss: 4.236560198478401e-05\n",
      "Epoch: 24000\tTrain loss: 1.7186572563332447e-05\n",
      "Epoch: 24050\tTrain loss: 6.852817705294001e-06\n",
      "Epoch: 24100\tTrain loss: 2.8701007295239833e-06\n",
      "Epoch: 24150\tTrain loss: 1.1718702062069042e-06\n",
      "Epoch: 24200\tTrain loss: 4.791680368043671e-07\n",
      "Epoch: 24250\tTrain loss: 3.9637734516873024e-07\n",
      "Epoch: 24300\tTrain loss: 2.227581319402816e-07\n",
      "Epoch: 24350\tTrain loss: 7.18357547668802e-07\n",
      "Epoch: 24400\tTrain loss: 1.774042539182119e-05\n",
      "Epoch: 24450\tTrain loss: 0.008019812391012238\n",
      "Epoch: 24500\tTrain loss: 32351.705520629883\n",
      "Epoch: 24550\tTrain loss: 1459.353012084961\n",
      "Epoch: 24600\tTrain loss: 1155.0587997436523\n",
      "Epoch: 24650\tTrain loss: 979.9739532470703\n",
      "Epoch: 24700\tTrain loss: 854.7322998046875\n",
      "Epoch: 24750\tTrain loss: 763.8262786865234\n",
      "Epoch: 24800\tTrain loss: 691.5082321166992\n",
      "Epoch: 24850\tTrain loss: 692.0945167541504\n",
      "Epoch: 24900\tTrain loss: 598.5168762207031\n",
      "Epoch: 24950\tTrain loss: 558.8451919555664\n",
      "Epoch: 25000\tTrain loss: 548.803108215332\n",
      "Epoch: 25050\tTrain loss: 505.3682556152344\n",
      "Epoch: 25100\tTrain loss: 487.9666213989258\n",
      "Epoch: 25150\tTrain loss: 464.1306457519531\n",
      "Epoch: 25200\tTrain loss: 446.14173889160156\n",
      "Epoch: 25250\tTrain loss: 429.5112495422363\n",
      "Epoch: 25300\tTrain loss: 414.3632469177246\n",
      "Epoch: 25350\tTrain loss: 399.41955947875977\n",
      "Epoch: 25400\tTrain loss: 385.73085021972656\n",
      "Epoch: 25450\tTrain loss: 372.14248275756836\n",
      "Epoch: 25500\tTrain loss: 359.7098579406738\n",
      "Epoch: 25550\tTrain loss: 373.7178421020508\n",
      "Epoch: 25600\tTrain loss: 337.41534423828125\n",
      "Epoch: 25650\tTrain loss: 324.8300018310547\n",
      "Epoch: 25700\tTrain loss: 313.4256820678711\n",
      "Epoch: 25750\tTrain loss: 303.2149887084961\n",
      "Epoch: 25800\tTrain loss: 291.85050201416016\n",
      "Epoch: 25850\tTrain loss: 280.1426467895508\n",
      "Epoch: 25900\tTrain loss: 269.90186309814453\n",
      "Epoch: 25950\tTrain loss: 259.8868637084961\n",
      "Epoch: 26000\tTrain loss: 250.09628295898438\n",
      "Epoch: 26050\tTrain loss: 240.90095138549805\n",
      "Epoch: 26100\tTrain loss: 231.12942504882812\n",
      "Epoch: 26150\tTrain loss: 221.8887939453125\n",
      "Epoch: 26200\tTrain loss: 212.67383766174316\n",
      "Epoch: 26250\tTrain loss: 203.4429702758789\n",
      "Epoch: 26300\tTrain loss: 194.46405029296875\n",
      "Epoch: 26350\tTrain loss: 185.89363861083984\n",
      "Epoch: 26400\tTrain loss: 177.4036464691162\n",
      "Epoch: 26450\tTrain loss: 169.04029083251953\n",
      "Epoch: 26500\tTrain loss: 160.80508422851562\n",
      "Epoch: 26550\tTrain loss: 152.5856704711914\n",
      "Epoch: 26600\tTrain loss: 144.66534423828125\n",
      "Epoch: 26650\tTrain loss: 136.89179229736328\n",
      "Epoch: 26700\tTrain loss: 129.51670837402344\n",
      "Epoch: 26750\tTrain loss: 122.43629837036133\n",
      "Epoch: 26800\tTrain loss: 115.86840057373047\n",
      "Epoch: 26850\tTrain loss: 109.38026809692383\n",
      "Epoch: 26900\tTrain loss: 103.19690322875977\n",
      "Epoch: 26950\tTrain loss: 97.30998992919922\n",
      "Epoch: 27000\tTrain loss: 91.73437881469727\n",
      "Epoch: 27050\tTrain loss: 105.59897232055664\n",
      "Epoch: 27100\tTrain loss: 85.14182662963867\n",
      "Epoch: 27150\tTrain loss: 77.93451404571533\n",
      "Epoch: 27200\tTrain loss: 72.80734634399414\n",
      "Epoch: 27250\tTrain loss: 68.19407653808594\n",
      "Epoch: 27300\tTrain loss: 63.84451484680176\n",
      "Epoch: 27350\tTrain loss: 59.674747467041016\n",
      "Epoch: 27400\tTrain loss: 55.7973575592041\n",
      "Epoch: 27450\tTrain loss: 52.03106498718262\n",
      "Epoch: 27500\tTrain loss: 48.43050503730774\n",
      "Epoch: 27550\tTrain loss: 45.043785095214844\n",
      "Epoch: 27600\tTrain loss: 41.75873303413391\n",
      "Epoch: 27650\tTrain loss: 38.711533308029175\n",
      "Epoch: 27700\tTrain loss: 37.2482967376709\n",
      "Epoch: 27750\tTrain loss: 33.42202091217041\n",
      "Epoch: 27800\tTrain loss: 30.646607875823975\n",
      "Epoch: 27850\tTrain loss: 27.6193505525589\n",
      "Epoch: 27900\tTrain loss: 25.97625160217285\n",
      "Epoch: 27950\tTrain loss: 23.773819088935852\n",
      "Epoch: 28000\tTrain loss: 21.769317626953125\n",
      "Epoch: 28050\tTrain loss: 19.867170453071594\n",
      "Epoch: 28100\tTrain loss: 18.98682451248169\n",
      "Epoch: 28150\tTrain loss: 16.47858715057373\n",
      "Epoch: 28200\tTrain loss: 14.932052791118622\n",
      "Epoch: 28250\tTrain loss: 13.504305362701416\n",
      "Epoch: 28300\tTrain loss: 12.157796621322632\n",
      "Epoch: 28350\tTrain loss: 10.94588565826416\n",
      "Epoch: 28400\tTrain loss: 9.817418098449707\n",
      "Epoch: 28450\tTrain loss: 8.770087659358978\n",
      "Epoch: 28500\tTrain loss: 7.8224382400512695\n",
      "Epoch: 28550\tTrain loss: 6.975165605545044\n",
      "Epoch: 28600\tTrain loss: 16.833885192871094\n",
      "Epoch: 28650\tTrain loss: 5.600941896438599\n",
      "Epoch: 28700\tTrain loss: 4.970439672470093\n",
      "Epoch: 28750\tTrain loss: 4.251045793294907\n",
      "Epoch: 28800\tTrain loss: 3.727970004081726\n",
      "Epoch: 28850\tTrain loss: 3.2405163049697876\n",
      "Epoch: 28900\tTrain loss: 2.8130979537963867\n",
      "Epoch: 28950\tTrain loss: 2.436566710472107\n",
      "Epoch: 29000\tTrain loss: 2.0966106057167053\n",
      "Epoch: 29050\tTrain loss: 1.8043835163116455\n",
      "Epoch: 29100\tTrain loss: 1.5444080829620361\n",
      "Epoch: 29150\tTrain loss: 1.3263763785362244\n",
      "Epoch: 29200\tTrain loss: 1.2043824791908264\n",
      "Epoch: 29250\tTrain loss: 0.9899513423442841\n",
      "Epoch: 29300\tTrain loss: 1.2604740262031555\n",
      "Epoch: 29350\tTrain loss: 2.3861421942710876\n",
      "Epoch: 29400\tTrain loss: 0.6587830483913422\n",
      "Epoch: 29450\tTrain loss: 0.4704059883952141\n",
      "Epoch: 29500\tTrain loss: 0.40506018698215485\n",
      "Epoch: 29550\tTrain loss: 0.38817398250102997\n",
      "Epoch: 29600\tTrain loss: 0.2664339765906334\n",
      "Epoch: 29650\tTrain loss: 0.29728519171476364\n",
      "Epoch: 29700\tTrain loss: 0.19320864975452423\n",
      "Epoch: 29750\tTrain loss: 0.1503068506717682\n",
      "Epoch: 29800\tTrain loss: 0.16744646430015564\n",
      "Epoch: 29850\tTrain loss: 0.29080265015363693\n",
      "Epoch: 29900\tTrain loss: 0.1228102259337902\n",
      "Epoch: 29950\tTrain loss: 1.1570192277431488\n",
      "Epoch: 30000\tTrain loss: 0.05386696197092533\n",
      "Epoch: 30050\tTrain loss: 0.046936819329857826\n",
      "Epoch: 30100\tTrain loss: 0.8697646036744118\n",
      "Epoch: 30150\tTrain loss: 0.4625811129808426\n",
      "Epoch: 30200\tTrain loss: 0.08859185874462128\n",
      "Epoch: 30250\tTrain loss: 0.04056532494723797\n",
      "Epoch: 30300\tTrain loss: 0.01562069309875369\n",
      "Epoch: 30350\tTrain loss: 0.011779978405684233\n",
      "Epoch: 30400\tTrain loss: 0.017173569183796644\n",
      "Epoch: 30450\tTrain loss: 2.464359164237976\n",
      "Epoch: 30500\tTrain loss: 0.08395828306674957\n",
      "Epoch: 30550\tTrain loss: 0.012149210553616285\n",
      "Epoch: 30600\tTrain loss: 0.00664896797388792\n",
      "Epoch: 30650\tTrain loss: 0.004405497340485454\n",
      "Epoch: 30700\tTrain loss: 0.0030660692718811333\n",
      "Epoch: 30750\tTrain loss: 0.0022134847240522504\n",
      "Epoch: 30800\tTrain loss: 0.0016173143521882594\n",
      "Epoch: 30850\tTrain loss: 0.0011672130785882473\n",
      "Epoch: 30900\tTrain loss: 0.0008811149746179581\n",
      "Epoch: 30950\tTrain loss: 0.1005884725600481\n",
      "Epoch: 31000\tTrain loss: 0.12757306639105082\n",
      "Epoch: 31050\tTrain loss: 1.2384580075740814\n",
      "Epoch: 31100\tTrain loss: 1.7942188382148743\n",
      "Epoch: 31150\tTrain loss: 0.013248109258711338\n",
      "Epoch: 31200\tTrain loss: 0.004572842153720558\n",
      "Epoch: 31250\tTrain loss: 0.0018290483858436346\n",
      "Epoch: 31300\tTrain loss: 0.000877489015692845\n",
      "Epoch: 31350\tTrain loss: 0.000457566467957804\n",
      "Epoch: 31400\tTrain loss: 0.00025100097991526127\n",
      "Epoch: 31450\tTrain loss: 0.00014089182513998821\n",
      "Epoch: 31500\tTrain loss: 7.97722223069286e-05\n",
      "Epoch: 31550\tTrain loss: 4.542791521089384e-05\n",
      "Epoch: 31600\tTrain loss: 2.5847709366644267e-05\n",
      "Epoch: 31650\tTrain loss: 1.49559450619563e-05\n",
      "Epoch: 31700\tTrain loss: 8.673404408909846e-06\n",
      "Epoch: 31750\tTrain loss: 6.017757641529897e-06\n",
      "Epoch: 31800\tTrain loss: 2.858122968518728e-06\n",
      "Epoch: 31850\tTrain loss: 3.099632613157155e-05\n",
      "Epoch: 31900\tTrain loss: 0.07287268759682775\n",
      "Epoch: 31950\tTrain loss: 572.0535144805908\n",
      "Epoch: 32000\tTrain loss: 959.0829620361328\n",
      "Epoch: 32050\tTrain loss: 181.93742752075195\n",
      "Epoch: 32100\tTrain loss: 133.23869800567627\n",
      "Epoch: 32150\tTrain loss: 106.37685203552246\n",
      "Epoch: 32200\tTrain loss: 95.37237739562988\n",
      "Epoch: 32250\tTrain loss: 87.63722133636475\n",
      "Epoch: 32300\tTrain loss: 81.30104351043701\n",
      "Epoch: 32350\tTrain loss: 74.9462251663208\n",
      "Epoch: 32400\tTrain loss: 110.83783626556396\n",
      "Epoch: 32450\tTrain loss: 64.890625\n",
      "Epoch: 32500\tTrain loss: 61.54404544830322\n",
      "Epoch: 32550\tTrain loss: 55.57748603820801\n",
      "Epoch: 32600\tTrain loss: 225.7828140258789\n",
      "Epoch: 32650\tTrain loss: 33.288025856018066\n",
      "Epoch: 32700\tTrain loss: 22.046463012695312\n",
      "Epoch: 32750\tTrain loss: 18.59318232536316\n",
      "Epoch: 32800\tTrain loss: 18.285995543003082\n",
      "Epoch: 32850\tTrain loss: 31.80365800857544\n",
      "Epoch: 32900\tTrain loss: 5.6574713587760925\n",
      "Epoch: 32950\tTrain loss: 4.116750478744507\n",
      "Epoch: 33000\tTrain loss: 3.1028369665145874\n",
      "Epoch: 33050\tTrain loss: 2.2134940028190613\n",
      "Epoch: 33100\tTrain loss: 1.7268879488110542\n",
      "Epoch: 33150\tTrain loss: 1.455037385225296\n",
      "Epoch: 33200\tTrain loss: 1.2436607033014297\n",
      "Epoch: 33250\tTrain loss: 1.0748428404331207\n",
      "Epoch: 33300\tTrain loss: 0.9363023489713669\n",
      "Epoch: 33350\tTrain loss: 0.8196767866611481\n",
      "Epoch: 33400\tTrain loss: 0.719923734664917\n",
      "Epoch: 33450\tTrain loss: 0.6337438076734543\n",
      "Epoch: 33500\tTrain loss: 0.5580322369933128\n",
      "Epoch: 33550\tTrain loss: 0.4906625896692276\n",
      "Epoch: 33600\tTrain loss: 0.4308066666126251\n",
      "Epoch: 33650\tTrain loss: 0.37737971171736717\n",
      "Epoch: 33700\tTrain loss: 0.3291539028286934\n",
      "Epoch: 33750\tTrain loss: 0.2863762676715851\n",
      "Epoch: 33800\tTrain loss: 0.24806255102157593\n",
      "Epoch: 33850\tTrain loss: 0.21391375362873077\n",
      "Epoch: 33900\tTrain loss: 0.18385732918977737\n",
      "Epoch: 33950\tTrain loss: 0.1577317975461483\n",
      "Epoch: 34000\tTrain loss: 0.1340495813637972\n",
      "Epoch: 34050\tTrain loss: 0.11392063647508621\n",
      "Epoch: 34100\tTrain loss: 0.09723530150949955\n",
      "Epoch: 34150\tTrain loss: 0.0807022713124752\n",
      "Epoch: 34200\tTrain loss: 0.06776893325150013\n",
      "Epoch: 34250\tTrain loss: 0.05748562328517437\n",
      "Epoch: 34300\tTrain loss: 0.1692497581243515\n",
      "Epoch: 34350\tTrain loss: 0.10219401121139526\n",
      "Epoch: 34400\tTrain loss: 0.04572264105081558\n",
      "Epoch: 34450\tTrain loss: 0.046016281470656395\n",
      "Epoch: 34500\tTrain loss: 2.6558464765548706\n",
      "Epoch: 34550\tTrain loss: 0.07676853612065315\n",
      "Epoch: 34600\tTrain loss: 0.022760191932320595\n",
      "Epoch: 34650\tTrain loss: 0.01672177715227008\n",
      "Epoch: 34700\tTrain loss: 0.012861272785812616\n",
      "Epoch: 34750\tTrain loss: 0.011160021647810936\n",
      "Epoch: 34800\tTrain loss: 1.1421316862106323\n",
      "Epoch: 34850\tTrain loss: 0.03570800367742777\n",
      "Epoch: 34900\tTrain loss: 0.10703725600615144\n",
      "Epoch: 34950\tTrain loss: 0.26395387202501297\n",
      "Epoch: 35000\tTrain loss: 1.2269805520772934\n",
      "Epoch: 35050\tTrain loss: 0.22877707332372665\n",
      "Epoch: 35100\tTrain loss: 0.01565222628414631\n",
      "Epoch: 35150\tTrain loss: 0.004011780489236116\n",
      "Epoch: 35200\tTrain loss: 0.0018215837189927697\n",
      "Epoch: 35250\tTrain loss: 0.0012330738245509565\n",
      "Epoch: 35300\tTrain loss: 0.0009076297137653455\n",
      "Epoch: 35350\tTrain loss: 0.0006172861030790955\n",
      "Epoch: 35400\tTrain loss: 0.0008874659397406504\n",
      "Epoch: 35450\tTrain loss: 0.5106002688407898\n",
      "Epoch: 35500\tTrain loss: 12.009307712316513\n",
      "Epoch: 35550\tTrain loss: 0.026738585904240608\n",
      "Epoch: 35600\tTrain loss: 0.004190131556242704\n",
      "Epoch: 35650\tTrain loss: 0.0013816669816151261\n",
      "Epoch: 35700\tTrain loss: 0.0006047329516150057\n",
      "Epoch: 35750\tTrain loss: 0.0003272200483479537\n",
      "Epoch: 35800\tTrain loss: 0.00018280759104527533\n",
      "Epoch: 35850\tTrain loss: 0.0001048061403707834\n",
      "Epoch: 35900\tTrain loss: 6.322949684545165e-05\n",
      "Epoch: 35950\tTrain loss: 4.183770579402335e-05\n",
      "Epoch: 36000\tTrain loss: 0.0008076399681158364\n",
      "Epoch: 36050\tTrain loss: 1.0772898271679878\n",
      "Epoch: 36100\tTrain loss: 0.22854620963335037\n",
      "Epoch: 36150\tTrain loss: 0.024963205913081765\n",
      "Epoch: 36200\tTrain loss: 1.1413561701774597\n",
      "Epoch: 36250\tTrain loss: 4.215798020362854\n",
      "Epoch: 36300\tTrain loss: 0.04314250126481056\n",
      "Epoch: 36350\tTrain loss: 0.0054232970578596\n",
      "Epoch: 36400\tTrain loss: 0.0009646960243117064\n",
      "Epoch: 36450\tTrain loss: 0.0004883551519014873\n",
      "Epoch: 36500\tTrain loss: 0.00034732819767668843\n",
      "Epoch: 36550\tTrain loss: 0.0047001619823277\n",
      "Epoch: 36600\tTrain loss: 0.0006044785732228775\n",
      "Epoch: 36650\tTrain loss: 1.244999384653056e-05\n",
      "Epoch: 36700\tTrain loss: 5.710308641937445e-06\n",
      "Epoch: 36750\tTrain loss: 6.418070324798464e-05\n",
      "Epoch: 36800\tTrain loss: 0.0026167744072154164\n",
      "Epoch: 36850\tTrain loss: 0.16744734719395638\n",
      "Epoch: 36900\tTrain loss: 0.20341801643371582\n",
      "Epoch: 36950\tTrain loss: 0.0039986943593248725\n",
      "Epoch: 37000\tTrain loss: 0.0004546423879219219\n",
      "Epoch: 37050\tTrain loss: 0.00013710573693970218\n",
      "Epoch: 37100\tTrain loss: 3.8468214370368514e-05\n",
      "Epoch: 37150\tTrain loss: 0.0007788233542669332\n",
      "Epoch: 37200\tTrain loss: 0.0007871763373259455\n",
      "Epoch: 37250\tTrain loss: 0.00018732980606728233\n",
      "Epoch: 37300\tTrain loss: 0.00010558165013208054\n",
      "Epoch: 37350\tTrain loss: 0.019541653338819742\n",
      "Epoch: 37400\tTrain loss: 0.7161238379776478\n",
      "Epoch: 37450\tTrain loss: 4.707648873329163\n",
      "Epoch: 37500\tTrain loss: 0.10452959686517715\n",
      "Epoch: 37550\tTrain loss: 0.0061186994425952435\n",
      "Epoch: 37600\tTrain loss: 0.002231047546956688\n",
      "Epoch: 37650\tTrain loss: 0.0013885600492358208\n",
      "Epoch: 37700\tTrain loss: 0.0008886119030648842\n",
      "Epoch: 37750\tTrain loss: 0.0007040143245831132\n",
      "Epoch: 37800\tTrain loss: 0.0005107972829136997\n",
      "Epoch: 37850\tTrain loss: 37.97384440898895\n",
      "Epoch: 37900\tTrain loss: 0.021511176601052284\n",
      "Epoch: 37950\tTrain loss: 0.005284933838993311\n",
      "Epoch: 38000\tTrain loss: 0.002990877954289317\n",
      "Epoch: 38050\tTrain loss: 0.0014633794780820608\n",
      "Epoch: 38100\tTrain loss: 0.0009155101433862001\n",
      "Epoch: 38150\tTrain loss: 0.0006077572907088324\n",
      "Epoch: 38200\tTrain loss: 0.0004235064407112077\n",
      "Epoch: 38250\tTrain loss: 0.00030151531973388046\n",
      "Epoch: 38300\tTrain loss: 0.000216984779399354\n",
      "Epoch: 38350\tTrain loss: 0.0002839934313669801\n",
      "Epoch: 38400\tTrain loss: 0.00018463780725141987\n",
      "Epoch: 38450\tTrain loss: 8.159187564160675e-05\n",
      "Epoch: 38500\tTrain loss: 5.7921195548260584e-05\n",
      "Epoch: 38550\tTrain loss: 0.010608654120005667\n",
      "Epoch: 38600\tTrain loss: 251.8372802734375\n",
      "Epoch: 38650\tTrain loss: 107.53691482543945\n",
      "Epoch: 38700\tTrain loss: 46.96890640258789\n",
      "Epoch: 38750\tTrain loss: 32.065940856933594\n",
      "Epoch: 38800\tTrain loss: 26.982666969299316\n",
      "Epoch: 38850\tTrain loss: 26.589370727539062\n",
      "Epoch: 38900\tTrain loss: 30.80948233604431\n",
      "Epoch: 38950\tTrain loss: 24.965201377868652\n",
      "Epoch: 39000\tTrain loss: 20.955764770507812\n",
      "Epoch: 39050\tTrain loss: 17.548597931861877\n",
      "Epoch: 39100\tTrain loss: 14.69010591506958\n",
      "Epoch: 39150\tTrain loss: 11.848130106925964\n",
      "Epoch: 39200\tTrain loss: 8.959388762712479\n",
      "Epoch: 39250\tTrain loss: 11.252012968063354\n",
      "Epoch: 39300\tTrain loss: 8.133722305297852\n",
      "Epoch: 39350\tTrain loss: 5.546179160475731\n",
      "Epoch: 39400\tTrain loss: 7.8010029792785645\n",
      "Epoch: 39450\tTrain loss: 5.489461779594421\n",
      "Epoch: 39500\tTrain loss: 25.863179206848145\n",
      "Epoch: 39550\tTrain loss: 7.471599340438843\n",
      "Epoch: 39600\tTrain loss: 5.684354156255722\n",
      "Epoch: 39650\tTrain loss: 4.9520697593688965\n",
      "Epoch: 39700\tTrain loss: 4.339427947998047\n",
      "Epoch: 39750\tTrain loss: 3.81443452835083\n",
      "Epoch: 39800\tTrain loss: 3.3515055179595947\n",
      "Epoch: 39850\tTrain loss: 3.46500825881958\n",
      "Epoch: 39900\tTrain loss: 5.190008163452148\n",
      "Epoch: 39950\tTrain loss: 2.3968334272503853\n",
      "Epoch: 40000\tTrain loss: 2.060056507587433\n",
      "Epoch: 40050\tTrain loss: 1.7739256024360657\n",
      "Epoch: 40100\tTrain loss: 1.5200869292020798\n",
      "Epoch: 40150\tTrain loss: 1.2927912473678589\n",
      "Epoch: 40200\tTrain loss: 1.0864594355225563\n",
      "Epoch: 40250\tTrain loss: 0.8997797071933746\n",
      "Epoch: 40300\tTrain loss: 0.7343878149986267\n",
      "Epoch: 40350\tTrain loss: 0.6838837116956711\n",
      "Epoch: 40400\tTrain loss: 0.5341003388166428\n",
      "Epoch: 40450\tTrain loss: 1.5376814603805542\n",
      "Epoch: 40500\tTrain loss: 1.4447275698184967\n",
      "Epoch: 40550\tTrain loss: 2.6248985528945923\n",
      "Epoch: 40600\tTrain loss: 0.36323773115873337\n",
      "Epoch: 40650\tTrain loss: 0.2828967273235321\n",
      "Epoch: 40700\tTrain loss: 0.22327671200037003\n",
      "Epoch: 40750\tTrain loss: 0.17681924253702164\n",
      "Epoch: 40800\tTrain loss: 0.13965042680501938\n",
      "Epoch: 40850\tTrain loss: 0.11053433269262314\n",
      "Epoch: 40900\tTrain loss: 0.10174180194735527\n",
      "Epoch: 40950\tTrain loss: 0.14295794069766998\n",
      "Epoch: 41000\tTrain loss: 0.18337469920516014\n",
      "Epoch: 41050\tTrain loss: 0.06873723678290844\n",
      "Epoch: 41100\tTrain loss: 0.04627217643428594\n",
      "Epoch: 41150\tTrain loss: 0.048544490709900856\n",
      "Epoch: 41200\tTrain loss: 0.024321136064827442\n",
      "Epoch: 41250\tTrain loss: 0.05005785636603832\n",
      "Epoch: 41300\tTrain loss: 0.015025060158222914\n",
      "Epoch: 41350\tTrain loss: 2.9499980807304382\n",
      "Epoch: 41400\tTrain loss: 1.6702589690685272\n",
      "Epoch: 41450\tTrain loss: 0.1892547942698002\n",
      "Epoch: 41500\tTrain loss: 0.05357184447348118\n",
      "Epoch: 41550\tTrain loss: 0.06598729640245438\n",
      "Epoch: 41600\tTrain loss: 0.18798489589244127\n",
      "Epoch: 41650\tTrain loss: 0.0209611487807706\n",
      "Epoch: 41700\tTrain loss: 0.002908135298639536\n",
      "Epoch: 41750\tTrain loss: 0.0018736616730166133\n",
      "Epoch: 41800\tTrain loss: 0.0012165603111498058\n",
      "Epoch: 41850\tTrain loss: 0.0007800516323186457\n",
      "Epoch: 41900\tTrain loss: 0.0005686534932465293\n",
      "Epoch: 41950\tTrain loss: 8.186920262873173\n",
      "Epoch: 42000\tTrain loss: 0.41997409611940384\n",
      "Epoch: 42050\tTrain loss: 2.459810860455036\n",
      "Epoch: 42100\tTrain loss: 2.454144597053528\n",
      "Epoch: 42150\tTrain loss: 0.016659693093970418\n",
      "Epoch: 42200\tTrain loss: 0.0014142112922854722\n",
      "Epoch: 42250\tTrain loss: 0.0005865709099452943\n",
      "Epoch: 42300\tTrain loss: 0.0003181922002113424\n",
      "Epoch: 42350\tTrain loss: 0.00017870061128633097\n",
      "Epoch: 42400\tTrain loss: 0.00010129915608558804\n",
      "Epoch: 42450\tTrain loss: 5.7874118283507414e-05\n",
      "Epoch: 42500\tTrain loss: 3.3290774808847345e-05\n",
      "Epoch: 42550\tTrain loss: 1.880569669765464e-05\n",
      "Epoch: 42600\tTrain loss: 1.1096783282482647e-05\n",
      "Epoch: 42650\tTrain loss: 9.469150882068789e-06\n",
      "Epoch: 42700\tTrain loss: 6.151147681521252e-05\n",
      "Epoch: 42750\tTrain loss: 5.620713511689246e-05\n",
      "Epoch: 42800\tTrain loss: 0.08893490862101316\n",
      "Epoch: 42850\tTrain loss: 0.6082145422697067\n",
      "Epoch: 42900\tTrain loss: 0.027246123645454645\n",
      "Epoch: 42950\tTrain loss: 0.00024614849826321006\n",
      "Epoch: 43000\tTrain loss: 0.0029211812507128343\n",
      "Epoch: 43050\tTrain loss: 0.00032119674870045856\n",
      "Epoch: 43100\tTrain loss: 3.7823071479797363\n",
      "Epoch: 43150\tTrain loss: 0.0025669028400443494\n",
      "Epoch: 43200\tTrain loss: 0.06086494028568268\n",
      "Epoch: 43250\tTrain loss: 6.315344162285328\n",
      "Epoch: 43300\tTrain loss: 2.8803486227989197\n",
      "Epoch: 43350\tTrain loss: 0.012650844524614513\n",
      "Epoch: 43400\tTrain loss: 0.0014200247533153743\n",
      "Epoch: 43450\tTrain loss: 0.0004158068768447265\n",
      "Epoch: 43500\tTrain loss: 0.00015424837329192087\n",
      "Epoch: 43550\tTrain loss: 6.992354246904142e-05\n",
      "Epoch: 43600\tTrain loss: 3.375698270247085e-05\n",
      "Epoch: 43650\tTrain loss: 1.6873493223101832e-05\n",
      "Epoch: 43700\tTrain loss: 8.821402161629521e-06\n",
      "Epoch: 43750\tTrain loss: 4.032426204503281e-06\n",
      "Epoch: 43800\tTrain loss: 2.1385365300830017e-06\n",
      "Epoch: 43850\tTrain loss: 1.7362348557981022e-05\n",
      "Epoch: 43900\tTrain loss: 8.36752012105535e-05\n",
      "Epoch: 43950\tTrain loss: 84.79429519176483\n",
      "Epoch: 44000\tTrain loss: 1.0965768098831177\n",
      "Epoch: 44050\tTrain loss: 0.4572291672229767\n",
      "Epoch: 44100\tTrain loss: 0.2649665093049407\n",
      "Epoch: 44150\tTrain loss: 0.16006631404161453\n",
      "Epoch: 44200\tTrain loss: 0.09594232589006424\n",
      "Epoch: 44250\tTrain loss: 0.05716683715581894\n",
      "Epoch: 44300\tTrain loss: 0.033640225417912006\n",
      "Epoch: 44350\tTrain loss: 0.019434220914263278\n",
      "Epoch: 44400\tTrain loss: 0.011192113161087036\n",
      "Epoch: 44450\tTrain loss: 0.006330510004772805\n",
      "Epoch: 44500\tTrain loss: 0.0035472128074616194\n",
      "Epoch: 44550\tTrain loss: 0.0019523786941135768\n",
      "Epoch: 44600\tTrain loss: 0.0010728048800956458\n",
      "Epoch: 44650\tTrain loss: 0.0005836799100507051\n",
      "Epoch: 44700\tTrain loss: 0.0003136599843855947\n",
      "Epoch: 44750\tTrain loss: 0.0001684140165707504\n",
      "Epoch: 44800\tTrain loss: 9.08761030586902e-05\n",
      "Epoch: 44850\tTrain loss: 4.852980055147782e-05\n",
      "Epoch: 44900\tTrain loss: 2.5808022655837703e-05\n",
      "Epoch: 44950\tTrain loss: 1.3990527122587082e-05\n",
      "Epoch: 45000\tTrain loss: 7.206658210634487e-06\n",
      "Epoch: 45050\tTrain loss: 3.870832870234153e-06\n",
      "Epoch: 45100\tTrain loss: 4.742317969430587e-06\n",
      "Epoch: 45150\tTrain loss: 8.824649967209552e-06\n",
      "Epoch: 45200\tTrain loss: 0.009159528446616605\n",
      "Epoch: 45250\tTrain loss: 0.026444594375789165\n",
      "Epoch: 45300\tTrain loss: 0.8334981501102448\n",
      "Epoch: 45350\tTrain loss: 7.464634820818901\n",
      "Epoch: 45400\tTrain loss: 0.02606006618589163\n",
      "Epoch: 45450\tTrain loss: 0.005040494492277503\n",
      "Epoch: 45500\tTrain loss: 0.0016180570237338543\n",
      "Epoch: 45550\tTrain loss: 0.0006071730749681592\n",
      "Epoch: 45600\tTrain loss: 0.0002390500740148127\n",
      "Epoch: 45650\tTrain loss: 9.732420494401595e-05\n",
      "Epoch: 45700\tTrain loss: 4.047620859637391e-05\n",
      "Epoch: 45750\tTrain loss: 1.6798577235022094e-05\n",
      "Epoch: 45800\tTrain loss: 6.7985072291776305e-06\n",
      "Epoch: 45850\tTrain loss: 2.7806614184555656e-06\n",
      "Epoch: 45900\tTrain loss: 1.1932234542655351e-06\n",
      "Epoch: 45950\tTrain loss: 5.520026462590977e-07\n",
      "Epoch: 46000\tTrain loss: 2.8643085769886056e-07\n",
      "Epoch: 46050\tTrain loss: 8.175731665005515e-07\n",
      "Epoch: 46100\tTrain loss: 0.00043021121200581547\n",
      "Epoch: 46150\tTrain loss: 0.0002179975126637146\n",
      "Epoch: 46200\tTrain loss: 16.935450792312622\n",
      "Epoch: 46250\tTrain loss: 0.22464577108621597\n",
      "Epoch: 46300\tTrain loss: 0.1154837179929018\n",
      "Epoch: 46350\tTrain loss: 0.07567476760596037\n",
      "Epoch: 46400\tTrain loss: 0.04999707266688347\n",
      "Epoch: 46450\tTrain loss: 0.03205922245979309\n",
      "Epoch: 46500\tTrain loss: 0.020098823006264865\n",
      "Epoch: 46550\tTrain loss: 0.012877192348241806\n",
      "Epoch: 46600\tTrain loss: 0.008631631964817643\n",
      "Epoch: 46650\tTrain loss: 0.006038066581822932\n",
      "Epoch: 46700\tTrain loss: 0.004368615918792784\n",
      "Epoch: 46750\tTrain loss: 0.003200839157216251\n",
      "Epoch: 46800\tTrain loss: 0.002152424189262092\n",
      "Epoch: 46850\tTrain loss: 0.0015097318537300453\n",
      "Epoch: 46900\tTrain loss: 0.0013088854611851275\n",
      "Epoch: 46950\tTrain loss: 0.001009697574772872\n",
      "Epoch: 47000\tTrain loss: 0.26164646819233894\n",
      "Epoch: 47050\tTrain loss: 0.0017500717658549547\n",
      "Epoch: 47100\tTrain loss: 0.06408402975648642\n",
      "Epoch: 47150\tTrain loss: 0.2665721047669649\n",
      "Epoch: 47200\tTrain loss: 0.03230839129537344\n",
      "Epoch: 47250\tTrain loss: 0.013892270231735893\n",
      "Epoch: 47300\tTrain loss: 0.006127690896391869\n",
      "Epoch: 47350\tTrain loss: 0.0027734084287658334\n",
      "Epoch: 47400\tTrain loss: 0.0012050783261656761\n",
      "Epoch: 47450\tTrain loss: 0.0005030897882534191\n",
      "Epoch: 47500\tTrain loss: 0.00021458073024405167\n",
      "Epoch: 47550\tTrain loss: 9.585751104168594e-05\n",
      "Epoch: 47600\tTrain loss: 4.540779809758533e-05\n",
      "Epoch: 47650\tTrain loss: 2.304128065588884e-05\n",
      "Epoch: 47700\tTrain loss: 1.2226164926687488e-05\n",
      "Epoch: 47750\tTrain loss: 6.84523774907575e-06\n",
      "Epoch: 47800\tTrain loss: 7.576924872410018e-06\n",
      "Epoch: 47850\tTrain loss: 3.0824409122942598e-06\n",
      "Epoch: 47900\tTrain loss: 0.00021650981216225773\n",
      "Epoch: 47950\tTrain loss: 8.450108225588338e-06\n",
      "Epoch: 48000\tTrain loss: 5.686320662334765e-06\n",
      "Epoch: 48050\tTrain loss: 0.29712238907814026\n",
      "Epoch: 48100\tTrain loss: 1.3299192190170288\n",
      "Epoch: 48150\tTrain loss: 0.015200589783489704\n",
      "Epoch: 48200\tTrain loss: 0.0026474589249119163\n",
      "Epoch: 48250\tTrain loss: 0.0008137509867083281\n",
      "Epoch: 48300\tTrain loss: 0.00025877718144329265\n",
      "Epoch: 48350\tTrain loss: 8.154266834026203e-05\n",
      "Epoch: 48400\tTrain loss: 2.5000524146889802e-05\n",
      "Epoch: 48450\tTrain loss: 7.881887540861499e-06\n",
      "Epoch: 48500\tTrain loss: 2.7117707759316545e-06\n",
      "Epoch: 48550\tTrain loss: 1.080693806443378e-06\n",
      "Epoch: 48600\tTrain loss: 6.169313593318293e-07\n",
      "Epoch: 48650\tTrain loss: 3.7123811580386246e-07\n",
      "Epoch: 48700\tTrain loss: 1.8991365465126364e-05\n",
      "Epoch: 48750\tTrain loss: 4.014841152866211e-07\n",
      "Epoch: 48800\tTrain loss: 0.11152080865576863\n",
      "Epoch: 48850\tTrain loss: 0.2519016726873815\n",
      "Epoch: 48900\tTrain loss: 7.555354595184326\n",
      "Epoch: 48950\tTrain loss: 0.03642128873616457\n",
      "Epoch: 49000\tTrain loss: 0.0013665857368323486\n",
      "Epoch: 49050\tTrain loss: 0.0005559583951253444\n",
      "Epoch: 49100\tTrain loss: 0.00019754035747610033\n",
      "Epoch: 49150\tTrain loss: 7.951376574055757e-05\n",
      "Epoch: 49200\tTrain loss: 3.1673568628320936e-05\n",
      "Epoch: 49250\tTrain loss: 1.2470398814912187e-05\n",
      "Epoch: 49300\tTrain loss: 4.890827540293685e-06\n",
      "Epoch: 49350\tTrain loss: 1.9029158124794776e-06\n",
      "Epoch: 49400\tTrain loss: 8.190749269942899e-07\n",
      "Epoch: 49450\tTrain loss: 4.1348393153839424e-07\n",
      "Epoch: 49500\tTrain loss: 2.714921762958511e-07\n",
      "Epoch: 49550\tTrain loss: 2.2202341654065094e-07\n",
      "Epoch: 49600\tTrain loss: 2.43755572171267e-07\n",
      "Epoch: 49650\tTrain loss: 5.263023467705352e-05\n",
      "Epoch: 49700\tTrain loss: 0.0010211787921434734\n",
      "Epoch: 49750\tTrain loss: 1.685997560620308\n",
      "Epoch: 49800\tTrain loss: 0.07123223878443241\n",
      "Epoch: 49850\tTrain loss: 0.002558063482865691\n",
      "Epoch: 49900\tTrain loss: 0.00011681148680509068\n",
      "Epoch: 49950\tTrain loss: 3.343009575473843e-05\n",
      "Training CFRNN\n",
      "Epoch: 0\tTrain loss: 1431151.25\n",
      "Epoch: 50\tTrain loss: 1383622.546875\n",
      "Epoch: 100\tTrain loss: 1339698.796875\n",
      "Epoch: 150\tTrain loss: 1297780.1875\n",
      "Epoch: 200\tTrain loss: 1257495.65625\n",
      "Epoch: 250\tTrain loss: 1218651.28125\n",
      "Epoch: 300\tTrain loss: 1181017.375\n",
      "Epoch: 350\tTrain loss: 1144566.875\n",
      "Epoch: 400\tTrain loss: 1109298.34375\n",
      "Epoch: 450\tTrain loss: 1075030.640625\n",
      "Epoch: 500\tTrain loss: 1041739.109375\n",
      "Epoch: 550\tTrain loss: 1009646.75\n",
      "Epoch: 600\tTrain loss: 978590.59375\n",
      "Epoch: 650\tTrain loss: 948351.96875\n",
      "Epoch: 700\tTrain loss: 919148.5\n",
      "Epoch: 750\tTrain loss: 890763.015625\n",
      "Epoch: 800\tTrain loss: 863452.125\n",
      "Epoch: 850\tTrain loss: 836901.09375\n",
      "Epoch: 900\tTrain loss: 811156.3515625\n",
      "Epoch: 950\tTrain loss: 786402.9921875\n",
      "Epoch: 1000\tTrain loss: 762445.453125\n",
      "Epoch: 1050\tTrain loss: 739158.6875\n",
      "Epoch: 1100\tTrain loss: 716737.4453125\n",
      "Epoch: 1150\tTrain loss: 695243.765625\n",
      "Epoch: 1200\tTrain loss: 674283.01953125\n",
      "Epoch: 1250\tTrain loss: 654218.2734375\n",
      "Epoch: 1300\tTrain loss: 634772.8125\n",
      "Epoch: 1350\tTrain loss: 616003.1875\n",
      "Epoch: 1400\tTrain loss: 597914.09375\n",
      "Epoch: 1450\tTrain loss: 580663.19921875\n",
      "Epoch: 1500\tTrain loss: 563904.859375\n",
      "Epoch: 1550\tTrain loss: 547841.9375\n",
      "Epoch: 1600\tTrain loss: 532458.53125\n",
      "Epoch: 1650\tTrain loss: 517639.671875\n",
      "Epoch: 1700\tTrain loss: 503502.80859375\n",
      "Epoch: 1750\tTrain loss: 489875.109375\n",
      "Epoch: 1800\tTrain loss: 476941.166015625\n",
      "Epoch: 1850\tTrain loss: 464407.9873046875\n",
      "Epoch: 1900\tTrain loss: 452552.4931640625\n",
      "Epoch: 1950\tTrain loss: 441291.9375\n",
      "Epoch: 2000\tTrain loss: 430524.21875\n",
      "Epoch: 2050\tTrain loss: 420257.03125\n",
      "Epoch: 2100\tTrain loss: 410485.5\n",
      "Epoch: 2150\tTrain loss: 401232.5625\n",
      "Epoch: 2200\tTrain loss: 392477.515625\n",
      "Epoch: 2250\tTrain loss: 384200.505859375\n",
      "Epoch: 2300\tTrain loss: 376332.1875\n",
      "Epoch: 2350\tTrain loss: 368956.21875\n",
      "Epoch: 2400\tTrain loss: 361995.75\n",
      "Epoch: 2450\tTrain loss: 355395.10546875\n",
      "Epoch: 2500\tTrain loss: 349297.859375\n",
      "Epoch: 2550\tTrain loss: 343613.15234375\n",
      "Epoch: 2600\tTrain loss: 338189.75\n",
      "Epoch: 2650\tTrain loss: 333196.953125\n",
      "Epoch: 2700\tTrain loss: 328556.7734375\n",
      "Epoch: 2750\tTrain loss: 324239.765625\n",
      "Epoch: 2800\tTrain loss: 320223.4375\n",
      "Epoch: 2850\tTrain loss: 316520.109375\n",
      "Epoch: 2900\tTrain loss: 313078.125\n",
      "Epoch: 2950\tTrain loss: 309942.5390625\n",
      "Epoch: 3000\tTrain loss: 307072.951171875\n",
      "Epoch: 3050\tTrain loss: 304460.5703125\n",
      "Epoch: 3100\tTrain loss: 302070.1171875\n",
      "Epoch: 3150\tTrain loss: 299906.1953125\n",
      "Epoch: 3200\tTrain loss: 297891.171875\n",
      "Epoch: 3250\tTrain loss: 296075.234375\n",
      "Epoch: 3300\tTrain loss: 271428.341796875\n",
      "Epoch: 3350\tTrain loss: 234374.7265625\n",
      "Epoch: 3400\tTrain loss: 224355.640625\n",
      "Epoch: 3450\tTrain loss: 215480.03747558594\n",
      "Epoch: 3500\tTrain loss: 206965.93359375\n",
      "Epoch: 3550\tTrain loss: 198742.85919189453\n",
      "Epoch: 3600\tTrain loss: 190797.70736694336\n",
      "Epoch: 3650\tTrain loss: 183116.0625\n",
      "Epoch: 3700\tTrain loss: 175669.109375\n",
      "Epoch: 3750\tTrain loss: 168547.3984375\n",
      "Epoch: 3800\tTrain loss: 161564.4453125\n",
      "Epoch: 3850\tTrain loss: 154887.19104003906\n",
      "Epoch: 3900\tTrain loss: 148395.02728271484\n",
      "Epoch: 3950\tTrain loss: 142041.01879882812\n",
      "Epoch: 4000\tTrain loss: 135956.59765625\n",
      "Epoch: 4050\tTrain loss: 130097.7890625\n",
      "Epoch: 4100\tTrain loss: 124354.38134765625\n",
      "Epoch: 4150\tTrain loss: 118841.29052734375\n",
      "Epoch: 4200\tTrain loss: 113515.947265625\n",
      "Epoch: 4250\tTrain loss: 108346.912109375\n",
      "Epoch: 4300\tTrain loss: 103361.419921875\n",
      "Epoch: 4350\tTrain loss: 98534.38671875\n",
      "Epoch: 4400\tTrain loss: 93841.59609985352\n",
      "Epoch: 4450\tTrain loss: 89363.6171875\n",
      "Epoch: 4500\tTrain loss: 85040.65002441406\n",
      "Epoch: 4550\tTrain loss: 80832.787109375\n",
      "Epoch: 4600\tTrain loss: 76834.64157104492\n",
      "Epoch: 4650\tTrain loss: 72949.923828125\n",
      "Epoch: 4700\tTrain loss: 69204.33151245117\n",
      "Epoch: 4750\tTrain loss: 65640.099609375\n",
      "Epoch: 4800\tTrain loss: 62222.80685424805\n",
      "Epoch: 4850\tTrain loss: 58896.914489746094\n",
      "Epoch: 4900\tTrain loss: 55763.855041503906\n",
      "Epoch: 4950\tTrain loss: 52998.355529785156\n",
      "Epoch: 5000\tTrain loss: 49885.69580078125\n",
      "Epoch: 5050\tTrain loss: 47076.427734375\n",
      "Epoch: 5100\tTrain loss: 44443.58251953125\n",
      "Epoch: 5150\tTrain loss: 41943.498474121094\n",
      "Epoch: 5200\tTrain loss: 39557.12274169922\n",
      "Epoch: 5250\tTrain loss: 37289.794677734375\n",
      "Epoch: 5300\tTrain loss: 35136.04962158203\n",
      "Epoch: 5350\tTrain loss: 33119.870361328125\n",
      "Epoch: 5400\tTrain loss: 31217.22265625\n",
      "Epoch: 5450\tTrain loss: 29397.20068359375\n",
      "Epoch: 5500\tTrain loss: 27696.759033203125\n",
      "Epoch: 5550\tTrain loss: 26103.21519470215\n",
      "Epoch: 5600\tTrain loss: 24616.726806640625\n",
      "Epoch: 5650\tTrain loss: 23219.00506591797\n",
      "Epoch: 5700\tTrain loss: 21917.696166992188\n",
      "Epoch: 5750\tTrain loss: 20705.590393066406\n",
      "Epoch: 5800\tTrain loss: 19567.068359375\n",
      "Epoch: 5850\tTrain loss: 18536.48095703125\n",
      "Epoch: 5900\tTrain loss: 17569.660766601562\n",
      "Epoch: 5950\tTrain loss: 16683.33871459961\n",
      "Epoch: 6000\tTrain loss: 15861.480407714844\n",
      "Epoch: 6050\tTrain loss: 15121.23892211914\n",
      "Epoch: 6100\tTrain loss: 14457.396301269531\n",
      "Epoch: 6150\tTrain loss: 13828.269165039062\n",
      "Epoch: 6200\tTrain loss: 13278.957092285156\n",
      "Epoch: 6250\tTrain loss: 12774.195678710938\n",
      "Epoch: 6300\tTrain loss: 12319.715087890625\n",
      "Epoch: 6350\tTrain loss: 11917.090637207031\n",
      "Epoch: 6400\tTrain loss: 11111.312561035156\n",
      "Epoch: 6450\tTrain loss: 10137.723937988281\n",
      "Epoch: 6500\tTrain loss: 9383.615692138672\n",
      "Epoch: 6550\tTrain loss: 8733.12191772461\n",
      "Epoch: 6600\tTrain loss: 8128.8194580078125\n",
      "Epoch: 6650\tTrain loss: 7568.9754638671875\n",
      "Epoch: 6700\tTrain loss: 7036.169860839844\n",
      "Epoch: 6750\tTrain loss: 6537.040588378906\n",
      "Epoch: 6800\tTrain loss: 6073.348846435547\n",
      "Epoch: 6850\tTrain loss: 5635.365417480469\n",
      "Epoch: 6900\tTrain loss: 5241.56982421875\n",
      "Epoch: 6950\tTrain loss: 4867.801788330078\n",
      "Epoch: 7000\tTrain loss: 4530.572296142578\n",
      "Epoch: 7050\tTrain loss: 4238.114440917969\n",
      "Epoch: 7100\tTrain loss: 3920.1797790527344\n",
      "Epoch: 7150\tTrain loss: 3642.5210571289062\n",
      "Epoch: 7200\tTrain loss: 3387.6931762695312\n",
      "Epoch: 7250\tTrain loss: 3154.206787109375\n",
      "Epoch: 7300\tTrain loss: 2940.638885498047\n",
      "Epoch: 7350\tTrain loss: 2745.7680053710938\n",
      "Epoch: 7400\tTrain loss: 2571.9934692382812\n",
      "Epoch: 7450\tTrain loss: 5705.118469238281\n",
      "Epoch: 7500\tTrain loss: 5002.886016845703\n",
      "Epoch: 7550\tTrain loss: 4512.180236816406\n",
      "Epoch: 7600\tTrain loss: 4092.9461669921875\n",
      "Epoch: 7650\tTrain loss: 3745.959259033203\n",
      "Epoch: 7700\tTrain loss: 3461.2481689453125\n",
      "Epoch: 7750\tTrain loss: 3152.0450134277344\n",
      "Epoch: 7800\tTrain loss: 2892.2095642089844\n",
      "Epoch: 7850\tTrain loss: 2664.770233154297\n",
      "Epoch: 7900\tTrain loss: 2466.653854370117\n",
      "Epoch: 7950\tTrain loss: 2294.234619140625\n",
      "Epoch: 8000\tTrain loss: 2142.362274169922\n",
      "Epoch: 8050\tTrain loss: 2010.0902099609375\n",
      "Epoch: 8100\tTrain loss: 1892.9251098632812\n",
      "Epoch: 8150\tTrain loss: 1791.3927307128906\n",
      "Epoch: 8200\tTrain loss: 1703.0311279296875\n",
      "Epoch: 8250\tTrain loss: 1623.185302734375\n",
      "Epoch: 8300\tTrain loss: 1554.0247802734375\n",
      "Epoch: 8350\tTrain loss: 1491.0644226074219\n",
      "Epoch: 8400\tTrain loss: 1435.3161926269531\n",
      "Epoch: 8450\tTrain loss: 1385.1510620117188\n",
      "Epoch: 8500\tTrain loss: 1339.4118041992188\n",
      "Epoch: 8550\tTrain loss: 1298.2372131347656\n",
      "Epoch: 8600\tTrain loss: 1260.27978515625\n",
      "Epoch: 8650\tTrain loss: 1224.7304992675781\n",
      "Epoch: 8700\tTrain loss: 1191.7421875\n",
      "Epoch: 8750\tTrain loss: 1160.614013671875\n",
      "Epoch: 8800\tTrain loss: 1130.5836181640625\n",
      "Epoch: 8850\tTrain loss: 1102.6718139648438\n",
      "Epoch: 8900\tTrain loss: 1075.8423767089844\n",
      "Epoch: 8950\tTrain loss: 1050.1936645507812\n",
      "Epoch: 9000\tTrain loss: 1024.8901062011719\n",
      "Epoch: 9050\tTrain loss: 1000.6664428710938\n",
      "Epoch: 9100\tTrain loss: 977.2405700683594\n",
      "Epoch: 9150\tTrain loss: 953.9833984375\n",
      "Epoch: 9200\tTrain loss: 931.6880035400391\n",
      "Epoch: 9250\tTrain loss: 909.6663818359375\n",
      "Epoch: 9300\tTrain loss: 887.3365783691406\n",
      "Epoch: 9350\tTrain loss: 864.8963317871094\n",
      "Epoch: 9400\tTrain loss: 842.6298522949219\n",
      "Epoch: 9450\tTrain loss: 820.2054901123047\n",
      "Epoch: 9500\tTrain loss: 798.1486511230469\n",
      "Epoch: 9550\tTrain loss: 778.780517578125\n",
      "Epoch: 9600\tTrain loss: 755.6643676757812\n",
      "Epoch: 9650\tTrain loss: 734.9016265869141\n",
      "Epoch: 9700\tTrain loss: 714.9921875\n",
      "Epoch: 9750\tTrain loss: 695.5367431640625\n",
      "Epoch: 9800\tTrain loss: 676.2349853515625\n",
      "Epoch: 9850\tTrain loss: 657.76904296875\n",
      "Epoch: 9900\tTrain loss: 640.0722351074219\n",
      "Epoch: 9950\tTrain loss: 622.7402038574219\n",
      "Epoch: 10000\tTrain loss: 605.7907409667969\n",
      "Epoch: 10050\tTrain loss: 589.3298034667969\n",
      "Epoch: 10100\tTrain loss: 573.6483917236328\n",
      "Epoch: 10150\tTrain loss: 558.1233825683594\n",
      "Epoch: 10200\tTrain loss: 543.0664672851562\n",
      "Epoch: 10250\tTrain loss: 528.3487091064453\n",
      "Epoch: 10300\tTrain loss: 514.1221160888672\n",
      "Epoch: 10350\tTrain loss: 498.86993408203125\n",
      "Epoch: 10400\tTrain loss: 479.177001953125\n",
      "Epoch: 10450\tTrain loss: 458.2362060546875\n",
      "Epoch: 10500\tTrain loss: 436.9640808105469\n",
      "Epoch: 10550\tTrain loss: 415.73585510253906\n",
      "Epoch: 10600\tTrain loss: 648.4715270996094\n",
      "Epoch: 10650\tTrain loss: 601.4120178222656\n",
      "Epoch: 10700\tTrain loss: 380.7310028076172\n",
      "Epoch: 10750\tTrain loss: 363.9090270996094\n",
      "Epoch: 10800\tTrain loss: 348.03253173828125\n",
      "Epoch: 10850\tTrain loss: 332.9145202636719\n",
      "Epoch: 10900\tTrain loss: 318.43975830078125\n",
      "Epoch: 10950\tTrain loss: 304.38159942626953\n",
      "Epoch: 11000\tTrain loss: 290.77574157714844\n",
      "Epoch: 11050\tTrain loss: 277.58522033691406\n",
      "Epoch: 11100\tTrain loss: 264.8328094482422\n",
      "Epoch: 11150\tTrain loss: 252.1536102294922\n",
      "Epoch: 11200\tTrain loss: 241.0124740600586\n",
      "Epoch: 11250\tTrain loss: 228.65270233154297\n",
      "Epoch: 11300\tTrain loss: 216.1205291748047\n",
      "Epoch: 11350\tTrain loss: 204.42518615722656\n",
      "Epoch: 11400\tTrain loss: 193.8927001953125\n",
      "Epoch: 11450\tTrain loss: 186.06190490722656\n",
      "Epoch: 11500\tTrain loss: 204.6642837524414\n",
      "Epoch: 11550\tTrain loss: 166.66544723510742\n",
      "Epoch: 11600\tTrain loss: 157.87217712402344\n",
      "Epoch: 11650\tTrain loss: 149.41348266601562\n",
      "Epoch: 11700\tTrain loss: 141.32246780395508\n",
      "Epoch: 11750\tTrain loss: 133.44526290893555\n",
      "Epoch: 11800\tTrain loss: 125.89434814453125\n",
      "Epoch: 11850\tTrain loss: 118.48431777954102\n",
      "Epoch: 11900\tTrain loss: 116.93592834472656\n",
      "Epoch: 11950\tTrain loss: 92.00292587280273\n",
      "Epoch: 12000\tTrain loss: 82.81563949584961\n",
      "Epoch: 12050\tTrain loss: 72.13833999633789\n",
      "Epoch: 12100\tTrain loss: 65.38629150390625\n",
      "Epoch: 12150\tTrain loss: 59.51533317565918\n",
      "Epoch: 12200\tTrain loss: 54.27854537963867\n",
      "Epoch: 12250\tTrain loss: 49.60719871520996\n",
      "Epoch: 12300\tTrain loss: 45.34318542480469\n",
      "Epoch: 12350\tTrain loss: 41.39912223815918\n",
      "Epoch: 12400\tTrain loss: 38.505778074264526\n",
      "Epoch: 12450\tTrain loss: 37.70405864715576\n",
      "Epoch: 12500\tTrain loss: 32.75758957862854\n",
      "Epoch: 12550\tTrain loss: 29.43900966644287\n",
      "Epoch: 12600\tTrain loss: 26.806915283203125\n",
      "Epoch: 12650\tTrain loss: 23.811009883880615\n",
      "Epoch: 12700\tTrain loss: 21.474398612976074\n",
      "Epoch: 12750\tTrain loss: 18.85237979888916\n",
      "Epoch: 12800\tTrain loss: 16.71323871612549\n",
      "Epoch: 12850\tTrain loss: 14.798785209655762\n",
      "Epoch: 12900\tTrain loss: 13.071238279342651\n",
      "Epoch: 12950\tTrain loss: 11.524554967880249\n",
      "Epoch: 13000\tTrain loss: 10.1170072555542\n",
      "Epoch: 13050\tTrain loss: 8.849530696868896\n",
      "Epoch: 13100\tTrain loss: 7.707028865814209\n",
      "Epoch: 13150\tTrain loss: 6.684439420700073\n",
      "Epoch: 13200\tTrain loss: 5.785660147666931\n",
      "Epoch: 13250\tTrain loss: 4.970283389091492\n",
      "Epoch: 13300\tTrain loss: 4.261997699737549\n",
      "Epoch: 13350\tTrain loss: 4.682085752487183\n",
      "Epoch: 13400\tTrain loss: 3.0929264426231384\n",
      "Epoch: 13450\tTrain loss: 2.599717378616333\n",
      "Epoch: 13500\tTrain loss: 2.190751850605011\n",
      "Epoch: 13550\tTrain loss: 1.835703194141388\n",
      "Epoch: 13600\tTrain loss: 1.5280222296714783\n",
      "Epoch: 13650\tTrain loss: 1.2633553445339203\n",
      "Epoch: 13700\tTrain loss: 1.0483424961566925\n",
      "Epoch: 13750\tTrain loss: 0.8492939174175262\n",
      "Epoch: 13800\tTrain loss: 0.716657280921936\n",
      "Epoch: 13850\tTrain loss: 1.1815216541290283\n",
      "Epoch: 13900\tTrain loss: 0.4428316801786423\n",
      "Epoch: 13950\tTrain loss: 0.34953124821186066\n",
      "Epoch: 14000\tTrain loss: 0.27467820793390274\n",
      "Epoch: 14050\tTrain loss: 0.21358783543109894\n",
      "Epoch: 14100\tTrain loss: 0.165068618953228\n",
      "Epoch: 14150\tTrain loss: 0.12621615268290043\n",
      "Epoch: 14200\tTrain loss: 0.09567880537360907\n",
      "Epoch: 14250\tTrain loss: 0.07143659237772226\n",
      "Epoch: 14300\tTrain loss: 0.05299135111272335\n",
      "Epoch: 14350\tTrain loss: 0.039287885650992393\n",
      "Epoch: 14400\tTrain loss: 0.027915288927033544\n",
      "Epoch: 14450\tTrain loss: 0.16563325840979815\n",
      "Epoch: 14500\tTrain loss: 0.12850352004170418\n",
      "Epoch: 14550\tTrain loss: 0.038638876751065254\n",
      "Epoch: 14600\tTrain loss: 383.8286895751953\n",
      "Epoch: 14650\tTrain loss: 1621.5445251464844\n",
      "Epoch: 14700\tTrain loss: 1041.133529663086\n",
      "Epoch: 14750\tTrain loss: 795.3462524414062\n",
      "Epoch: 14800\tTrain loss: 674.5535278320312\n",
      "Epoch: 14850\tTrain loss: 609.5057373046875\n",
      "Epoch: 14900\tTrain loss: 566.4060821533203\n",
      "Epoch: 14950\tTrain loss: 522.7994079589844\n",
      "Epoch: 15000\tTrain loss: 461.6572265625\n",
      "Epoch: 15050\tTrain loss: 405.3369140625\n",
      "Epoch: 15100\tTrain loss: 363.4197540283203\n",
      "Epoch: 15150\tTrain loss: 335.06236267089844\n",
      "Epoch: 15200\tTrain loss: 314.31451416015625\n",
      "Epoch: 15250\tTrain loss: 301.06941986083984\n",
      "Epoch: 15300\tTrain loss: 281.5083923339844\n",
      "Epoch: 15350\tTrain loss: 270.92919921875\n",
      "Epoch: 15400\tTrain loss: 262.74432373046875\n",
      "Epoch: 15450\tTrain loss: 255.59461212158203\n",
      "Epoch: 15500\tTrain loss: 249.0833511352539\n",
      "Epoch: 15550\tTrain loss: 243.3653450012207\n",
      "Epoch: 15600\tTrain loss: 238.1694793701172\n",
      "Epoch: 15650\tTrain loss: 233.38690185546875\n",
      "Epoch: 15700\tTrain loss: 228.81826400756836\n",
      "Epoch: 15750\tTrain loss: 225.15032958984375\n",
      "Epoch: 15800\tTrain loss: 222.4493865966797\n",
      "Epoch: 15850\tTrain loss: 218.53023529052734\n",
      "Epoch: 15900\tTrain loss: 214.88528442382812\n",
      "Epoch: 15950\tTrain loss: 211.45502090454102\n",
      "Epoch: 16000\tTrain loss: 208.13032054901123\n",
      "Epoch: 16050\tTrain loss: 205.04181289672852\n",
      "Epoch: 16100\tTrain loss: 202.3253173828125\n",
      "Epoch: 16150\tTrain loss: 199.37831115722656\n",
      "Epoch: 16200\tTrain loss: 196.72410583496094\n",
      "Epoch: 16250\tTrain loss: 194.10909461975098\n",
      "Epoch: 16300\tTrain loss: 191.92359733581543\n",
      "Epoch: 16350\tTrain loss: 189.42767906188965\n",
      "Epoch: 16400\tTrain loss: 187.12893295288086\n",
      "Epoch: 16450\tTrain loss: 184.974365234375\n",
      "Epoch: 16500\tTrain loss: 182.88137650489807\n",
      "Epoch: 16550\tTrain loss: 183.77545166015625\n",
      "Epoch: 16600\tTrain loss: 179.22499084472656\n",
      "Epoch: 16650\tTrain loss: 177.25370919704437\n",
      "Epoch: 16700\tTrain loss: 175.5278091430664\n",
      "Epoch: 16750\tTrain loss: 173.86874389648438\n",
      "Epoch: 16800\tTrain loss: 1261.7257690429688\n",
      "Epoch: 16850\tTrain loss: 506.92517852783203\n",
      "Epoch: 16900\tTrain loss: 456.76953125\n",
      "Epoch: 16950\tTrain loss: 416.6043395996094\n",
      "Epoch: 17000\tTrain loss: 392.5364685058594\n",
      "Epoch: 17050\tTrain loss: 375.5442199707031\n",
      "Epoch: 17100\tTrain loss: 362.5783233642578\n",
      "Epoch: 17150\tTrain loss: 351.1212615966797\n",
      "Epoch: 17200\tTrain loss: 343.6861572265625\n",
      "Epoch: 17250\tTrain loss: 334.0750961303711\n",
      "Epoch: 17300\tTrain loss: 326.09740447998047\n",
      "Epoch: 17350\tTrain loss: 318.7466583251953\n",
      "Epoch: 17400\tTrain loss: 310.1911163330078\n",
      "Epoch: 17450\tTrain loss: 283.3765563964844\n",
      "Epoch: 17500\tTrain loss: 273.5646743774414\n",
      "Epoch: 17550\tTrain loss: 265.2570037841797\n",
      "Epoch: 17600\tTrain loss: 261.49385023117065\n",
      "Epoch: 17650\tTrain loss: 254.89867401123047\n",
      "Epoch: 17700\tTrain loss: 248.42429733276367\n",
      "Epoch: 17750\tTrain loss: 243.52085876464844\n",
      "Epoch: 17800\tTrain loss: 238.0826759338379\n",
      "Epoch: 17850\tTrain loss: 233.80096435546875\n",
      "Epoch: 17900\tTrain loss: 229.1346435546875\n",
      "Epoch: 17950\tTrain loss: 224.3575668334961\n",
      "Epoch: 18000\tTrain loss: 220.00991821289062\n",
      "Epoch: 18050\tTrain loss: 215.56674194335938\n",
      "Epoch: 18100\tTrain loss: 210.79730224609375\n",
      "Epoch: 18150\tTrain loss: 204.44761085510254\n",
      "Epoch: 18200\tTrain loss: 196.62353515625\n",
      "Epoch: 18250\tTrain loss: 1709.330810546875\n",
      "Epoch: 18300\tTrain loss: 816.1194152832031\n",
      "Epoch: 18350\tTrain loss: 350.53981018066406\n",
      "Epoch: 18400\tTrain loss: 321.7887496948242\n",
      "Epoch: 18450\tTrain loss: 300.3019256591797\n",
      "Epoch: 18500\tTrain loss: 286.33387756347656\n",
      "Epoch: 18550\tTrain loss: 264.4348449707031\n",
      "Epoch: 18600\tTrain loss: 243.0135498046875\n",
      "Epoch: 18650\tTrain loss: 211.9201831817627\n",
      "Epoch: 18700\tTrain loss: 194.9073028564453\n",
      "Epoch: 18750\tTrain loss: 184.12121045589447\n",
      "Epoch: 18800\tTrain loss: 180.72423553466797\n",
      "Epoch: 18850\tTrain loss: 177.07840061187744\n",
      "Epoch: 18900\tTrain loss: 175.5782241821289\n",
      "Epoch: 18950\tTrain loss: 176.38508796691895\n",
      "Epoch: 19000\tTrain loss: 171.08485412597656\n",
      "Epoch: 19050\tTrain loss: 168.4509048461914\n",
      "Epoch: 19100\tTrain loss: 166.23510074615479\n",
      "Epoch: 19150\tTrain loss: 164.23902463912964\n",
      "Epoch: 19200\tTrain loss: 162.3411979675293\n",
      "Epoch: 19250\tTrain loss: 160.6364288330078\n",
      "Epoch: 19300\tTrain loss: 174.23557376861572\n",
      "Epoch: 19350\tTrain loss: 158.24562454223633\n",
      "Epoch: 19400\tTrain loss: 156.23872756958008\n",
      "Epoch: 19450\tTrain loss: 154.36570739746094\n",
      "Epoch: 19500\tTrain loss: 152.32388278096914\n",
      "Epoch: 19550\tTrain loss: 150.0436019897461\n",
      "Epoch: 19600\tTrain loss: 147.54943418502808\n",
      "Epoch: 19650\tTrain loss: 144.9318618774414\n",
      "Epoch: 19700\tTrain loss: 144.30329132080078\n",
      "Epoch: 19750\tTrain loss: 139.4093578606844\n",
      "Epoch: 19800\tTrain loss: 160.85596466064453\n",
      "Epoch: 19850\tTrain loss: 142.64708971977234\n",
      "Epoch: 19900\tTrain loss: 136.7088122367859\n",
      "Epoch: 19950\tTrain loss: 130.3423912525177\n",
      "Epoch: 20000\tTrain loss: 123.60391998291016\n",
      "Epoch: 20050\tTrain loss: 111.09103399515152\n",
      "Epoch: 20100\tTrain loss: 135.76725006103516\n",
      "Epoch: 20150\tTrain loss: 97.95257043838501\n",
      "Epoch: 20200\tTrain loss: 344.28936767578125\n",
      "Epoch: 20250\tTrain loss: 270.7941436767578\n",
      "Epoch: 20300\tTrain loss: 211.66438484191895\n",
      "Epoch: 20350\tTrain loss: 201.81660759449005\n",
      "Epoch: 20400\tTrain loss: 197.14091730117798\n",
      "Epoch: 20450\tTrain loss: 194.6624755859375\n",
      "Epoch: 20500\tTrain loss: 190.97406768798828\n",
      "Epoch: 20550\tTrain loss: 187.0915985107422\n",
      "Epoch: 20600\tTrain loss: 182.45482773333788\n",
      "Epoch: 20650\tTrain loss: 178.40357208251953\n",
      "Epoch: 20700\tTrain loss: 171.0403436422348\n",
      "Epoch: 20750\tTrain loss: 159.58237332105637\n",
      "Epoch: 20800\tTrain loss: 155.06217193603516\n",
      "Epoch: 20850\tTrain loss: 153.10945892333984\n",
      "Epoch: 20900\tTrain loss: 151.91488647460938\n",
      "Epoch: 20950\tTrain loss: 150.98699060082436\n",
      "Epoch: 21000\tTrain loss: 150.47100830078125\n",
      "Epoch: 21050\tTrain loss: 149.75760029256344\n",
      "Epoch: 21100\tTrain loss: 149.20656721293926\n",
      "Epoch: 21150\tTrain loss: 148.85781860351562\n",
      "Epoch: 21200\tTrain loss: 148.05487569049\n",
      "Epoch: 21250\tTrain loss: 147.4298616796732\n",
      "Epoch: 21300\tTrain loss: 146.69739713147283\n",
      "Epoch: 21350\tTrain loss: 146.06378343701363\n",
      "Epoch: 21400\tTrain loss: 144.92487866431475\n",
      "Epoch: 21450\tTrain loss: 143.22470524907112\n",
      "Epoch: 21500\tTrain loss: 140.2785415649414\n",
      "Epoch: 21550\tTrain loss: 136.9152603149414\n",
      "Epoch: 21600\tTrain loss: 133.11227416992188\n",
      "Epoch: 21650\tTrain loss: 108.64875793457031\n",
      "Epoch: 21700\tTrain loss: 92.8158619850874\n",
      "Epoch: 21750\tTrain loss: 82.26758523285389\n",
      "Epoch: 21800\tTrain loss: 74.93093490600586\n",
      "Epoch: 21850\tTrain loss: 66.05590057373047\n",
      "Epoch: 21900\tTrain loss: 59.49837684631348\n",
      "Epoch: 21950\tTrain loss: 395.84199714660645\n",
      "Epoch: 22000\tTrain loss: 262.6914978027344\n",
      "Epoch: 22050\tTrain loss: 223.7853775024414\n",
      "Epoch: 22100\tTrain loss: 187.7420539855957\n",
      "Epoch: 22150\tTrain loss: 161.80084609985352\n",
      "Epoch: 22200\tTrain loss: 141.34551239013672\n",
      "Epoch: 22250\tTrain loss: 137.26471710205078\n",
      "Epoch: 22300\tTrain loss: 118.43345165252686\n",
      "Epoch: 22350\tTrain loss: 88.93526840209961\n",
      "Epoch: 22400\tTrain loss: 180.00245666503906\n",
      "Epoch: 22450\tTrain loss: 159.53173351287842\n",
      "Epoch: 22500\tTrain loss: 130.91491603851318\n",
      "Epoch: 22550\tTrain loss: 104.4698555469513\n",
      "Epoch: 22600\tTrain loss: 6600.582382202148\n",
      "Epoch: 22650\tTrain loss: 251.98542022705078\n",
      "Epoch: 22700\tTrain loss: 184.1820297241211\n",
      "Epoch: 22750\tTrain loss: 174.4793701171875\n",
      "Epoch: 22800\tTrain loss: 168.44844818115234\n",
      "Epoch: 22850\tTrain loss: 163.88310718536377\n",
      "Epoch: 22900\tTrain loss: 160.4592068195343\n",
      "Epoch: 22950\tTrain loss: 157.89890265464783\n",
      "Epoch: 23000\tTrain loss: 155.89295268058777\n",
      "Epoch: 23050\tTrain loss: 2770.8902435302734\n",
      "Epoch: 23100\tTrain loss: 226.85528945922852\n",
      "Epoch: 23150\tTrain loss: 116.33564949035645\n",
      "Epoch: 23200\tTrain loss: 89.68572616577148\n",
      "Epoch: 23250\tTrain loss: 78.56212043762207\n",
      "Epoch: 23300\tTrain loss: 71.24345016479492\n",
      "Epoch: 23350\tTrain loss: 65.4281769990921\n",
      "Epoch: 23400\tTrain loss: 60.4720401763916\n",
      "Epoch: 23450\tTrain loss: 56.15403509140015\n",
      "Epoch: 23500\tTrain loss: 52.30158531665802\n",
      "Epoch: 23550\tTrain loss: 48.824174880981445\n",
      "Epoch: 23600\tTrain loss: 45.62435209751129\n",
      "Epoch: 23650\tTrain loss: 42.651145935058594\n",
      "Epoch: 23700\tTrain loss: 39.86374092102051\n",
      "Epoch: 23750\tTrain loss: 37.20498812198639\n",
      "Epoch: 23800\tTrain loss: 34.75675868988037\n",
      "Epoch: 23850\tTrain loss: 32.40821933746338\n",
      "Epoch: 23900\tTrain loss: 30.175439834594727\n",
      "Epoch: 23950\tTrain loss: 28.040847182273865\n",
      "Epoch: 24000\tTrain loss: 26.006545066833496\n",
      "Epoch: 24050\tTrain loss: 24.181222915649414\n",
      "Epoch: 24100\tTrain loss: 25.268712043762207\n",
      "Epoch: 24150\tTrain loss: 20.801074981689453\n",
      "Epoch: 24200\tTrain loss: 19.058207988739014\n",
      "Epoch: 24250\tTrain loss: 17.44711744785309\n",
      "Epoch: 24300\tTrain loss: 15.937329083681107\n",
      "Epoch: 24350\tTrain loss: 14.916992664337158\n",
      "Epoch: 24400\tTrain loss: 13.333182334899902\n",
      "Epoch: 24450\tTrain loss: 12.25416088104248\n",
      "Epoch: 24500\tTrain loss: 13.536219596862793\n",
      "Epoch: 24550\tTrain loss: 10.118326544761658\n",
      "Epoch: 24600\tTrain loss: 8.947614669799805\n",
      "Epoch: 24650\tTrain loss: 8.635632187128067\n",
      "Epoch: 24700\tTrain loss: 9.933066368103027\n",
      "Epoch: 24750\tTrain loss: 6.353657364845276\n",
      "Epoch: 24800\tTrain loss: 5.5781858041882515\n",
      "Epoch: 24850\tTrain loss: 4.913366794586182\n",
      "Epoch: 24900\tTrain loss: 5.772106409072876\n",
      "Epoch: 24950\tTrain loss: 3.9829952716827393\n",
      "Epoch: 25000\tTrain loss: 3.356557633727789\n",
      "Epoch: 25050\tTrain loss: 2.9003387689590454\n",
      "Epoch: 25100\tTrain loss: 2.5039360523223877\n",
      "Epoch: 25150\tTrain loss: 2.159183979034424\n",
      "Epoch: 25200\tTrain loss: 3.692692816257477\n",
      "Epoch: 25250\tTrain loss: 2.0025276243686676\n",
      "Epoch: 25300\tTrain loss: 1.493095874786377\n",
      "Epoch: 25350\tTrain loss: 1.2874757200479507\n",
      "Epoch: 25400\tTrain loss: 1.743531346321106\n",
      "Epoch: 25450\tTrain loss: 2.9846982955932617\n",
      "Epoch: 25500\tTrain loss: 0.8243153989315033\n",
      "Epoch: 25550\tTrain loss: 0.6611806154251099\n",
      "Epoch: 25600\tTrain loss: 0.5600562691688538\n",
      "Epoch: 25650\tTrain loss: 0.6384965479373932\n",
      "Epoch: 25700\tTrain loss: 0.6748833954334259\n",
      "Epoch: 25750\tTrain loss: 0.6212382167577744\n",
      "Epoch: 25800\tTrain loss: 0.5985798835754395\n",
      "Epoch: 25850\tTrain loss: 1.822998583316803\n",
      "Epoch: 25900\tTrain loss: 15.650406837463379\n",
      "Epoch: 25950\tTrain loss: 969.238525390625\n",
      "Epoch: 26000\tTrain loss: 9536.62939453125\n",
      "Epoch: 26050\tTrain loss: 8226.229248046875\n",
      "Epoch: 26100\tTrain loss: 3482.7379760742188\n",
      "Epoch: 26150\tTrain loss: 1999.8934326171875\n",
      "Epoch: 26200\tTrain loss: 1021.0898132324219\n",
      "Epoch: 26250\tTrain loss: 1044.1806335449219\n",
      "Epoch: 26300\tTrain loss: 809.2263793945312\n",
      "Epoch: 26350\tTrain loss: 704.8008346557617\n",
      "Epoch: 26400\tTrain loss: 761.0143127441406\n",
      "Epoch: 26450\tTrain loss: 690.9442291259766\n",
      "Epoch: 26500\tTrain loss: 643.9045562744141\n",
      "Epoch: 26550\tTrain loss: 604.811767578125\n",
      "Epoch: 26600\tTrain loss: 571.4596557617188\n",
      "Epoch: 26650\tTrain loss: 542.7736434936523\n",
      "Epoch: 26700\tTrain loss: 517.6918640136719\n",
      "Epoch: 26750\tTrain loss: 495.7471160888672\n",
      "Epoch: 26800\tTrain loss: 476.3520202636719\n",
      "Epoch: 26850\tTrain loss: 458.82707595825195\n",
      "Epoch: 26900\tTrain loss: 443.19152069091797\n",
      "Epoch: 26950\tTrain loss: 428.8343276977539\n",
      "Epoch: 27000\tTrain loss: 415.25762939453125\n",
      "Epoch: 27050\tTrain loss: 402.2884979248047\n",
      "Epoch: 27100\tTrain loss: 389.63848876953125\n",
      "Epoch: 27150\tTrain loss: 377.79563903808594\n",
      "Epoch: 27200\tTrain loss: 366.57989501953125\n",
      "Epoch: 27250\tTrain loss: 355.56561279296875\n",
      "Epoch: 27300\tTrain loss: 345.0617370605469\n",
      "Epoch: 27350\tTrain loss: 333.25598907470703\n",
      "Epoch: 27400\tTrain loss: 322.1621856689453\n",
      "Epoch: 27450\tTrain loss: 313.62845611572266\n",
      "Epoch: 27500\tTrain loss: 305.34217834472656\n",
      "Epoch: 27550\tTrain loss: 296.9681396484375\n",
      "Epoch: 27600\tTrain loss: 288.3928756713867\n",
      "Epoch: 27650\tTrain loss: 279.64939880371094\n",
      "Epoch: 27700\tTrain loss: 270.1435089111328\n",
      "Epoch: 27750\tTrain loss: 259.85218048095703\n",
      "Epoch: 27800\tTrain loss: 249.06034088134766\n",
      "Epoch: 27850\tTrain loss: 235.94442749023438\n",
      "Epoch: 27900\tTrain loss: 222.47572326660156\n",
      "Epoch: 27950\tTrain loss: 209.1003761291504\n",
      "Epoch: 28000\tTrain loss: 198.08410263061523\n",
      "Epoch: 28050\tTrain loss: 186.28085327148438\n",
      "Epoch: 28100\tTrain loss: 176.1759376525879\n",
      "Epoch: 28150\tTrain loss: 171.28651428222656\n",
      "Epoch: 28200\tTrain loss: 156.99114608764648\n",
      "Epoch: 28250\tTrain loss: 138.71212005615234\n",
      "Epoch: 28300\tTrain loss: 144.4697380065918\n",
      "Epoch: 28350\tTrain loss: 134.92790603637695\n",
      "Epoch: 28400\tTrain loss: 125.27730560302734\n",
      "Epoch: 28450\tTrain loss: 115.09077906608582\n",
      "Epoch: 28500\tTrain loss: 104.95182237029076\n",
      "Epoch: 28550\tTrain loss: 99.84040832519531\n",
      "Epoch: 28600\tTrain loss: 84.06237983703613\n",
      "Epoch: 28650\tTrain loss: 81.25263214111328\n",
      "Epoch: 28700\tTrain loss: 72.6573429107666\n",
      "Epoch: 28750\tTrain loss: 65.46341323852539\n",
      "Epoch: 28800\tTrain loss: 58.37434959411621\n",
      "Epoch: 28850\tTrain loss: 52.211320877075195\n",
      "Epoch: 28900\tTrain loss: 47.901429176330566\n",
      "Epoch: 28950\tTrain loss: 41.39157485961914\n",
      "Epoch: 29000\tTrain loss: 36.25570297241211\n",
      "Epoch: 29050\tTrain loss: 30.80640411376953\n",
      "Epoch: 29100\tTrain loss: 25.10984230041504\n",
      "Epoch: 29150\tTrain loss: 20.727100372314453\n",
      "Epoch: 29200\tTrain loss: 1564.8867492675781\n",
      "Epoch: 29250\tTrain loss: 566.1810503005981\n",
      "Epoch: 29300\tTrain loss: 288.6151466369629\n",
      "Epoch: 29350\tTrain loss: 245.1847801208496\n",
      "Epoch: 29400\tTrain loss: 243.52703475952148\n",
      "Epoch: 29450\tTrain loss: 220.28174591064453\n",
      "Epoch: 29500\tTrain loss: 202.8671498298645\n",
      "Epoch: 29550\tTrain loss: 200.68651580810547\n",
      "Epoch: 29600\tTrain loss: 191.9577178955078\n",
      "Epoch: 29650\tTrain loss: 185.24597549438477\n",
      "Epoch: 29700\tTrain loss: 186.1711883544922\n",
      "Epoch: 29750\tTrain loss: 177.2975616455078\n",
      "Epoch: 29800\tTrain loss: 173.2355194091797\n",
      "Epoch: 29850\tTrain loss: 170.6891632080078\n",
      "Epoch: 29900\tTrain loss: 182.21625518798828\n",
      "Epoch: 29950\tTrain loss: 172.72991943359375\n",
      "Epoch: 30000\tTrain loss: 166.78261375427246\n",
      "Epoch: 30050\tTrain loss: 163.29312630742788\n",
      "Epoch: 30100\tTrain loss: 159.36063627898693\n",
      "Epoch: 30150\tTrain loss: 156.43390369415283\n",
      "Epoch: 30200\tTrain loss: 153.9960174560547\n",
      "Epoch: 30250\tTrain loss: 154.32096099853516\n",
      "Epoch: 30300\tTrain loss: 150.73786783218384\n",
      "Epoch: 30350\tTrain loss: 149.65008568763733\n",
      "Epoch: 30400\tTrain loss: 149.75985717773438\n",
      "Epoch: 30450\tTrain loss: 144.23508834838867\n",
      "Epoch: 30500\tTrain loss: 138.91345596313477\n",
      "Epoch: 30550\tTrain loss: 134.76773124933243\n",
      "Epoch: 30600\tTrain loss: 125.28457260131836\n",
      "Epoch: 30650\tTrain loss: 117.61594772338867\n",
      "Epoch: 30700\tTrain loss: 110.91985416412354\n",
      "Epoch: 30750\tTrain loss: 124.07878112792969\n",
      "Epoch: 30800\tTrain loss: 105.28292083740234\n",
      "Epoch: 30850\tTrain loss: 94.87051773071289\n",
      "Epoch: 30900\tTrain loss: 87.05803310871124\n",
      "Epoch: 30950\tTrain loss: 98.58452606201172\n",
      "Epoch: 31000\tTrain loss: 70.35000610351562\n",
      "Epoch: 31050\tTrain loss: 80.52587342262268\n",
      "Epoch: 31100\tTrain loss: 61.538320541381836\n",
      "Epoch: 31150\tTrain loss: 50.37494897842407\n",
      "Epoch: 31200\tTrain loss: 596.3119697570801\n",
      "Epoch: 31250\tTrain loss: 251.8486328125\n",
      "Epoch: 31300\tTrain loss: 125.84313488006592\n",
      "Epoch: 31350\tTrain loss: 71.88268852233887\n",
      "Epoch: 31400\tTrain loss: 58.774254858493805\n",
      "Epoch: 31450\tTrain loss: 49.249409675598145\n",
      "Epoch: 31500\tTrain loss: 38.038366228342056\n",
      "Epoch: 31550\tTrain loss: 26.722187876701355\n",
      "Epoch: 31600\tTrain loss: 22.729326248168945\n",
      "Epoch: 31650\tTrain loss: 15.17672872543335\n",
      "Epoch: 31700\tTrain loss: 12.258790969848633\n",
      "Epoch: 31750\tTrain loss: 9.024938583374023\n",
      "Epoch: 31800\tTrain loss: 297.6873278617859\n",
      "Epoch: 31850\tTrain loss: 2819.4610900878906\n",
      "Epoch: 31900\tTrain loss: 120.80514335632324\n",
      "Epoch: 31950\tTrain loss: 90.78215026855469\n",
      "Epoch: 32000\tTrain loss: 65.68110847473145\n",
      "Epoch: 32050\tTrain loss: 34.70121479034424\n",
      "Epoch: 32100\tTrain loss: 28.387868881225586\n",
      "Epoch: 32150\tTrain loss: 24.15104579925537\n",
      "Epoch: 32200\tTrain loss: 21.004584789276123\n",
      "Epoch: 32250\tTrain loss: 18.484456300735474\n",
      "Epoch: 32300\tTrain loss: 16.386045455932617\n",
      "Epoch: 32350\tTrain loss: 14.567745923995972\n",
      "Epoch: 32400\tTrain loss: 12.942064046859741\n",
      "Epoch: 32450\tTrain loss: 11.473179578781128\n",
      "Epoch: 32500\tTrain loss: 10.129288077354431\n",
      "Epoch: 32550\tTrain loss: 8.922748565673828\n",
      "Epoch: 32600\tTrain loss: 7.817029237747192\n",
      "Epoch: 32650\tTrain loss: 6.818103313446045\n",
      "Epoch: 32700\tTrain loss: 5.918846786022186\n",
      "Epoch: 32750\tTrain loss: 5.118585824966431\n",
      "Epoch: 32800\tTrain loss: 4.400273025035858\n",
      "Epoch: 32850\tTrain loss: 3.7745431065559387\n",
      "Epoch: 32900\tTrain loss: 3.2205153107643127\n",
      "Epoch: 32950\tTrain loss: 2.7338325679302216\n",
      "Epoch: 33000\tTrain loss: 2.3176772594451904\n",
      "Epoch: 33050\tTrain loss: 1.9567875564098358\n",
      "Epoch: 33100\tTrain loss: 1.6485872715711594\n",
      "Epoch: 33150\tTrain loss: 1.3900248110294342\n",
      "Epoch: 33200\tTrain loss: 1.1726255118846893\n",
      "Epoch: 33250\tTrain loss: 0.9898403286933899\n",
      "Epoch: 33300\tTrain loss: 0.837764099240303\n",
      "Epoch: 33350\tTrain loss: 0.7093292623758316\n",
      "Epoch: 33400\tTrain loss: 0.6024200543761253\n",
      "Epoch: 33450\tTrain loss: 0.5146481916308403\n",
      "Epoch: 33500\tTrain loss: 0.44082726538181305\n",
      "Epoch: 33550\tTrain loss: 0.3796103149652481\n",
      "Epoch: 33600\tTrain loss: 0.3356240727007389\n",
      "Epoch: 33650\tTrain loss: 0.28281763941049576\n",
      "Epoch: 33700\tTrain loss: 0.22695207595825195\n",
      "Epoch: 33750\tTrain loss: 0.19170575588941574\n",
      "Epoch: 33800\tTrain loss: 0.1613815240561962\n",
      "Epoch: 33850\tTrain loss: 0.14277121797204018\n",
      "Epoch: 33900\tTrain loss: 1.4382764883339405\n",
      "Epoch: 33950\tTrain loss: 1.3383680582046509\n",
      "Epoch: 34000\tTrain loss: 934.9609375\n",
      "Epoch: 34050\tTrain loss: 484.47537994384766\n",
      "Epoch: 34100\tTrain loss: 438.9001922607422\n",
      "Epoch: 34150\tTrain loss: 414.6939239501953\n",
      "Epoch: 34200\tTrain loss: 398.4754638671875\n",
      "Epoch: 34250\tTrain loss: 386.22044372558594\n",
      "Epoch: 34300\tTrain loss: 375.5093536376953\n",
      "Epoch: 34350\tTrain loss: 364.5915222167969\n",
      "Epoch: 34400\tTrain loss: 347.93597412109375\n",
      "Epoch: 34450\tTrain loss: 323.5063171386719\n",
      "Epoch: 34500\tTrain loss: 303.63671875\n",
      "Epoch: 34550\tTrain loss: 285.11952209472656\n",
      "Epoch: 34600\tTrain loss: 265.1180953979492\n",
      "Epoch: 34650\tTrain loss: 250.54658126831055\n",
      "Epoch: 34700\tTrain loss: 240.07136917114258\n",
      "Epoch: 34750\tTrain loss: 231.29741668701172\n",
      "Epoch: 34800\tTrain loss: 223.4019012451172\n",
      "Epoch: 34850\tTrain loss: 216.14210319519043\n",
      "Epoch: 34900\tTrain loss: 209.63550567626953\n",
      "Epoch: 34950\tTrain loss: 203.5358123779297\n",
      "Epoch: 35000\tTrain loss: 197.79961395263672\n",
      "Epoch: 35050\tTrain loss: 192.69036102294922\n",
      "Epoch: 35100\tTrain loss: 187.92544555664062\n",
      "Epoch: 35150\tTrain loss: 183.34585571289062\n",
      "Epoch: 35200\tTrain loss: 179.01879119873047\n",
      "Epoch: 35250\tTrain loss: 391.91876220703125\n",
      "Epoch: 35300\tTrain loss: 346.3306579589844\n",
      "Epoch: 35350\tTrain loss: 325.7372417449951\n",
      "Epoch: 35400\tTrain loss: 314.9434814453125\n",
      "Epoch: 35450\tTrain loss: 307.2255401611328\n",
      "Epoch: 35500\tTrain loss: 300.84554290771484\n",
      "Epoch: 35550\tTrain loss: 295.1665573120117\n",
      "Epoch: 35600\tTrain loss: 288.5234069824219\n",
      "Epoch: 35650\tTrain loss: 281.8020248413086\n",
      "Epoch: 35700\tTrain loss: 273.5756301879883\n",
      "Epoch: 35750\tTrain loss: 262.0763702392578\n",
      "Epoch: 35800\tTrain loss: 246.40696716308594\n",
      "Epoch: 35850\tTrain loss: 233.5186004638672\n",
      "Epoch: 35900\tTrain loss: 220.33188247680664\n",
      "Epoch: 35950\tTrain loss: 233.10881805419922\n",
      "Epoch: 36000\tTrain loss: 208.3530502319336\n",
      "Epoch: 36050\tTrain loss: 209.51361083984375\n",
      "Epoch: 36100\tTrain loss: 214.9316864013672\n",
      "Epoch: 36150\tTrain loss: 202.1234359741211\n",
      "Epoch: 36200\tTrain loss: 190.4893035888672\n",
      "Epoch: 36250\tTrain loss: 412.9562683105469\n",
      "Epoch: 36300\tTrain loss: 263.8511428833008\n",
      "Epoch: 36350\tTrain loss: 251.07238006591797\n",
      "Epoch: 36400\tTrain loss: 242.10520935058594\n",
      "Epoch: 36450\tTrain loss: 233.81838989257812\n",
      "Epoch: 36500\tTrain loss: 227.20952606201172\n",
      "Epoch: 36550\tTrain loss: 218.76455688476562\n",
      "Epoch: 36600\tTrain loss: 205.6266860961914\n",
      "Epoch: 36650\tTrain loss: 191.08280181884766\n",
      "Epoch: 36700\tTrain loss: 182.66221618652344\n",
      "Epoch: 36750\tTrain loss: 176.60441970825195\n",
      "Epoch: 36800\tTrain loss: 170.33340454101562\n",
      "Epoch: 36850\tTrain loss: 163.56343460083008\n",
      "Epoch: 36900\tTrain loss: 155.43471145629883\n",
      "Epoch: 36950\tTrain loss: 149.61707305908203\n",
      "Epoch: 37000\tTrain loss: 147.29130172729492\n",
      "Epoch: 37050\tTrain loss: 160.28985595703125\n",
      "Epoch: 37100\tTrain loss: 138.48920440673828\n",
      "Epoch: 37150\tTrain loss: 134.77106094360352\n",
      "Epoch: 37200\tTrain loss: 138.8561019897461\n",
      "Epoch: 37250\tTrain loss: 145.37352752685547\n",
      "Epoch: 37300\tTrain loss: 126.30120468139648\n",
      "Epoch: 37350\tTrain loss: 172.9509620666504\n",
      "Epoch: 37400\tTrain loss: 136.38368606567383\n",
      "Epoch: 37450\tTrain loss: 122.60627365112305\n",
      "Epoch: 37500\tTrain loss: 112.52184295654297\n",
      "Epoch: 37550\tTrain loss: 104.36558151245117\n",
      "Epoch: 37600\tTrain loss: 97.1594123840332\n",
      "Epoch: 37650\tTrain loss: 90.55891418457031\n",
      "Epoch: 37700\tTrain loss: 84.19626235961914\n",
      "Epoch: 37750\tTrain loss: 78.17753410339355\n",
      "Epoch: 37800\tTrain loss: 302.6936340332031\n",
      "Epoch: 37850\tTrain loss: 130.23419618606567\n",
      "Epoch: 37900\tTrain loss: 121.8569564819336\n",
      "Epoch: 37950\tTrain loss: 130.12393188476562\n",
      "Epoch: 38000\tTrain loss: 116.84506225585938\n",
      "Epoch: 38050\tTrain loss: 111.75520706176758\n",
      "Epoch: 38100\tTrain loss: 111.58816528320312\n",
      "Epoch: 38150\tTrain loss: 205.60433197021484\n",
      "Epoch: 38200\tTrain loss: 124.2923583984375\n",
      "Epoch: 38250\tTrain loss: 111.93244743347168\n",
      "Epoch: 38300\tTrain loss: 105.36563491821289\n",
      "Epoch: 38350\tTrain loss: 100.10359382629395\n",
      "Epoch: 38400\tTrain loss: 101.90097427368164\n",
      "Epoch: 38450\tTrain loss: 94.20046615600586\n",
      "Epoch: 38500\tTrain loss: 89.74515914916992\n",
      "Epoch: 38550\tTrain loss: 97.8706169128418\n",
      "Epoch: 38600\tTrain loss: 84.92280769348145\n",
      "Epoch: 38650\tTrain loss: 81.44636154174805\n",
      "Epoch: 38700\tTrain loss: 97.65657043457031\n",
      "Epoch: 38750\tTrain loss: 77.78028678894043\n",
      "Epoch: 38800\tTrain loss: 71.18298530578613\n",
      "Epoch: 38850\tTrain loss: 66.60696792602539\n",
      "Epoch: 38900\tTrain loss: 208.53695678710938\n",
      "Epoch: 38950\tTrain loss: 202.85393524169922\n",
      "Epoch: 39000\tTrain loss: 201.7490577697754\n",
      "Epoch: 39050\tTrain loss: 169.55106353759766\n",
      "Epoch: 39100\tTrain loss: 119.96135330200195\n",
      "Epoch: 39150\tTrain loss: 166.40435409545898\n",
      "Epoch: 39200\tTrain loss: 79.88820266723633\n",
      "Epoch: 39250\tTrain loss: 62.97484302520752\n",
      "Epoch: 39300\tTrain loss: 54.06964302062988\n",
      "Epoch: 39350\tTrain loss: 42.33684158325195\n",
      "Epoch: 39400\tTrain loss: 30.63294506072998\n",
      "Epoch: 39450\tTrain loss: 1260.9051971435547\n",
      "Epoch: 39500\tTrain loss: 1184.1300354003906\n",
      "Epoch: 39550\tTrain loss: 352.6812057495117\n",
      "Epoch: 39600\tTrain loss: 321.7662582397461\n",
      "Epoch: 39650\tTrain loss: 304.8885726928711\n",
      "Epoch: 39700\tTrain loss: 288.75826263427734\n",
      "Epoch: 39750\tTrain loss: 272.73807525634766\n",
      "Epoch: 39800\tTrain loss: 258.2596130371094\n",
      "Epoch: 39850\tTrain loss: 242.85421752929688\n",
      "Epoch: 39900\tTrain loss: 320.68429946899414\n",
      "Epoch: 39950\tTrain loss: 257.160888671875\n",
      "Epoch: 40000\tTrain loss: 237.47844648361206\n",
      "Epoch: 40050\tTrain loss: 196.59232711791992\n",
      "Epoch: 40100\tTrain loss: 162.64322662353516\n",
      "Epoch: 40150\tTrain loss: 151.0535011291504\n",
      "Epoch: 40200\tTrain loss: 142.15972518920898\n",
      "Epoch: 40250\tTrain loss: 135.35349082946777\n",
      "Epoch: 40300\tTrain loss: 127.38973617553711\n",
      "Epoch: 40350\tTrain loss: 120.05789947509766\n",
      "Epoch: 40400\tTrain loss: 115.47285842895508\n",
      "Epoch: 40450\tTrain loss: 108.92950248718262\n",
      "Epoch: 40500\tTrain loss: 102.64999008178711\n",
      "Epoch: 40550\tTrain loss: 105.7323169708252\n",
      "Epoch: 40600\tTrain loss: 128.4207558631897\n",
      "Epoch: 40650\tTrain loss: 88.30106735229492\n",
      "Epoch: 40700\tTrain loss: 83.11010360717773\n",
      "Epoch: 40750\tTrain loss: 78.33674430847168\n",
      "Epoch: 40800\tTrain loss: 73.86108207702637\n",
      "Epoch: 40850\tTrain loss: 69.71638488769531\n",
      "Epoch: 40900\tTrain loss: 65.78418159484863\n",
      "Epoch: 40950\tTrain loss: 62.03643798828125\n",
      "Epoch: 41000\tTrain loss: 58.57911562919617\n",
      "Epoch: 41050\tTrain loss: 55.310988426208496\n",
      "Epoch: 41100\tTrain loss: 52.1956090927124\n",
      "Epoch: 41150\tTrain loss: 49.262362003326416\n",
      "Epoch: 41200\tTrain loss: 46.47915077209473\n",
      "Epoch: 41250\tTrain loss: 43.752264976501465\n",
      "Epoch: 41300\tTrain loss: 41.16918659210205\n",
      "Epoch: 41350\tTrain loss: 38.82991170883179\n",
      "Epoch: 41400\tTrain loss: 36.30416417121887\n",
      "Epoch: 41450\tTrain loss: 34.010581970214844\n",
      "Epoch: 41500\tTrain loss: 32.29163122177124\n",
      "Epoch: 41550\tTrain loss: 29.77311897277832\n",
      "Epoch: 41600\tTrain loss: 28.09215223789215\n",
      "Epoch: 41650\tTrain loss: 26.336859226226807\n",
      "Epoch: 41700\tTrain loss: 24.166973114013672\n",
      "Epoch: 41750\tTrain loss: 27.05924367904663\n",
      "Epoch: 41800\tTrain loss: 21.079259872436523\n",
      "Epoch: 41850\tTrain loss: 19.470670700073242\n",
      "Epoch: 41900\tTrain loss: 17.98126494884491\n",
      "Epoch: 41950\tTrain loss: 16.631277084350586\n",
      "Epoch: 42000\tTrain loss: 15.39864206314087\n",
      "Epoch: 42050\tTrain loss: 15.052746772766113\n",
      "Epoch: 42100\tTrain loss: 13.060828685760498\n",
      "Epoch: 42150\tTrain loss: 12.048465430736542\n",
      "Epoch: 42200\tTrain loss: 13.1457998752594\n",
      "Epoch: 42250\tTrain loss: 12.333226680755615\n",
      "Epoch: 42300\tTrain loss: 9.327151775360107\n",
      "Epoch: 42350\tTrain loss: 8.510232925415039\n",
      "Epoch: 42400\tTrain loss: 7.762974560260773\n",
      "Epoch: 42450\tTrain loss: 7.067159686237574\n",
      "Epoch: 42500\tTrain loss: 6.4300137758255005\n",
      "Epoch: 42550\tTrain loss: 5.847275018692017\n",
      "Epoch: 42600\tTrain loss: 5.297482460737228\n",
      "Epoch: 42650\tTrain loss: 4.791831664741039\n",
      "Epoch: 42700\tTrain loss: 4.325355123728514\n",
      "Epoch: 42750\tTrain loss: 4.087715744972229\n",
      "Epoch: 42800\tTrain loss: 5.344381809234619\n",
      "Epoch: 42850\tTrain loss: 3.2137664556503296\n",
      "Epoch: 42900\tTrain loss: 2.8426783680915833\n",
      "Epoch: 42950\tTrain loss: 2.5335596948862076\n",
      "Epoch: 43000\tTrain loss: 2.262068808078766\n",
      "Epoch: 43050\tTrain loss: 2.0386050939559937\n",
      "Epoch: 43100\tTrain loss: 1.948835849761963\n",
      "Epoch: 43150\tTrain loss: 6.257805585861206\n",
      "Epoch: 43200\tTrain loss: 2.206184446811676\n",
      "Epoch: 43250\tTrain loss: 1.3327287435531616\n",
      "Epoch: 43300\tTrain loss: 1.163336120545864\n",
      "Epoch: 43350\tTrain loss: 1.0218126885592937\n",
      "Epoch: 43400\tTrain loss: 4158.82861328125\n",
      "Epoch: 43450\tTrain loss: 943.1248168945312\n",
      "Epoch: 43500\tTrain loss: 736.833740234375\n",
      "Epoch: 43550\tTrain loss: 624.7921752929688\n",
      "Epoch: 43600\tTrain loss: 573.49365234375\n",
      "Epoch: 43650\tTrain loss: 533.6198043823242\n",
      "Epoch: 43700\tTrain loss: 512.3643951416016\n",
      "Epoch: 43750\tTrain loss: 478.41070556640625\n",
      "Epoch: 43800\tTrain loss: 446.10003662109375\n",
      "Epoch: 43850\tTrain loss: 419.4130554199219\n",
      "Epoch: 43900\tTrain loss: 397.91802978515625\n",
      "Epoch: 43950\tTrain loss: 3150.920654296875\n",
      "Epoch: 44000\tTrain loss: 441.3254699707031\n",
      "Epoch: 44050\tTrain loss: 375.6549377441406\n",
      "Epoch: 44100\tTrain loss: 339.30165100097656\n",
      "Epoch: 44150\tTrain loss: 676.9430236816406\n",
      "Epoch: 44200\tTrain loss: 348.2656707763672\n",
      "Epoch: 44250\tTrain loss: 330.59745025634766\n",
      "Epoch: 44300\tTrain loss: 318.14139556884766\n",
      "Epoch: 44350\tTrain loss: 308.2077178955078\n",
      "Epoch: 44400\tTrain loss: 299.4191131591797\n",
      "Epoch: 44450\tTrain loss: 291.4441375732422\n",
      "Epoch: 44500\tTrain loss: 283.6913604736328\n",
      "Epoch: 44550\tTrain loss: 275.8174133300781\n",
      "Epoch: 44600\tTrain loss: 267.27930450439453\n",
      "Epoch: 44650\tTrain loss: 258.3779811859131\n",
      "Epoch: 44700\tTrain loss: 250.62537384033203\n",
      "Epoch: 44750\tTrain loss: 242.14535427093506\n",
      "Epoch: 44800\tTrain loss: 235.20899200439453\n",
      "Epoch: 44850\tTrain loss: 226.85613250732422\n",
      "Epoch: 44900\tTrain loss: 219.4203643798828\n",
      "Epoch: 44950\tTrain loss: 207.5435333251953\n",
      "Epoch: 45000\tTrain loss: 198.43188095092773\n",
      "Epoch: 45050\tTrain loss: 192.04515075683594\n",
      "Epoch: 45100\tTrain loss: 186.30784606933594\n",
      "Epoch: 45150\tTrain loss: 182.9118194580078\n",
      "Epoch: 45200\tTrain loss: 180.8590850830078\n",
      "Epoch: 45250\tTrain loss: 178.55501747131348\n",
      "Epoch: 45300\tTrain loss: 176.7168173789978\n",
      "Epoch: 45350\tTrain loss: 174.92776012420654\n",
      "Epoch: 45400\tTrain loss: 174.55489349365234\n",
      "Epoch: 45450\tTrain loss: 173.221923828125\n",
      "Epoch: 45500\tTrain loss: 171.10031127929688\n",
      "Epoch: 45550\tTrain loss: 169.9092559814453\n",
      "Epoch: 45600\tTrain loss: 168.90243530273438\n",
      "Epoch: 45650\tTrain loss: 168.0293960571289\n",
      "Epoch: 45700\tTrain loss: 167.0659749507904\n",
      "Epoch: 45750\tTrain loss: 166.36294555664062\n",
      "Epoch: 45800\tTrain loss: 165.6444320678711\n",
      "Epoch: 45850\tTrain loss: 165.7634048461914\n",
      "Epoch: 45900\tTrain loss: 166.25849151611328\n",
      "Epoch: 45950\tTrain loss: 164.3132781982422\n",
      "Epoch: 46000\tTrain loss: 163.49922943115234\n",
      "Epoch: 46050\tTrain loss: 162.63140106201172\n",
      "Epoch: 46100\tTrain loss: 162.05379486083984\n",
      "Epoch: 46150\tTrain loss: 161.26610565185547\n",
      "Epoch: 46200\tTrain loss: 160.47149658203125\n",
      "Epoch: 46250\tTrain loss: 163.40320587158203\n",
      "Epoch: 46300\tTrain loss: 1091.3068542480469\n",
      "Epoch: 46350\tTrain loss: 715.7959899902344\n",
      "Epoch: 46400\tTrain loss: 625.8671112060547\n",
      "Epoch: 46450\tTrain loss: 581.3880004882812\n",
      "Epoch: 46500\tTrain loss: 532.953857421875\n",
      "Epoch: 46550\tTrain loss: 575.5022583007812\n",
      "Epoch: 46600\tTrain loss: 2560.928321838379\n",
      "Epoch: 46650\tTrain loss: 913.3553161621094\n",
      "Epoch: 46700\tTrain loss: 708.4312286376953\n",
      "Epoch: 46750\tTrain loss: 603.8594818115234\n",
      "Epoch: 46800\tTrain loss: 543.6681060791016\n",
      "Epoch: 46850\tTrain loss: 498.64247131347656\n",
      "Epoch: 46900\tTrain loss: 463.5433654785156\n",
      "Epoch: 46950\tTrain loss: 440.8913269042969\n",
      "Epoch: 47000\tTrain loss: 853.6715393066406\n",
      "Epoch: 47050\tTrain loss: 403.3965606689453\n",
      "Epoch: 47100\tTrain loss: 376.42234802246094\n",
      "Epoch: 47150\tTrain loss: 357.56822204589844\n",
      "Epoch: 47200\tTrain loss: 339.2266616821289\n",
      "Epoch: 47250\tTrain loss: 326.9813690185547\n",
      "Epoch: 47300\tTrain loss: 321.5440216064453\n",
      "Epoch: 47350\tTrain loss: 296.2198944091797\n",
      "Epoch: 47400\tTrain loss: 290.7450180053711\n",
      "Epoch: 47450\tTrain loss: 274.08477783203125\n",
      "Epoch: 47500\tTrain loss: 260.6938705444336\n",
      "Epoch: 47550\tTrain loss: 245.83792877197266\n",
      "Epoch: 47600\tTrain loss: 233.65320587158203\n",
      "Epoch: 47650\tTrain loss: 742.0200042724609\n",
      "Epoch: 47700\tTrain loss: 262.30445098876953\n",
      "Epoch: 47750\tTrain loss: 249.92816925048828\n",
      "Epoch: 47800\tTrain loss: 239.1161994934082\n",
      "Epoch: 47850\tTrain loss: 230.68511486053467\n",
      "Epoch: 47900\tTrain loss: 224.01316452026367\n",
      "Epoch: 47950\tTrain loss: 218.44759368896484\n",
      "Epoch: 48000\tTrain loss: 213.3875560760498\n",
      "Epoch: 48050\tTrain loss: 208.94325256347656\n",
      "Epoch: 48100\tTrain loss: 204.6527121067047\n",
      "Epoch: 48150\tTrain loss: 200.84728240966797\n",
      "Epoch: 48200\tTrain loss: 197.18739318847656\n",
      "Epoch: 48250\tTrain loss: 193.76507568359375\n",
      "Epoch: 48300\tTrain loss: 190.5236587524414\n",
      "Epoch: 48350\tTrain loss: 187.27332592010498\n",
      "Epoch: 48400\tTrain loss: 184.21183013916016\n",
      "Epoch: 48450\tTrain loss: 181.13308000564575\n",
      "Epoch: 48500\tTrain loss: 177.83345937728882\n",
      "Epoch: 48550\tTrain loss: 174.15593719482422\n",
      "Epoch: 48600\tTrain loss: 170.04665637016296\n",
      "Epoch: 48650\tTrain loss: 166.69238948822021\n",
      "Epoch: 48700\tTrain loss: 164.51861095428467\n",
      "Epoch: 48750\tTrain loss: 163.01831531524658\n",
      "Epoch: 48800\tTrain loss: 161.7531509399414\n",
      "Epoch: 48850\tTrain loss: 160.43472161889076\n",
      "Epoch: 48900\tTrain loss: 159.24264526367188\n",
      "Epoch: 48950\tTrain loss: 157.90887451171875\n",
      "Epoch: 49000\tTrain loss: 156.29625713825226\n",
      "Epoch: 49050\tTrain loss: 154.2495880126953\n",
      "Epoch: 49100\tTrain loss: 151.50414776802063\n",
      "Epoch: 49150\tTrain loss: 149.1823272705078\n",
      "Epoch: 49200\tTrain loss: 146.5470962524414\n",
      "Epoch: 49250\tTrain loss: 143.52743977308273\n",
      "Epoch: 49300\tTrain loss: 140.3434066772461\n",
      "Epoch: 49350\tTrain loss: 136.38661193847656\n",
      "Epoch: 49400\tTrain loss: 141.81890106201172\n",
      "Epoch: 49450\tTrain loss: 125.96085357666016\n",
      "Epoch: 49500\tTrain loss: 113.82499194145203\n",
      "Epoch: 49550\tTrain loss: 91.42599296569824\n",
      "Epoch: 49600\tTrain loss: 100.90280532836914\n",
      "Epoch: 49650\tTrain loss: 70.16128015518188\n",
      "Epoch: 49700\tTrain loss: 56.80892467498779\n",
      "Epoch: 49750\tTrain loss: 47.91091787815094\n",
      "Epoch: 49800\tTrain loss: 828.6658058166504\n",
      "Epoch: 49850\tTrain loss: 3590.613639831543\n",
      "Epoch: 49900\tTrain loss: 357.5569763183594\n",
      "Epoch: 49950\tTrain loss: 5170.919189453125\n",
      "Training CFRNN\n",
      "Epoch: 0\tTrain loss: 4701698.671875\n",
      "Epoch: 50\tTrain loss: 4641081.25\n",
      "Epoch: 100\tTrain loss: 4584602.1875\n",
      "Epoch: 150\tTrain loss: 4529750.21875\n",
      "Epoch: 200\tTrain loss: 4477090.046875\n",
      "Epoch: 250\tTrain loss: 4425460.28125\n",
      "Epoch: 300\tTrain loss: 4374787.40625\n",
      "Epoch: 350\tTrain loss: 4325420.296875\n",
      "Epoch: 400\tTrain loss: 4277140.421875\n",
      "Epoch: 450\tTrain loss: 4230171.984375\n",
      "Epoch: 500\tTrain loss: 4183721.15625\n",
      "Epoch: 550\tTrain loss: 4138340.03125\n",
      "Epoch: 600\tTrain loss: 4094032.796875\n",
      "Epoch: 650\tTrain loss: 4050846.625\n",
      "Epoch: 700\tTrain loss: 4008252.640625\n",
      "Epoch: 750\tTrain loss: 3966627.3203125\n",
      "Epoch: 800\tTrain loss: 3925820.75\n",
      "Epoch: 850\tTrain loss: 3885682.3828125\n",
      "Epoch: 900\tTrain loss: 3846599.25\n",
      "Epoch: 950\tTrain loss: 3808278.8203125\n",
      "Epoch: 1000\tTrain loss: 3771103.0625\n",
      "Epoch: 1050\tTrain loss: 3734165.703125\n",
      "Epoch: 1100\tTrain loss: 3698309.25\n",
      "Epoch: 1150\tTrain loss: 3663262.3359375\n",
      "Epoch: 1200\tTrain loss: 3629121.640625\n",
      "Epoch: 1250\tTrain loss: 3595292.828125\n",
      "Epoch: 1300\tTrain loss: 3562425.98828125\n",
      "Epoch: 1350\tTrain loss: 3530433.1328125\n",
      "Epoch: 1400\tTrain loss: 3498890.4296875\n",
      "Epoch: 1450\tTrain loss: 3468225.15625\n",
      "Epoch: 1500\tTrain loss: 3438149.359375\n",
      "Epoch: 1550\tTrain loss: 3408730.83203125\n",
      "Epoch: 1600\tTrain loss: 3379837.6171875\n",
      "Epoch: 1650\tTrain loss: 3351993.78125\n",
      "Epoch: 1700\tTrain loss: 3324569.736328125\n",
      "Epoch: 1750\tTrain loss: 3297898.58984375\n",
      "Epoch: 1800\tTrain loss: 3271629.1328125\n",
      "Epoch: 1850\tTrain loss: 3246153.16015625\n",
      "Epoch: 1900\tTrain loss: 3221287.28515625\n",
      "Epoch: 1950\tTrain loss: 3197093.98046875\n",
      "Epoch: 2000\tTrain loss: 3173578.32421875\n",
      "Epoch: 2050\tTrain loss: 3150674.642578125\n",
      "Epoch: 2100\tTrain loss: 3128075.330078125\n",
      "Epoch: 2150\tTrain loss: 3106262.240234375\n",
      "Epoch: 2200\tTrain loss: 3085062.822265625\n",
      "Epoch: 2250\tTrain loss: 3064325.455078125\n",
      "Epoch: 2300\tTrain loss: 3044388.6650390625\n",
      "Epoch: 2350\tTrain loss: 3024657.21484375\n",
      "Epoch: 2400\tTrain loss: 3005744.302734375\n",
      "Epoch: 2450\tTrain loss: 2987165.1767578125\n",
      "Epoch: 2500\tTrain loss: 2969368.5810546875\n",
      "Epoch: 2550\tTrain loss: 2951801.521484375\n",
      "Epoch: 2600\tTrain loss: 2935026.005859375\n",
      "Epoch: 2650\tTrain loss: 2918610.859375\n",
      "Epoch: 2700\tTrain loss: 2902696.330078125\n",
      "Epoch: 2750\tTrain loss: 2887132.541015625\n",
      "Epoch: 2800\tTrain loss: 2872111.09375\n",
      "Epoch: 2850\tTrain loss: 2857813.1938476562\n",
      "Epoch: 2900\tTrain loss: 2843652.419921875\n",
      "Epoch: 2950\tTrain loss: 2830188.8505859375\n",
      "Epoch: 3000\tTrain loss: 2816911.6147460938\n",
      "Epoch: 3050\tTrain loss: 2804152.810546875\n",
      "Epoch: 3100\tTrain loss: 2791957.142578125\n",
      "Epoch: 3150\tTrain loss: 2779876.333984375\n",
      "Epoch: 3200\tTrain loss: 2768390.580078125\n",
      "Epoch: 3250\tTrain loss: 2757292.7734375\n",
      "Epoch: 3300\tTrain loss: 2735224.111328125\n",
      "Epoch: 3350\tTrain loss: 2719414.81640625\n",
      "Epoch: 3400\tTrain loss: 2668251.23046875\n",
      "Epoch: 3450\tTrain loss: 2642199.8662109375\n",
      "Epoch: 3500\tTrain loss: 2611891.7048339844\n",
      "Epoch: 3550\tTrain loss: 2590836.0092773438\n",
      "Epoch: 3600\tTrain loss: 2571275.2861328125\n",
      "Epoch: 3650\tTrain loss: 2552011.6345214844\n",
      "Epoch: 3700\tTrain loss: 2532893.0302124023\n",
      "Epoch: 3750\tTrain loss: 2541578.6015625\n",
      "Epoch: 3800\tTrain loss: 2495586.7983398438\n",
      "Epoch: 3850\tTrain loss: 2476382.196411133\n",
      "Epoch: 3900\tTrain loss: 2457779.0311279297\n",
      "Epoch: 3950\tTrain loss: 2439247.3071899414\n",
      "Epoch: 4000\tTrain loss: 2420873.6818847656\n",
      "Epoch: 4050\tTrain loss: 2402585.4783325195\n",
      "Epoch: 4100\tTrain loss: 2384559.524230957\n",
      "Epoch: 4150\tTrain loss: 2366489.1599121094\n",
      "Epoch: 4200\tTrain loss: 2348292.3135375977\n",
      "Epoch: 4250\tTrain loss: 2481804.114135742\n",
      "Epoch: 4300\tTrain loss: 2462468.510498047\n",
      "Epoch: 4350\tTrain loss: 2370272.972229004\n",
      "Epoch: 4400\tTrain loss: 2353203.2244873047\n",
      "Epoch: 4450\tTrain loss: 2335875.005493164\n",
      "Epoch: 4500\tTrain loss: 2318830.6826171875\n",
      "Epoch: 4550\tTrain loss: 2301624.971557617\n",
      "Epoch: 4600\tTrain loss: 2284678.919189453\n",
      "Epoch: 4650\tTrain loss: 2267989.13482666\n",
      "Epoch: 4700\tTrain loss: 2251192.5709228516\n",
      "Epoch: 4750\tTrain loss: 2234369.130004883\n",
      "Epoch: 4800\tTrain loss: 2217696.3973083496\n",
      "Epoch: 4850\tTrain loss: 2201140.1470336914\n",
      "Epoch: 4900\tTrain loss: 2184657.2553100586\n",
      "Epoch: 4950\tTrain loss: 2168269.737060547\n",
      "Epoch: 5000\tTrain loss: 2152074.1486206055\n",
      "Epoch: 5050\tTrain loss: 2135783.9552001953\n",
      "Epoch: 5100\tTrain loss: 2119413.156616211\n",
      "Epoch: 5150\tTrain loss: 2103349.6533203125\n",
      "Epoch: 5200\tTrain loss: 2087239.2493896484\n",
      "Epoch: 5250\tTrain loss: 2071379.6457519531\n",
      "Epoch: 5300\tTrain loss: 2055461.750213623\n",
      "Epoch: 5350\tTrain loss: 2039495.26953125\n",
      "Epoch: 5400\tTrain loss: 2036150.619140625\n",
      "Epoch: 5450\tTrain loss: 2008673.185180664\n",
      "Epoch: 5500\tTrain loss: 1992888.4813232422\n",
      "Epoch: 5550\tTrain loss: 1977298.547302246\n",
      "Epoch: 5600\tTrain loss: 1961798.7525024414\n",
      "Epoch: 5650\tTrain loss: 1946274.8397827148\n",
      "Epoch: 5700\tTrain loss: 1930755.8178100586\n",
      "Epoch: 5750\tTrain loss: 1915558.8876342773\n",
      "Epoch: 5800\tTrain loss: 1900143.4572143555\n",
      "Epoch: 5850\tTrain loss: 1884947.9240722656\n",
      "Epoch: 5900\tTrain loss: 1869899.9212036133\n",
      "Epoch: 5950\tTrain loss: 1854689.4094238281\n",
      "Epoch: 6000\tTrain loss: 1839851.5573120117\n",
      "Epoch: 6050\tTrain loss: 1824950.328125\n",
      "Epoch: 6100\tTrain loss: 1810041.868774414\n",
      "Epoch: 6150\tTrain loss: 1795168.5866088867\n",
      "Epoch: 6200\tTrain loss: 1780439.9728393555\n",
      "Epoch: 6250\tTrain loss: 1765755.5354309082\n",
      "Epoch: 6300\tTrain loss: 1751167.6774291992\n",
      "Epoch: 6350\tTrain loss: 1736586.0353393555\n",
      "Epoch: 6400\tTrain loss: 1721966.131286621\n",
      "Epoch: 6450\tTrain loss: 1707540.84375\n",
      "Epoch: 6500\tTrain loss: 1693165.7524414062\n",
      "Epoch: 6550\tTrain loss: 1678873.6288452148\n",
      "Epoch: 6600\tTrain loss: 1664751.437866211\n",
      "Epoch: 6650\tTrain loss: 1650468.8359375\n",
      "Epoch: 6700\tTrain loss: 1636378.6455078125\n",
      "Epoch: 6750\tTrain loss: 1622321.7139892578\n",
      "Epoch: 6800\tTrain loss: 1608394.4027404785\n",
      "Epoch: 6850\tTrain loss: 1594513.8953857422\n",
      "Epoch: 6900\tTrain loss: 1580572.1673583984\n",
      "Epoch: 6950\tTrain loss: 1566907.351989746\n",
      "Epoch: 7000\tTrain loss: 1553036.2840576172\n",
      "Epoch: 7050\tTrain loss: 1539360.3350219727\n",
      "Epoch: 7100\tTrain loss: 1525769.9096069336\n",
      "Epoch: 7150\tTrain loss: 1512624.0610351562\n",
      "Epoch: 7200\tTrain loss: 1498767.4137878418\n",
      "Epoch: 7250\tTrain loss: 1485369.8671264648\n",
      "Epoch: 7300\tTrain loss: 1472127.0701293945\n",
      "Epoch: 7350\tTrain loss: 1458711.1943359375\n",
      "Epoch: 7400\tTrain loss: 1445625.8645629883\n",
      "Epoch: 7450\tTrain loss: 1432476.897644043\n",
      "Epoch: 7500\tTrain loss: 1419285.0846252441\n",
      "Epoch: 7550\tTrain loss: 1406229.7872924805\n",
      "Epoch: 7600\tTrain loss: 1393399.8026123047\n",
      "Epoch: 7650\tTrain loss: 1380445.6006469727\n",
      "Epoch: 7700\tTrain loss: 1367694.4340820312\n",
      "Epoch: 7750\tTrain loss: 1354766.6391601562\n",
      "Epoch: 7800\tTrain loss: 1342045.1336669922\n",
      "Epoch: 7850\tTrain loss: 1329752.5830078125\n",
      "Epoch: 7900\tTrain loss: 1316961.2426757812\n",
      "Epoch: 7950\tTrain loss: 1304286.8249511719\n",
      "Epoch: 8000\tTrain loss: 1291812.2863769531\n",
      "Epoch: 8050\tTrain loss: 1279442.8103027344\n",
      "Epoch: 8100\tTrain loss: 1267219.9737548828\n",
      "Epoch: 8150\tTrain loss: 1254855.9446411133\n",
      "Epoch: 8200\tTrain loss: 1242632.4433898926\n",
      "Epoch: 8250\tTrain loss: 1230529.6860656738\n",
      "Epoch: 8300\tTrain loss: 1218413.4402770996\n",
      "Epoch: 8350\tTrain loss: 1152751.5010986328\n",
      "Epoch: 8400\tTrain loss: 1139453.4802856445\n",
      "Epoch: 8450\tTrain loss: 1126831.4667358398\n",
      "Epoch: 8500\tTrain loss: 1114329.968322754\n",
      "Epoch: 8550\tTrain loss: 1102143.982849121\n",
      "Epoch: 8600\tTrain loss: 1090021.7547607422\n",
      "Epoch: 8650\tTrain loss: 1077990.1190490723\n",
      "Epoch: 8700\tTrain loss: 1066069.342590332\n",
      "Epoch: 8750\tTrain loss: 1054149.9801635742\n",
      "Epoch: 8800\tTrain loss: 1042508.1087036133\n",
      "Epoch: 8850\tTrain loss: 1030868.911895752\n",
      "Epoch: 8900\tTrain loss: 1019432.2752685547\n",
      "Epoch: 8950\tTrain loss: 1007826.5308837891\n",
      "Epoch: 9000\tTrain loss: 996428.4127807617\n",
      "Epoch: 9050\tTrain loss: 985106.5020141602\n",
      "Epoch: 9100\tTrain loss: 973704.4979858398\n",
      "Epoch: 9150\tTrain loss: 962546.1278686523\n",
      "Epoch: 9200\tTrain loss: 951420.9626464844\n",
      "Epoch: 9250\tTrain loss: 940501.6657104492\n",
      "Epoch: 9300\tTrain loss: 929453.9915161133\n",
      "Epoch: 9350\tTrain loss: 918629.1556396484\n",
      "Epoch: 9400\tTrain loss: 907714.1823120117\n",
      "Epoch: 9450\tTrain loss: 896954.2830200195\n",
      "Epoch: 9500\tTrain loss: 886259.7673034668\n",
      "Epoch: 9550\tTrain loss: 875747.8329772949\n",
      "Epoch: 9600\tTrain loss: 865092.3280639648\n",
      "Epoch: 9650\tTrain loss: 854696.9688720703\n",
      "Epoch: 9700\tTrain loss: 844298.4802856445\n",
      "Epoch: 9750\tTrain loss: 833867.0723571777\n",
      "Epoch: 9800\tTrain loss: 823579.1571044922\n",
      "Epoch: 9850\tTrain loss: 813386.7626953125\n",
      "Epoch: 9900\tTrain loss: 803354.2639160156\n",
      "Epoch: 9950\tTrain loss: 793272.0762023926\n",
      "Epoch: 10000\tTrain loss: 783173.2648925781\n",
      "Epoch: 10050\tTrain loss: 773256.3327026367\n",
      "Epoch: 10100\tTrain loss: 763402.2510986328\n",
      "Epoch: 10150\tTrain loss: 753691.0789794922\n",
      "Epoch: 10200\tTrain loss: 743958.7575683594\n",
      "Epoch: 10250\tTrain loss: 734227.1448974609\n",
      "Epoch: 10300\tTrain loss: 724637.385925293\n",
      "Epoch: 10350\tTrain loss: 715188.7899780273\n",
      "Epoch: 10400\tTrain loss: 705660.582824707\n",
      "Epoch: 10450\tTrain loss: 696278.5156555176\n",
      "Epoch: 10500\tTrain loss: 687039.2108764648\n",
      "Epoch: 10550\tTrain loss: 677703.5285644531\n",
      "Epoch: 10600\tTrain loss: 668577.1414794922\n",
      "Epoch: 10650\tTrain loss: 659510.62890625\n",
      "Epoch: 10700\tTrain loss: 650480.6784973145\n",
      "Epoch: 10750\tTrain loss: 641406.5198364258\n",
      "Epoch: 10800\tTrain loss: 632478.3432006836\n",
      "Epoch: 10850\tTrain loss: 623654.0379638672\n",
      "Epoch: 10900\tTrain loss: 614973.4309997559\n",
      "Epoch: 10950\tTrain loss: 606259.608215332\n",
      "Epoch: 11000\tTrain loss: 597620.9944152832\n",
      "Epoch: 11050\tTrain loss: 589070.0212402344\n",
      "Epoch: 11100\tTrain loss: 580541.0595092773\n",
      "Epoch: 11150\tTrain loss: 572058.2162780762\n",
      "Epoch: 11200\tTrain loss: 563696.8273925781\n",
      "Epoch: 11250\tTrain loss: 555460.3516235352\n",
      "Epoch: 11300\tTrain loss: 547228.7899780273\n",
      "Epoch: 11350\tTrain loss: 539051.3247680664\n",
      "Epoch: 11400\tTrain loss: 530884.6473388672\n",
      "Epoch: 11450\tTrain loss: 522874.70794677734\n",
      "Epoch: 11500\tTrain loss: 514964.8269042969\n",
      "Epoch: 11550\tTrain loss: 507090.58825683594\n",
      "Epoch: 11600\tTrain loss: 499243.0805053711\n",
      "Epoch: 11650\tTrain loss: 491407.9069519043\n",
      "Epoch: 11700\tTrain loss: 483723.8172607422\n",
      "Epoch: 11750\tTrain loss: 476159.7448425293\n",
      "Epoch: 11800\tTrain loss: 468519.0914001465\n",
      "Epoch: 11850\tTrain loss: 461101.81719970703\n",
      "Epoch: 11900\tTrain loss: 453710.9931945801\n",
      "Epoch: 11950\tTrain loss: 446327.22494506836\n",
      "Epoch: 12000\tTrain loss: 439021.4385986328\n",
      "Epoch: 12050\tTrain loss: 431797.9495239258\n",
      "Epoch: 12100\tTrain loss: 424642.5112915039\n",
      "Epoch: 12150\tTrain loss: 417579.25704956055\n",
      "Epoch: 12200\tTrain loss: 410525.13525390625\n",
      "Epoch: 12250\tTrain loss: 403577.1784667969\n",
      "Epoch: 12300\tTrain loss: 396749.0278625488\n",
      "Epoch: 12350\tTrain loss: 577000.2950439453\n",
      "Epoch: 12400\tTrain loss: 566763.140411377\n",
      "Epoch: 12450\tTrain loss: 557058.6516723633\n",
      "Epoch: 12500\tTrain loss: 547719.519317627\n",
      "Epoch: 12550\tTrain loss: 538486.0400695801\n",
      "Epoch: 12600\tTrain loss: 529392.3170166016\n",
      "Epoch: 12650\tTrain loss: 520619.39208984375\n",
      "Epoch: 12700\tTrain loss: 511882.12091064453\n",
      "Epoch: 12750\tTrain loss: 503403.0101928711\n",
      "Epoch: 12800\tTrain loss: 495021.4772949219\n",
      "Epoch: 12850\tTrain loss: 486796.8116455078\n",
      "Epoch: 12900\tTrain loss: 478696.8207092285\n",
      "Epoch: 12950\tTrain loss: 470618.74298095703\n",
      "Epoch: 13000\tTrain loss: 462751.71240234375\n",
      "Epoch: 13050\tTrain loss: 454939.8381652832\n",
      "Epoch: 13100\tTrain loss: 447293.73345947266\n",
      "Epoch: 13150\tTrain loss: 653381.4023132324\n",
      "Epoch: 13200\tTrain loss: 642365.9791564941\n",
      "Epoch: 13250\tTrain loss: 536478.6921386719\n",
      "Epoch: 13300\tTrain loss: 527905.4465332031\n",
      "Epoch: 13350\tTrain loss: 519592.04357910156\n",
      "Epoch: 13400\tTrain loss: 511338.3720703125\n",
      "Epoch: 13450\tTrain loss: 503304.6361694336\n",
      "Epoch: 13500\tTrain loss: 495415.3450012207\n",
      "Epoch: 13550\tTrain loss: 487638.56158447266\n",
      "Epoch: 13600\tTrain loss: 479913.7232055664\n",
      "Epoch: 13650\tTrain loss: 472313.8822631836\n",
      "Epoch: 13700\tTrain loss: 464760.71127319336\n",
      "Epoch: 13750\tTrain loss: 457392.0135498047\n",
      "Epoch: 13800\tTrain loss: 450087.46884155273\n",
      "Epoch: 13850\tTrain loss: 442805.27423095703\n",
      "Epoch: 13900\tTrain loss: 435653.7563781738\n",
      "Epoch: 13950\tTrain loss: 428639.45764160156\n",
      "Epoch: 14000\tTrain loss: 421625.2314758301\n",
      "Epoch: 14050\tTrain loss: 414777.5771789551\n",
      "Epoch: 14100\tTrain loss: 407960.92822265625\n",
      "Epoch: 14150\tTrain loss: 401143.6852722168\n",
      "Epoch: 14200\tTrain loss: 394524.1032104492\n",
      "Epoch: 14250\tTrain loss: 387923.6574707031\n",
      "Epoch: 14300\tTrain loss: 381388.13929748535\n",
      "Epoch: 14350\tTrain loss: 374868.45697021484\n",
      "Epoch: 14400\tTrain loss: 368528.6008605957\n",
      "Epoch: 14450\tTrain loss: 362149.65606689453\n",
      "Epoch: 14500\tTrain loss: 355878.1514892578\n",
      "Epoch: 14550\tTrain loss: 349698.1653137207\n",
      "Epoch: 14600\tTrain loss: 343579.59814453125\n",
      "Epoch: 14650\tTrain loss: 337493.1810913086\n",
      "Epoch: 14700\tTrain loss: 331517.19470214844\n",
      "Epoch: 14750\tTrain loss: 325585.5729370117\n",
      "Epoch: 14800\tTrain loss: 319758.58419799805\n",
      "Epoch: 14850\tTrain loss: 313963.51068115234\n",
      "Epoch: 14900\tTrain loss: 308148.4465942383\n",
      "Epoch: 14950\tTrain loss: 302519.2971496582\n",
      "Epoch: 15000\tTrain loss: 296863.6909790039\n",
      "Epoch: 15050\tTrain loss: 291339.34744262695\n",
      "Epoch: 15100\tTrain loss: 285804.06134033203\n",
      "Epoch: 15150\tTrain loss: 280365.2440185547\n",
      "Epoch: 15200\tTrain loss: 274994.4291687012\n",
      "Epoch: 15250\tTrain loss: 269665.25985717773\n",
      "Epoch: 15300\tTrain loss: 264458.65420532227\n",
      "Epoch: 15350\tTrain loss: 259274.5317993164\n",
      "Epoch: 15400\tTrain loss: 254131.42379760742\n",
      "Epoch: 15450\tTrain loss: 249043.6187133789\n",
      "Epoch: 15500\tTrain loss: 244045.81225585938\n",
      "Epoch: 15550\tTrain loss: 239085.6565246582\n",
      "Epoch: 15600\tTrain loss: 234138.3585510254\n",
      "Epoch: 15650\tTrain loss: 229308.50579833984\n",
      "Epoch: 15700\tTrain loss: 224567.97521972656\n",
      "Epoch: 15750\tTrain loss: 219842.49853515625\n",
      "Epoch: 15800\tTrain loss: 215155.66845703125\n",
      "Epoch: 15850\tTrain loss: 210549.5366821289\n",
      "Epoch: 15900\tTrain loss: 206035.88845825195\n",
      "Epoch: 15950\tTrain loss: 201551.9968109131\n",
      "Epoch: 16000\tTrain loss: 197089.73529052734\n",
      "Epoch: 16050\tTrain loss: 192745.0041656494\n",
      "Epoch: 16100\tTrain loss: 188427.8168334961\n",
      "Epoch: 16150\tTrain loss: 184138.60845947266\n",
      "Epoch: 16200\tTrain loss: 179946.61840820312\n",
      "Epoch: 16250\tTrain loss: 175838.56219482422\n",
      "Epoch: 16300\tTrain loss: 171822.0270690918\n",
      "Epoch: 16350\tTrain loss: 167687.6137084961\n",
      "Epoch: 16400\tTrain loss: 163711.19818115234\n",
      "Epoch: 16450\tTrain loss: 159814.4870300293\n",
      "Epoch: 16500\tTrain loss: 155985.3899230957\n",
      "Epoch: 16550\tTrain loss: 152147.00302124023\n",
      "Epoch: 16600\tTrain loss: 148443.97326660156\n",
      "Epoch: 16650\tTrain loss: 144750.01553344727\n",
      "Epoch: 16700\tTrain loss: 141135.2215576172\n",
      "Epoch: 16750\tTrain loss: 137519.2202758789\n",
      "Epoch: 16800\tTrain loss: 134030.20016479492\n",
      "Epoch: 16850\tTrain loss: 130528.8374633789\n",
      "Epoch: 16900\tTrain loss: 127150.39797973633\n",
      "Epoch: 16950\tTrain loss: 123794.19973754883\n",
      "Epoch: 17000\tTrain loss: 120461.00537109375\n",
      "Epoch: 17050\tTrain loss: 117242.95104980469\n",
      "Epoch: 17100\tTrain loss: 114058.55061340332\n",
      "Epoch: 17150\tTrain loss: 110903.25772094727\n",
      "Epoch: 17200\tTrain loss: 107840.70538330078\n",
      "Epoch: 17250\tTrain loss: 104786.03173828125\n",
      "Epoch: 17300\tTrain loss: 101817.13693237305\n",
      "Epoch: 17350\tTrain loss: 98920.44828796387\n",
      "Epoch: 17400\tTrain loss: 96059.34371948242\n",
      "Epoch: 17450\tTrain loss: 93241.50830078125\n",
      "Epoch: 17500\tTrain loss: 90484.01483154297\n",
      "Epoch: 17550\tTrain loss: 87785.53436279297\n",
      "Epoch: 17600\tTrain loss: 85104.8366394043\n",
      "Epoch: 17650\tTrain loss: 82512.32403564453\n",
      "Epoch: 17700\tTrain loss: 79947.47763061523\n",
      "Epoch: 17750\tTrain loss: 77458.0473022461\n",
      "Epoch: 17800\tTrain loss: 75685.48229980469\n",
      "Epoch: 17850\tTrain loss: 72621.19967651367\n",
      "Epoch: 17900\tTrain loss: 70250.18832397461\n",
      "Epoch: 17950\tTrain loss: 67981.21395874023\n",
      "Epoch: 18000\tTrain loss: 65717.86953735352\n",
      "Epoch: 18050\tTrain loss: 63521.727783203125\n",
      "Epoch: 18100\tTrain loss: 61375.69534301758\n",
      "Epoch: 18150\tTrain loss: 59283.19509887695\n",
      "Epoch: 18200\tTrain loss: 57230.94934082031\n",
      "Epoch: 18250\tTrain loss: 55249.81298828125\n",
      "Epoch: 18300\tTrain loss: 53301.04473876953\n",
      "Epoch: 18350\tTrain loss: 51378.93832397461\n",
      "Epoch: 18400\tTrain loss: 49537.442626953125\n",
      "Epoch: 18450\tTrain loss: 47711.59631347656\n",
      "Epoch: 18500\tTrain loss: 45952.70095825195\n",
      "Epoch: 18550\tTrain loss: 44233.44677734375\n",
      "Epoch: 18600\tTrain loss: 42575.10647583008\n",
      "Epoch: 18650\tTrain loss: 40937.00079345703\n",
      "Epoch: 18700\tTrain loss: 39355.01887512207\n",
      "Epoch: 18750\tTrain loss: 37813.991470336914\n",
      "Epoch: 18800\tTrain loss: 36357.73141479492\n",
      "Epoch: 18850\tTrain loss: 34870.56784057617\n",
      "Epoch: 18900\tTrain loss: 33462.14743041992\n",
      "Epoch: 18950\tTrain loss: 32097.821563720703\n",
      "Epoch: 19000\tTrain loss: 30766.824951171875\n",
      "Epoch: 19050\tTrain loss: 29491.754272460938\n",
      "Epoch: 19100\tTrain loss: 28266.21646118164\n",
      "Epoch: 19150\tTrain loss: 27039.042572021484\n",
      "Epoch: 19200\tTrain loss: 25864.772872924805\n",
      "Epoch: 19250\tTrain loss: 24727.749389648438\n",
      "Epoch: 19300\tTrain loss: 23633.557250976562\n",
      "Epoch: 19350\tTrain loss: 22572.067352294922\n",
      "Epoch: 19400\tTrain loss: 21542.523345947266\n",
      "Epoch: 19450\tTrain loss: 20555.774047851562\n",
      "Epoch: 19500\tTrain loss: 19599.206604003906\n",
      "Epoch: 19550\tTrain loss: 18805.381561279297\n",
      "Epoch: 19600\tTrain loss: 17864.471740722656\n",
      "Epoch: 19650\tTrain loss: 17009.28091430664\n",
      "Epoch: 19700\tTrain loss: 16183.629760742188\n",
      "Epoch: 19750\tTrain loss: 15401.78286743164\n",
      "Epoch: 19800\tTrain loss: 14634.882446289062\n",
      "Epoch: 19850\tTrain loss: 13909.010833740234\n",
      "Epoch: 19900\tTrain loss: 13209.00991821289\n",
      "Epoch: 19950\tTrain loss: 12542.510162353516\n",
      "Epoch: 20000\tTrain loss: 11894.247985839844\n",
      "Epoch: 20050\tTrain loss: 11274.362854003906\n",
      "Epoch: 20100\tTrain loss: 10689.453231811523\n",
      "Epoch: 20150\tTrain loss: 10115.187316894531\n",
      "Epoch: 20200\tTrain loss: 9577.109985351562\n",
      "Epoch: 20250\tTrain loss: 9060.512756347656\n",
      "Epoch: 20300\tTrain loss: 8566.030090332031\n",
      "Epoch: 20350\tTrain loss: 8100.278656005859\n",
      "Epoch: 20400\tTrain loss: 7649.06071472168\n",
      "Epoch: 20450\tTrain loss: 7226.809844970703\n",
      "Epoch: 20500\tTrain loss: 6824.560897827148\n",
      "Epoch: 20550\tTrain loss: 6436.086700439453\n",
      "Epoch: 20600\tTrain loss: 6075.407165527344\n",
      "Epoch: 20650\tTrain loss: 5729.4140625\n",
      "Epoch: 20700\tTrain loss: 5411.60090637207\n",
      "Epoch: 20750\tTrain loss: 5088.610336303711\n",
      "Epoch: 20800\tTrain loss: 4793.529113769531\n",
      "Epoch: 20850\tTrain loss: 4517.896331787109\n",
      "Epoch: 20900\tTrain loss: 4251.325912475586\n",
      "Epoch: 20950\tTrain loss: 4002.9295043945312\n",
      "Epoch: 21000\tTrain loss: 3766.7632751464844\n",
      "Epoch: 21050\tTrain loss: 3545.408477783203\n",
      "Epoch: 21100\tTrain loss: 3343.4526977539062\n",
      "Epoch: 21150\tTrain loss: 3118.3053283691406\n",
      "Epoch: 21200\tTrain loss: 61127.234924316406\n",
      "Epoch: 21250\tTrain loss: 53849.22412109375\n",
      "Epoch: 21300\tTrain loss: 48675.217193603516\n",
      "Epoch: 21350\tTrain loss: 44673.191986083984\n",
      "Epoch: 21400\tTrain loss: 41317.81005859375\n",
      "Epoch: 21450\tTrain loss: 38268.237716674805\n",
      "Epoch: 21500\tTrain loss: 35658.23434448242\n",
      "Epoch: 21550\tTrain loss: 33265.47717285156\n",
      "Epoch: 21600\tTrain loss: 31087.08660888672\n",
      "Epoch: 21650\tTrain loss: 29068.947204589844\n",
      "Epoch: 21700\tTrain loss: 27233.855422973633\n",
      "Epoch: 21750\tTrain loss: 25523.338973999023\n",
      "Epoch: 21800\tTrain loss: 23905.372436523438\n",
      "Epoch: 21850\tTrain loss: 22426.169219970703\n",
      "Epoch: 21900\tTrain loss: 21014.014602661133\n",
      "Epoch: 21950\tTrain loss: 19712.819122314453\n",
      "Epoch: 22000\tTrain loss: 18467.559524536133\n",
      "Epoch: 22050\tTrain loss: 17315.387481689453\n",
      "Epoch: 22100\tTrain loss: 16221.726669311523\n",
      "Epoch: 22150\tTrain loss: 15184.834350585938\n",
      "Epoch: 22200\tTrain loss: 14216.03385925293\n",
      "Epoch: 22250\tTrain loss: 13305.061279296875\n",
      "Epoch: 22300\tTrain loss: 12448.383239746094\n",
      "Epoch: 22350\tTrain loss: 11634.800918579102\n",
      "Epoch: 22400\tTrain loss: 10872.07893371582\n",
      "Epoch: 22450\tTrain loss: 10153.473022460938\n",
      "Epoch: 22500\tTrain loss: 9470.974472045898\n",
      "Epoch: 22550\tTrain loss: 8868.678756713867\n",
      "Epoch: 22600\tTrain loss: 8272.40104675293\n",
      "Epoch: 22650\tTrain loss: 7692.147323608398\n",
      "Epoch: 22700\tTrain loss: 7169.284118652344\n",
      "Epoch: 22750\tTrain loss: 6728.255157470703\n",
      "Epoch: 22800\tTrain loss: 6259.60383605957\n",
      "Epoch: 22850\tTrain loss: 5828.085433959961\n",
      "Epoch: 22900\tTrain loss: 5421.6478271484375\n",
      "Epoch: 22950\tTrain loss: 5033.685592651367\n",
      "Epoch: 23000\tTrain loss: 4767.191650390625\n",
      "Epoch: 23050\tTrain loss: 4367.453262329102\n",
      "Epoch: 23100\tTrain loss: 4058.655258178711\n",
      "Epoch: 23150\tTrain loss: 4253.892822265625\n",
      "Epoch: 23200\tTrain loss: 3735.3717041015625\n",
      "Epoch: 23250\tTrain loss: 3468.8379821777344\n",
      "Epoch: 23300\tTrain loss: 3346.949951171875\n",
      "Epoch: 23350\tTrain loss: 3052.657684326172\n",
      "Epoch: 23400\tTrain loss: 2839.3001708984375\n",
      "Epoch: 23450\tTrain loss: 2645.5706787109375\n",
      "Epoch: 23500\tTrain loss: 2467.5867919921875\n",
      "Epoch: 23550\tTrain loss: 2304.7099609375\n",
      "Epoch: 23600\tTrain loss: 2152.9876556396484\n",
      "Epoch: 23650\tTrain loss: 2013.0811157226562\n",
      "Epoch: 23700\tTrain loss: 1872.883804321289\n",
      "Epoch: 23750\tTrain loss: 1826.0463562011719\n",
      "Epoch: 23800\tTrain loss: 1712.6044921875\n",
      "Epoch: 23850\tTrain loss: 1609.6607666015625\n",
      "Epoch: 23900\tTrain loss: 1494.986083984375\n",
      "Epoch: 23950\tTrain loss: 1420.73876953125\n",
      "Epoch: 24000\tTrain loss: 1324.9298706054688\n",
      "Epoch: 24050\tTrain loss: 1248.7202453613281\n",
      "Epoch: 24100\tTrain loss: 1183.6856384277344\n",
      "Epoch: 24150\tTrain loss: 1117.0582885742188\n",
      "Epoch: 24200\tTrain loss: 1043.8766479492188\n",
      "Epoch: 24250\tTrain loss: 1084.8068237304688\n",
      "Epoch: 24300\tTrain loss: 926.1194458007812\n",
      "Epoch: 24350\tTrain loss: 863.2740020751953\n",
      "Epoch: 24400\tTrain loss: 2235.04248046875\n",
      "Epoch: 24450\tTrain loss: 19768.63037109375\n",
      "Epoch: 24500\tTrain loss: 2487.072998046875\n",
      "Epoch: 24550\tTrain loss: 1741.2117919921875\n",
      "Epoch: 24600\tTrain loss: 1485.9888000488281\n",
      "Epoch: 24650\tTrain loss: 1350.7537841796875\n",
      "Epoch: 24700\tTrain loss: 1222.7308654785156\n",
      "Epoch: 24750\tTrain loss: 1104.1356811523438\n",
      "Epoch: 24800\tTrain loss: 1008.6146850585938\n",
      "Epoch: 24850\tTrain loss: 933.7022094726562\n",
      "Epoch: 24900\tTrain loss: 887.9623718261719\n",
      "Epoch: 24950\tTrain loss: 819.184326171875\n",
      "Epoch: 25000\tTrain loss: 770.4892272949219\n",
      "Epoch: 25050\tTrain loss: 731.7542724609375\n",
      "Epoch: 25100\tTrain loss: 700.6058349609375\n",
      "Epoch: 25150\tTrain loss: 673.6931457519531\n",
      "Epoch: 25200\tTrain loss: 638.4953765869141\n",
      "Epoch: 25250\tTrain loss: 609.2147064208984\n",
      "Epoch: 25300\tTrain loss: 581.1654815673828\n",
      "Epoch: 25350\tTrain loss: 559.4503173828125\n",
      "Epoch: 25400\tTrain loss: 540.5768737792969\n",
      "Epoch: 25450\tTrain loss: 520.0652618408203\n",
      "Epoch: 25500\tTrain loss: 565.9914093017578\n",
      "Epoch: 25550\tTrain loss: 536.2792053222656\n",
      "Epoch: 25600\tTrain loss: 497.9629211425781\n",
      "Epoch: 25650\tTrain loss: 479.2794189453125\n",
      "Epoch: 25700\tTrain loss: 527.1934967041016\n",
      "Epoch: 25750\tTrain loss: 504.5421905517578\n",
      "Epoch: 25800\tTrain loss: 474.6379089355469\n",
      "Epoch: 25850\tTrain loss: 460.04107666015625\n",
      "Epoch: 25900\tTrain loss: 548.9210662841797\n",
      "Epoch: 25950\tTrain loss: 518.29052734375\n",
      "Epoch: 26000\tTrain loss: 510.03997802734375\n",
      "Epoch: 26050\tTrain loss: 504.05308532714844\n",
      "Epoch: 26100\tTrain loss: 498.6466827392578\n",
      "Epoch: 26150\tTrain loss: 493.7201232910156\n",
      "Epoch: 26200\tTrain loss: 488.7181701660156\n",
      "Epoch: 26250\tTrain loss: 483.9324035644531\n",
      "Epoch: 26300\tTrain loss: 478.1238250732422\n",
      "Epoch: 26350\tTrain loss: 471.0780487060547\n",
      "Epoch: 26400\tTrain loss: 454.6495056152344\n",
      "Epoch: 26450\tTrain loss: 488.25091552734375\n",
      "Epoch: 26500\tTrain loss: 420.0374755859375\n",
      "Epoch: 26550\tTrain loss: 395.35980224609375\n",
      "Epoch: 26600\tTrain loss: 397.7165069580078\n",
      "Epoch: 26650\tTrain loss: 359.6703109741211\n",
      "Epoch: 26700\tTrain loss: 351.64904022216797\n",
      "Epoch: 26750\tTrain loss: 339.95660400390625\n",
      "Epoch: 26800\tTrain loss: 357.14794158935547\n",
      "Epoch: 26850\tTrain loss: 319.37579345703125\n",
      "Epoch: 26900\tTrain loss: 314.14720916748047\n",
      "Epoch: 26950\tTrain loss: 307.4840621948242\n",
      "Epoch: 27000\tTrain loss: 332.7306823730469\n",
      "Epoch: 27050\tTrain loss: 298.02881622314453\n",
      "Epoch: 27100\tTrain loss: 294.0420150756836\n",
      "Epoch: 27150\tTrain loss: 289.6006774902344\n",
      "Epoch: 27200\tTrain loss: 286.09320068359375\n",
      "Epoch: 27250\tTrain loss: 282.4168014526367\n",
      "Epoch: 27300\tTrain loss: 1398.8809509277344\n",
      "Epoch: 27350\tTrain loss: 531.2587432861328\n",
      "Epoch: 27400\tTrain loss: 326.01416015625\n",
      "Epoch: 27450\tTrain loss: 319.4006576538086\n",
      "Epoch: 27500\tTrain loss: 315.04774475097656\n",
      "Epoch: 27550\tTrain loss: 311.4423370361328\n",
      "Epoch: 27600\tTrain loss: 308.24981689453125\n",
      "Epoch: 27650\tTrain loss: 305.2748489379883\n",
      "Epoch: 27700\tTrain loss: 302.4628677368164\n",
      "Epoch: 27750\tTrain loss: 299.70336151123047\n",
      "Epoch: 27800\tTrain loss: 297.02464294433594\n",
      "Epoch: 27850\tTrain loss: 294.25004959106445\n",
      "Epoch: 27900\tTrain loss: 291.3372573852539\n",
      "Epoch: 27950\tTrain loss: 287.9693298339844\n",
      "Epoch: 28000\tTrain loss: 281.8184051513672\n",
      "Epoch: 28050\tTrain loss: 283.3944282531738\n",
      "Epoch: 28100\tTrain loss: 313.80236053466797\n",
      "Epoch: 28150\tTrain loss: 255.0682144165039\n",
      "Epoch: 28200\tTrain loss: 249.28621673583984\n",
      "Epoch: 28250\tTrain loss: 245.28453063964844\n",
      "Epoch: 28300\tTrain loss: 241.75332641601562\n",
      "Epoch: 28350\tTrain loss: 238.4649658203125\n",
      "Epoch: 28400\tTrain loss: 235.49665641784668\n",
      "Epoch: 28450\tTrain loss: 232.41095733642578\n",
      "Epoch: 28500\tTrain loss: 289.5754165649414\n",
      "Epoch: 28550\tTrain loss: 227.5943832397461\n",
      "Epoch: 28600\tTrain loss: 224.77509307861328\n",
      "Epoch: 28650\tTrain loss: 222.11362266540527\n",
      "Epoch: 28700\tTrain loss: 219.51020050048828\n",
      "Epoch: 28750\tTrain loss: 216.82889366149902\n",
      "Epoch: 28800\tTrain loss: 214.02477645874023\n",
      "Epoch: 28850\tTrain loss: 1635.9474487304688\n",
      "Epoch: 28900\tTrain loss: 774.5932159423828\n",
      "Epoch: 28950\tTrain loss: 644.9404144287109\n",
      "Epoch: 29000\tTrain loss: 607.5098724365234\n",
      "Epoch: 29050\tTrain loss: 569.6343383789062\n",
      "Epoch: 29100\tTrain loss: 544.8623657226562\n",
      "Epoch: 29150\tTrain loss: 532.1336364746094\n",
      "Epoch: 29200\tTrain loss: 519.4153137207031\n",
      "Epoch: 29250\tTrain loss: 503.01275634765625\n",
      "Epoch: 29300\tTrain loss: 555.6868286132812\n",
      "Epoch: 29350\tTrain loss: 502.71475982666016\n",
      "Epoch: 29400\tTrain loss: 505.5020446777344\n",
      "Epoch: 29450\tTrain loss: 465.17657470703125\n",
      "Epoch: 29500\tTrain loss: 456.0837860107422\n",
      "Epoch: 29550\tTrain loss: 444.52777099609375\n",
      "Epoch: 29600\tTrain loss: 429.89805603027344\n",
      "Epoch: 29650\tTrain loss: 435.5574188232422\n",
      "Epoch: 29700\tTrain loss: 416.39034271240234\n",
      "Epoch: 29750\tTrain loss: 415.7901916503906\n",
      "Epoch: 29800\tTrain loss: 850.7061462402344\n",
      "Epoch: 29850\tTrain loss: 544.1260986328125\n",
      "Epoch: 29900\tTrain loss: 454.0057678222656\n",
      "Epoch: 29950\tTrain loss: 442.71954345703125\n",
      "Epoch: 30000\tTrain loss: 435.03562927246094\n",
      "Epoch: 30050\tTrain loss: 428.53101348876953\n",
      "Epoch: 30100\tTrain loss: 421.22352600097656\n",
      "Epoch: 30150\tTrain loss: 414.28575134277344\n",
      "Epoch: 30200\tTrain loss: 409.7041473388672\n",
      "Epoch: 30250\tTrain loss: 406.4690704345703\n",
      "Epoch: 30300\tTrain loss: 403.62235260009766\n",
      "Epoch: 30350\tTrain loss: 400.9917678833008\n",
      "Epoch: 30400\tTrain loss: 618.0370788574219\n",
      "Epoch: 30450\tTrain loss: 409.55738830566406\n",
      "Epoch: 30500\tTrain loss: 404.0531921386719\n",
      "Epoch: 30550\tTrain loss: 400.75416564941406\n",
      "Epoch: 30600\tTrain loss: 397.8621063232422\n",
      "Epoch: 30650\tTrain loss: 396.78526306152344\n",
      "Epoch: 30700\tTrain loss: 394.3480987548828\n",
      "Epoch: 30750\tTrain loss: 393.8364486694336\n",
      "Epoch: 30800\tTrain loss: 388.94395446777344\n",
      "Epoch: 30850\tTrain loss: 435.7224578857422\n",
      "Epoch: 30900\tTrain loss: 469.14854431152344\n",
      "Epoch: 30950\tTrain loss: 398.44700622558594\n",
      "Epoch: 31000\tTrain loss: 393.84629821777344\n",
      "Epoch: 31050\tTrain loss: 390.02282333374023\n",
      "Epoch: 31100\tTrain loss: 385.806640625\n",
      "Epoch: 31150\tTrain loss: 378.5386505126953\n",
      "Epoch: 31200\tTrain loss: 367.2548522949219\n",
      "Epoch: 31250\tTrain loss: 381.46849060058594\n",
      "Epoch: 31300\tTrain loss: 352.7412414550781\n",
      "Epoch: 31350\tTrain loss: 340.5726776123047\n",
      "Epoch: 31400\tTrain loss: 485.751953125\n",
      "Epoch: 31450\tTrain loss: 328.6118469238281\n",
      "Epoch: 31500\tTrain loss: 321.32923889160156\n",
      "Epoch: 31550\tTrain loss: 345.51548767089844\n",
      "Epoch: 31600\tTrain loss: 326.34571838378906\n",
      "Epoch: 31650\tTrain loss: 319.35939025878906\n",
      "Epoch: 31700\tTrain loss: 313.67506408691406\n",
      "Epoch: 31750\tTrain loss: 307.11004638671875\n",
      "Epoch: 31800\tTrain loss: 300.01222229003906\n",
      "Epoch: 31850\tTrain loss: 139761.654296875\n",
      "Epoch: 31900\tTrain loss: 6033.03857421875\n",
      "Epoch: 31950\tTrain loss: 2028.95068359375\n",
      "Epoch: 32000\tTrain loss: 1645.0645751953125\n",
      "Epoch: 32050\tTrain loss: 1411.03662109375\n",
      "Epoch: 32100\tTrain loss: 1233.7484741210938\n",
      "Epoch: 32150\tTrain loss: 1097.3392028808594\n",
      "Epoch: 32200\tTrain loss: 987.3758850097656\n",
      "Epoch: 32250\tTrain loss: 894.3380737304688\n",
      "Epoch: 32300\tTrain loss: 822.3241271972656\n",
      "Epoch: 32350\tTrain loss: 764.3815765380859\n",
      "Epoch: 32400\tTrain loss: 716.6798095703125\n",
      "Epoch: 32450\tTrain loss: 678.9255676269531\n",
      "Epoch: 32500\tTrain loss: 645.4862060546875\n",
      "Epoch: 32550\tTrain loss: 616.4501953125\n",
      "Epoch: 32600\tTrain loss: 593.2702941894531\n",
      "Epoch: 32650\tTrain loss: 574.0640411376953\n",
      "Epoch: 32700\tTrain loss: 558.0669555664062\n",
      "Epoch: 32750\tTrain loss: 543.9505615234375\n",
      "Epoch: 32800\tTrain loss: 532.1517791748047\n",
      "Epoch: 32850\tTrain loss: 522.2902069091797\n",
      "Epoch: 32900\tTrain loss: 514.2196197509766\n",
      "Epoch: 32950\tTrain loss: 507.43902587890625\n",
      "Epoch: 33000\tTrain loss: 501.8246612548828\n",
      "Epoch: 33050\tTrain loss: 497.08778381347656\n",
      "Epoch: 33100\tTrain loss: 493.063720703125\n",
      "Epoch: 33150\tTrain loss: 489.5434875488281\n",
      "Epoch: 33200\tTrain loss: 486.46217346191406\n",
      "Epoch: 33250\tTrain loss: 483.6993865966797\n",
      "Epoch: 33300\tTrain loss: 481.15895080566406\n",
      "Epoch: 33350\tTrain loss: 478.77947998046875\n",
      "Epoch: 33400\tTrain loss: 476.42633056640625\n",
      "Epoch: 33450\tTrain loss: 474.1874084472656\n",
      "Epoch: 33500\tTrain loss: 471.96812438964844\n",
      "Epoch: 33550\tTrain loss: 469.7453155517578\n",
      "Epoch: 33600\tTrain loss: 467.51318359375\n",
      "Epoch: 33650\tTrain loss: 465.23760986328125\n",
      "Epoch: 33700\tTrain loss: 462.9497833251953\n",
      "Epoch: 33750\tTrain loss: 460.5739288330078\n",
      "Epoch: 33800\tTrain loss: 458.1791534423828\n",
      "Epoch: 33850\tTrain loss: 455.7350769042969\n",
      "Epoch: 33900\tTrain loss: 453.18328857421875\n",
      "Epoch: 33950\tTrain loss: 450.5714874267578\n",
      "Epoch: 34000\tTrain loss: 447.8193054199219\n",
      "Epoch: 34050\tTrain loss: 444.98046875\n",
      "Epoch: 34100\tTrain loss: 442.0231018066406\n",
      "Epoch: 34150\tTrain loss: 438.74542236328125\n",
      "Epoch: 34200\tTrain loss: 434.9491271972656\n",
      "Epoch: 34250\tTrain loss: 430.74464416503906\n",
      "Epoch: 34300\tTrain loss: 427.0980224609375\n",
      "Epoch: 34350\tTrain loss: 423.8445281982422\n",
      "Epoch: 34400\tTrain loss: 420.6336212158203\n",
      "Epoch: 34450\tTrain loss: 417.4988555908203\n",
      "Epoch: 34500\tTrain loss: 414.3233947753906\n",
      "Epoch: 34550\tTrain loss: 411.1474609375\n",
      "Epoch: 34600\tTrain loss: 408.08172607421875\n",
      "Epoch: 34650\tTrain loss: 404.7751922607422\n",
      "Epoch: 34700\tTrain loss: 401.7532958984375\n",
      "Epoch: 34750\tTrain loss: 398.46685791015625\n",
      "Epoch: 34800\tTrain loss: 395.34510803222656\n",
      "Epoch: 34850\tTrain loss: 392.2366943359375\n",
      "Epoch: 34900\tTrain loss: 576.5240478515625\n",
      "Epoch: 34950\tTrain loss: 864.4763488769531\n",
      "Epoch: 35000\tTrain loss: 505.2386779785156\n",
      "Epoch: 35050\tTrain loss: 383.9897003173828\n",
      "Epoch: 35100\tTrain loss: 380.8796081542969\n",
      "Epoch: 35150\tTrain loss: 377.71849060058594\n",
      "Epoch: 35200\tTrain loss: 374.0583801269531\n",
      "Epoch: 35250\tTrain loss: 370.05835723876953\n",
      "Epoch: 35300\tTrain loss: 367.7345428466797\n",
      "Epoch: 35350\tTrain loss: 495.5529022216797\n",
      "Epoch: 35400\tTrain loss: 343.3688659667969\n",
      "Epoch: 35450\tTrain loss: 345.57720947265625\n",
      "Epoch: 35500\tTrain loss: 325.4264221191406\n",
      "Epoch: 35550\tTrain loss: 348.4717712402344\n",
      "Epoch: 35600\tTrain loss: 320.8474807739258\n",
      "Epoch: 35650\tTrain loss: 310.8533935546875\n",
      "Epoch: 35700\tTrain loss: 304.44850158691406\n",
      "Epoch: 35750\tTrain loss: 298.0641860961914\n",
      "Epoch: 35800\tTrain loss: 292.8201141357422\n",
      "Epoch: 35850\tTrain loss: 293.61749267578125\n",
      "Epoch: 35900\tTrain loss: 285.75150299072266\n",
      "Epoch: 35950\tTrain loss: 308.5997772216797\n",
      "Epoch: 36000\tTrain loss: 453.7799530029297\n",
      "Epoch: 36050\tTrain loss: 350.3801040649414\n",
      "Epoch: 36100\tTrain loss: 315.21507263183594\n",
      "Epoch: 36150\tTrain loss: 298.7605895996094\n",
      "Epoch: 36200\tTrain loss: 282.4652862548828\n",
      "Epoch: 36250\tTrain loss: 511.5388946533203\n",
      "Epoch: 36300\tTrain loss: 32223.8115234375\n",
      "Epoch: 36350\tTrain loss: 2855.4512329101562\n",
      "Epoch: 36400\tTrain loss: 1331.8340148925781\n",
      "Epoch: 36450\tTrain loss: 1123.5073852539062\n",
      "Epoch: 36500\tTrain loss: 992.7195739746094\n",
      "Epoch: 36550\tTrain loss: 904.0236511230469\n",
      "Epoch: 36600\tTrain loss: 818.6813049316406\n",
      "Epoch: 36650\tTrain loss: 739.8396606445312\n",
      "Epoch: 36700\tTrain loss: 682.4478759765625\n",
      "Epoch: 36750\tTrain loss: 630.3244323730469\n",
      "Epoch: 36800\tTrain loss: 559.8954162597656\n",
      "Epoch: 36850\tTrain loss: 537.0787620544434\n",
      "Epoch: 36900\tTrain loss: 511.4469451904297\n",
      "Epoch: 36950\tTrain loss: 491.5868606567383\n",
      "Epoch: 37000\tTrain loss: 458.7298889160156\n",
      "Epoch: 37050\tTrain loss: 436.96849060058594\n",
      "Epoch: 37100\tTrain loss: 426.7391052246094\n",
      "Epoch: 37150\tTrain loss: 418.2371597290039\n",
      "Epoch: 37200\tTrain loss: 409.9661178588867\n",
      "Epoch: 37250\tTrain loss: 404.00160217285156\n",
      "Epoch: 37300\tTrain loss: 398.1296691894531\n",
      "Epoch: 37350\tTrain loss: 392.9318542480469\n",
      "Epoch: 37400\tTrain loss: 389.2003173828125\n",
      "Epoch: 37450\tTrain loss: 384.6029815673828\n",
      "Epoch: 37500\tTrain loss: 380.7079391479492\n",
      "Epoch: 37550\tTrain loss: 376.1169891357422\n",
      "Epoch: 37600\tTrain loss: 372.0870895385742\n",
      "Epoch: 37650\tTrain loss: 368.0872039794922\n",
      "Epoch: 37700\tTrain loss: 365.7409362792969\n",
      "Epoch: 37750\tTrain loss: 363.03846740722656\n",
      "Epoch: 37800\tTrain loss: 358.6412658691406\n",
      "Epoch: 37850\tTrain loss: 356.5260772705078\n",
      "Epoch: 37900\tTrain loss: 353.0233154296875\n",
      "Epoch: 37950\tTrain loss: 349.4486389160156\n",
      "Epoch: 38000\tTrain loss: 347.6777038574219\n",
      "Epoch: 38050\tTrain loss: 343.7971668243408\n",
      "Epoch: 38100\tTrain loss: 340.71056365966797\n",
      "Epoch: 38150\tTrain loss: 349.72362518310547\n",
      "Epoch: 38200\tTrain loss: 338.5218963623047\n",
      "Epoch: 38250\tTrain loss: 334.51116943359375\n",
      "Epoch: 38300\tTrain loss: 330.8023986816406\n",
      "Epoch: 38350\tTrain loss: 327.15162658691406\n",
      "Epoch: 38400\tTrain loss: 323.4378356933594\n",
      "Epoch: 38450\tTrain loss: 319.94850158691406\n",
      "Epoch: 38500\tTrain loss: 316.5061492919922\n",
      "Epoch: 38550\tTrain loss: 312.8215026855469\n",
      "Epoch: 38600\tTrain loss: 1965.0010375976562\n",
      "Epoch: 38650\tTrain loss: 1360.3941040039062\n",
      "Epoch: 38700\tTrain loss: 1161.5548706054688\n",
      "Epoch: 38750\tTrain loss: 1045.9747924804688\n",
      "Epoch: 38800\tTrain loss: 923.0256042480469\n",
      "Epoch: 38850\tTrain loss: 651.9666595458984\n",
      "Epoch: 38900\tTrain loss: 408.4461364746094\n",
      "Epoch: 38950\tTrain loss: 385.140625\n",
      "Epoch: 39000\tTrain loss: 381.23695373535156\n",
      "Epoch: 39050\tTrain loss: 374.69200134277344\n",
      "Epoch: 39100\tTrain loss: 375.6804504394531\n",
      "Epoch: 39150\tTrain loss: 364.0335235595703\n",
      "Epoch: 39200\tTrain loss: 358.094482421875\n",
      "Epoch: 39250\tTrain loss: 354.12486267089844\n",
      "Epoch: 39300\tTrain loss: 351.0497131347656\n",
      "Epoch: 39350\tTrain loss: 348.43724060058594\n",
      "Epoch: 39400\tTrain loss: 346.03546142578125\n",
      "Epoch: 39450\tTrain loss: 346.5570068359375\n",
      "Epoch: 39500\tTrain loss: 342.60646057128906\n",
      "Epoch: 39550\tTrain loss: 339.86419677734375\n",
      "Epoch: 39600\tTrain loss: 337.7864074707031\n",
      "Epoch: 39650\tTrain loss: 335.41085052490234\n",
      "Epoch: 39700\tTrain loss: 333.09554290771484\n",
      "Epoch: 39750\tTrain loss: 333.287841796875\n",
      "Epoch: 39800\tTrain loss: 328.3004913330078\n",
      "Epoch: 39850\tTrain loss: 325.403751373291\n",
      "Epoch: 39900\tTrain loss: 343.4220428466797\n",
      "Epoch: 39950\tTrain loss: 320.1860122680664\n",
      "Epoch: 40000\tTrain loss: 317.12255859375\n",
      "Epoch: 40050\tTrain loss: 319.42676544189453\n",
      "Epoch: 40100\tTrain loss: 308.4602966308594\n",
      "Epoch: 40150\tTrain loss: 305.6559829711914\n",
      "Epoch: 40200\tTrain loss: 403.7698287963867\n",
      "Epoch: 40250\tTrain loss: 316.307373046875\n",
      "Epoch: 40300\tTrain loss: 309.41265869140625\n",
      "Epoch: 40350\tTrain loss: 303.8231945037842\n",
      "Epoch: 40400\tTrain loss: 298.59944343566895\n",
      "Epoch: 40450\tTrain loss: 293.4344177246094\n",
      "Epoch: 40500\tTrain loss: 288.3837890625\n",
      "Epoch: 40550\tTrain loss: 283.0628128051758\n",
      "Epoch: 40600\tTrain loss: 276.82818603515625\n",
      "Epoch: 40650\tTrain loss: 270.3045196533203\n",
      "Epoch: 40700\tTrain loss: 266.8075485229492\n",
      "Epoch: 40750\tTrain loss: 256.5871124267578\n",
      "Epoch: 40800\tTrain loss: 247.93444061279297\n",
      "Epoch: 40850\tTrain loss: 242.23690032958984\n",
      "Epoch: 40900\tTrain loss: 234.22998809814453\n",
      "Epoch: 40950\tTrain loss: 405.0770721435547\n",
      "Epoch: 41000\tTrain loss: 330.0707244873047\n",
      "Epoch: 41050\tTrain loss: 259.0689010620117\n",
      "Epoch: 41100\tTrain loss: 237.93319702148438\n",
      "Epoch: 41150\tTrain loss: 225.9775733947754\n",
      "Epoch: 41200\tTrain loss: 216.46474075317383\n",
      "Epoch: 41250\tTrain loss: 208.13574981689453\n",
      "Epoch: 41300\tTrain loss: 200.75837326049805\n",
      "Epoch: 41350\tTrain loss: 194.4889678955078\n",
      "Epoch: 41400\tTrain loss: 188.92496490478516\n",
      "Epoch: 41450\tTrain loss: 183.60278272628784\n",
      "Epoch: 41500\tTrain loss: 178.41814422607422\n",
      "Epoch: 41550\tTrain loss: 173.17468094825745\n",
      "Epoch: 41600\tTrain loss: 168.0298080444336\n",
      "Epoch: 41650\tTrain loss: 162.96663665771484\n",
      "Epoch: 41700\tTrain loss: 157.54025268554688\n",
      "Epoch: 41750\tTrain loss: 152.23380088806152\n",
      "Epoch: 41800\tTrain loss: 146.91176986694336\n",
      "Epoch: 41850\tTrain loss: 141.81711196899414\n",
      "Epoch: 41900\tTrain loss: 136.54432678222656\n",
      "Epoch: 41950\tTrain loss: 136.9123077392578\n",
      "Epoch: 42000\tTrain loss: 127.60617446899414\n",
      "Epoch: 42050\tTrain loss: 122.63241958618164\n",
      "Epoch: 42100\tTrain loss: 117.59348106384277\n",
      "Epoch: 42150\tTrain loss: 112.55530071258545\n",
      "Epoch: 42200\tTrain loss: 110.66480255126953\n",
      "Epoch: 42250\tTrain loss: 103.06057357788086\n",
      "Epoch: 42300\tTrain loss: 99.4319577217102\n",
      "Epoch: 42350\tTrain loss: 102.13418388366699\n",
      "Epoch: 42400\tTrain loss: 111.2578010559082\n",
      "Epoch: 42450\tTrain loss: 96.99099349975586\n",
      "Epoch: 42500\tTrain loss: 90.05801391601562\n",
      "Epoch: 42550\tTrain loss: 84.4692952632904\n",
      "Epoch: 42600\tTrain loss: 79.69168663024902\n",
      "Epoch: 42650\tTrain loss: 75.31077003479004\n",
      "Epoch: 42700\tTrain loss: 71.0368299484253\n",
      "Epoch: 42750\tTrain loss: 65.42075312137604\n",
      "Epoch: 42800\tTrain loss: 60.7369499206543\n",
      "Epoch: 42850\tTrain loss: 57.59448432922363\n",
      "Epoch: 42900\tTrain loss: 52.999154567718506\n",
      "Epoch: 42950\tTrain loss: 52.04978632926941\n",
      "Epoch: 43000\tTrain loss: 45.17529582977295\n",
      "Epoch: 43050\tTrain loss: 40.93651080131531\n",
      "Epoch: 43100\tTrain loss: 38.31820011138916\n",
      "Epoch: 43150\tTrain loss: 37.72609066963196\n",
      "Epoch: 43200\tTrain loss: 40.07624626159668\n",
      "Epoch: 43250\tTrain loss: 31.86258864402771\n",
      "Epoch: 43300\tTrain loss: 28.83328151702881\n",
      "Epoch: 43350\tTrain loss: 26.008609652519226\n",
      "Epoch: 43400\tTrain loss: 25.853459298610687\n",
      "Epoch: 43450\tTrain loss: 22.309460639953613\n",
      "Epoch: 43500\tTrain loss: 20.23440906405449\n",
      "Epoch: 43550\tTrain loss: 20.891309022903442\n",
      "Epoch: 43600\tTrain loss: 19.290814876556396\n",
      "Epoch: 43650\tTrain loss: 16.719743728637695\n",
      "Epoch: 43700\tTrain loss: 16.62586522102356\n",
      "Epoch: 43750\tTrain loss: 13.790988445281982\n",
      "Epoch: 43800\tTrain loss: 12.481324911117554\n",
      "Epoch: 43850\tTrain loss: 13.54602599143982\n",
      "Epoch: 43900\tTrain loss: 11.185138165950775\n",
      "Epoch: 43950\tTrain loss: 20.513568878173828\n",
      "Epoch: 44000\tTrain loss: 30.86914825439453\n",
      "Epoch: 44050\tTrain loss: 13.177650272846222\n",
      "Epoch: 44100\tTrain loss: 11.12564155459404\n",
      "Epoch: 44150\tTrain loss: 9.714407920837402\n",
      "Epoch: 44200\tTrain loss: 8.605525970458984\n",
      "Epoch: 44250\tTrain loss: 7.662332206964493\n",
      "Epoch: 44300\tTrain loss: 6.856306806206703\n",
      "Epoch: 44350\tTrain loss: 6.141403913497925\n",
      "Epoch: 44400\tTrain loss: 5.502191066741943\n",
      "Epoch: 44450\tTrain loss: 4.925085544586182\n",
      "Epoch: 44500\tTrain loss: 4.404259026050568\n",
      "Epoch: 44550\tTrain loss: 3.9304381981492043\n",
      "Epoch: 44600\tTrain loss: 3.836715579032898\n",
      "Epoch: 44650\tTrain loss: 4.333510518074036\n",
      "Epoch: 44700\tTrain loss: 2.8411706686019897\n",
      "Epoch: 44750\tTrain loss: 2.500955816358328\n",
      "Epoch: 44800\tTrain loss: 2.20120769739151\n",
      "Epoch: 44850\tTrain loss: 7.010935425758362\n",
      "Epoch: 44900\tTrain loss: 4.402613818645477\n",
      "Epoch: 44950\tTrain loss: 1.726630449295044\n",
      "Epoch: 45000\tTrain loss: 1.4541226625442505\n",
      "Epoch: 45050\tTrain loss: 1.263617992401123\n",
      "Epoch: 45100\tTrain loss: 1.0995541512966156\n",
      "Epoch: 45150\tTrain loss: 0.9485105900093913\n",
      "Epoch: 45200\tTrain loss: 1.1884718537330627\n",
      "Epoch: 45250\tTrain loss: 47.565134048461914\n",
      "Epoch: 45300\tTrain loss: 2.248001456260681\n",
      "Epoch: 45350\tTrain loss: 1.1739166378974915\n",
      "Epoch: 45400\tTrain loss: 0.8885804116725922\n",
      "Epoch: 45450\tTrain loss: 0.7305888235569\n",
      "Epoch: 45500\tTrain loss: 0.6111989915370941\n",
      "Epoch: 45550\tTrain loss: 0.5075526479631662\n",
      "Epoch: 45600\tTrain loss: 0.42612671852111816\n",
      "Epoch: 45650\tTrain loss: 0.35714830458164215\n",
      "Epoch: 45700\tTrain loss: 0.29867119528353214\n",
      "Epoch: 45750\tTrain loss: 0.26355980336666107\n",
      "Epoch: 45800\tTrain loss: 32.564093589782715\n",
      "Epoch: 45850\tTrain loss: 0.275582030415535\n",
      "Epoch: 45900\tTrain loss: 0.23080933839082718\n",
      "Epoch: 45950\tTrain loss: 0.21389577677473426\n",
      "Epoch: 46000\tTrain loss: 4.068698942661285\n",
      "Epoch: 46050\tTrain loss: 30.850457191467285\n",
      "Epoch: 46100\tTrain loss: 17.13424301147461\n",
      "Epoch: 46150\tTrain loss: 3196.7722778320312\n",
      "Epoch: 46200\tTrain loss: 54549.88415527344\n",
      "Epoch: 46250\tTrain loss: 47052.62072753906\n",
      "Epoch: 46300\tTrain loss: 41960.38781738281\n",
      "Epoch: 46350\tTrain loss: 37882.901123046875\n",
      "Epoch: 46400\tTrain loss: 35392.53643798828\n",
      "Epoch: 46450\tTrain loss: 32433.51092529297\n",
      "Epoch: 46500\tTrain loss: 29867.316833496094\n",
      "Epoch: 46550\tTrain loss: 27577.322998046875\n",
      "Epoch: 46600\tTrain loss: 25597.569702148438\n",
      "Epoch: 46650\tTrain loss: 23548.773986816406\n",
      "Epoch: 46700\tTrain loss: 21848.007934570312\n",
      "Epoch: 46750\tTrain loss: 20197.733154296875\n",
      "Epoch: 46800\tTrain loss: 1629.1233520507812\n",
      "Epoch: 46850\tTrain loss: 1470.2486877441406\n",
      "Epoch: 46900\tTrain loss: 1386.4932250976562\n",
      "Epoch: 46950\tTrain loss: 1337.0750122070312\n",
      "Epoch: 47000\tTrain loss: 1273.7010498046875\n",
      "Epoch: 47050\tTrain loss: 1128.4424743652344\n",
      "Epoch: 47100\tTrain loss: 2597.0834350585938\n",
      "Epoch: 47150\tTrain loss: 1561.0129089355469\n",
      "Epoch: 47200\tTrain loss: 915.8328552246094\n",
      "Epoch: 47250\tTrain loss: 823.5079650878906\n",
      "Epoch: 47300\tTrain loss: 812.0870666503906\n",
      "Epoch: 47350\tTrain loss: 760.240478515625\n",
      "Epoch: 47400\tTrain loss: 719.5134582519531\n",
      "Epoch: 47450\tTrain loss: 745.1349792480469\n",
      "Epoch: 47500\tTrain loss: 722.6251220703125\n",
      "Epoch: 47550\tTrain loss: 635.0935974121094\n",
      "Epoch: 47600\tTrain loss: 616.1907043457031\n",
      "Epoch: 47650\tTrain loss: 765.75830078125\n",
      "Epoch: 47700\tTrain loss: 587.2515869140625\n",
      "Epoch: 47750\tTrain loss: 555.8699035644531\n",
      "Epoch: 47800\tTrain loss: 561.9875335693359\n",
      "Epoch: 47850\tTrain loss: 609.3072967529297\n",
      "Epoch: 47900\tTrain loss: 545.302734375\n",
      "Epoch: 47950\tTrain loss: 533.5812377929688\n",
      "Epoch: 48000\tTrain loss: 511.61907958984375\n",
      "Epoch: 48050\tTrain loss: 511.57505798339844\n",
      "Epoch: 48100\tTrain loss: 482.8156280517578\n",
      "Epoch: 48150\tTrain loss: 488.5787353515625\n",
      "Epoch: 48200\tTrain loss: 532.9060821533203\n",
      "Epoch: 48250\tTrain loss: 462.3373107910156\n",
      "Epoch: 48300\tTrain loss: 456.216552734375\n",
      "Epoch: 48350\tTrain loss: 447.8491668701172\n",
      "Epoch: 48400\tTrain loss: 506.65399169921875\n",
      "Epoch: 48450\tTrain loss: 446.43858337402344\n",
      "Epoch: 48500\tTrain loss: 462.13612365722656\n",
      "Epoch: 48550\tTrain loss: 444.54551696777344\n",
      "Epoch: 48600\tTrain loss: 661.3111572265625\n",
      "Epoch: 48650\tTrain loss: 429.64588928222656\n",
      "Epoch: 48700\tTrain loss: 417.62815856933594\n",
      "Epoch: 48750\tTrain loss: 30042.4990234375\n",
      "Epoch: 48800\tTrain loss: 4005.582763671875\n",
      "Epoch: 48850\tTrain loss: 3459.3743286132812\n",
      "Epoch: 48900\tTrain loss: 2767.4243774414062\n",
      "Epoch: 48950\tTrain loss: 1746.2716674804688\n",
      "Epoch: 49000\tTrain loss: 1604.2597045898438\n",
      "Epoch: 49050\tTrain loss: 1525.4929809570312\n",
      "Epoch: 49100\tTrain loss: 1439.1331176757812\n",
      "Epoch: 49150\tTrain loss: 1315.7472534179688\n",
      "Epoch: 49200\tTrain loss: 822.9587707519531\n",
      "Epoch: 49250\tTrain loss: 696.1026916503906\n",
      "Epoch: 49300\tTrain loss: 668.493408203125\n",
      "Epoch: 49350\tTrain loss: 656.8031921386719\n",
      "Epoch: 49400\tTrain loss: 647.4722595214844\n",
      "Epoch: 49450\tTrain loss: 640.5619812011719\n",
      "Epoch: 49500\tTrain loss: 645.5672607421875\n",
      "Epoch: 49550\tTrain loss: 628.128662109375\n",
      "Epoch: 49600\tTrain loss: 615.1780090332031\n",
      "Epoch: 49650\tTrain loss: 619.8590698242188\n",
      "Epoch: 49700\tTrain loss: 602.4695434570312\n",
      "Epoch: 49750\tTrain loss: 608.9872283935547\n",
      "Epoch: 49800\tTrain loss: 603.1625366210938\n",
      "Epoch: 49850\tTrain loss: 584.4024810791016\n",
      "Epoch: 49900\tTrain loss: 588.1694946289062\n",
      "Epoch: 49950\tTrain loss: 576.7071228027344\n"
     ]
    }
   ],
   "source": [
    "for baseline in ['CFRNN']:\n",
    "    for seed in range(5):\n",
    "        run_medical_experiments(dataset='electricity', \n",
    "                                baseline=baseline,\n",
    "                                save_model=True, \n",
    "                                save_results=True,\n",
    "                                seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a45a98a5-3175-40ce-9c70-5c0b236300b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CFRNN\n",
      "50.0 \\(\\pm\\) 44.7\\%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for baseline in ['CFRNN']:\n",
    "    print(baseline)\n",
    "    coverages_mean, coverages_std = get_joint_medical_coverages(baseline, 'electricity', seeds=range(5))\n",
    "    \n",
    "    print('{:.1f} \\\\(\\\\pm\\\\) {:.1f}\\\\%'.format(coverages_mean, coverages_std))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4074c07c-9cca-49c8-83b3-156d108530a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CFRNN\n",
      "2884.1799169921874\n",
      "3796.0027420448046\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for baseline in ['CFRNN']:\n",
    "    print(baseline)\n",
    "    widths_mean, widths_std = get_medical_interval_widths(baseline, 'electricity', seeds=range(5))\n",
    "    \n",
    "    print(widths_mean)\n",
    "    print(widths_std)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b6d7af-a7c6-46eb-aead-23336fa9a665",
   "metadata": {},
   "source": [
    "## Ablation: Uncorrected calibration scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c58171e-7675-4983-851c-5a963d25aeec",
   "metadata": {},
   "source": [
    "#### Electricity Consumption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "752fa9ad-f6cc-43d3-9fb5-5bfdb071888e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50.0 \\(\\pm\\) 44.7\\%\n"
     ]
    }
   ],
   "source": [
    "coverages_mean, coverages_std = get_joint_medical_coverages('CFRNN', 'electricity', seeds=range(5), correct_conformal=True)\n",
    "    \n",
    "print('{:.1f} \\\\(\\\\pm\\\\) {:.1f}\\\\%'.format(coverages_mean, coverages_std))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "86d905e4-dc31-47ae-9b47-bda58c4c59aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50.0 \\(\\pm\\) 44.7\\%\n"
     ]
    }
   ],
   "source": [
    "coverages_mean, coverages_std = get_joint_medical_coverages('CFRNN', 'electricity', seeds=range(5), correct_conformal=False)\n",
    "    \n",
    "print('{:.1f} \\\\(\\\\pm\\\\) {:.1f}\\\\%'.format(coverages_mean, coverages_std))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "66c5f5a2-4423-4aff-b2e4-91cc7a3ce3be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100.0\\%, 100.0\\%]\n",
      "[0.0\\%, 0.0\\%]\n",
      "[0.0\\%, 50.0\\%]\n",
      "[50.0\\%, 50.0\\%]\n",
      "[100.0\\%, 100.0\\%]\n"
     ]
    }
   ],
   "source": [
    "for seed in range(5):\n",
    "    results = load_medical_results(dataset='electricity', baseline='CFRNN', seed=seed)\n",
    "    independent_coverages = results['Mean independent coverage']\n",
    "    print('[{:.1f}\\\\%, {:.1f}\\\\%]'.format(independent_coverages.min() * 100, independent_coverages.max() * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ab9b269d-13d6-4f21-8848-d6eaeb6bc8cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100.0\\%, 100.0\\%]\n",
      "[0.0\\%, 0.0\\%]\n",
      "[0.0\\%, 50.0\\%]\n",
      "[50.0\\%, 50.0\\%]\n",
      "[100.0\\%, 100.0\\%]\n"
     ]
    }
   ],
   "source": [
    "for seed in range(5):\n",
    "    uncorrected_mimic_results = get_uncorrected_medical_results(dataset='electricity', seed=seed)\n",
    "    independent_coverages = uncorrected_mimic_results['Mean independent coverage']\n",
    "    print('[{:.1f}\\\\%, {:.1f}\\\\%]'.format(independent_coverages.min() * 100, independent_coverages.max() * 100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('lab')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "55cd0a945cf9041a238b2dc7f6f29da44cde9fabaada4bd04363a8e3e2e8be13"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
